<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis数据结构及常见使用场景]]></title>
    <url>%2F2019%2F12%2F06%2FRedis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[Redis基础数据类型及使用场景 String 缓存功能：String字符串是最常用的数据类型，不仅仅是Redis，各个语言都是最基本类型，因此，利用Redis作为缓存，配合其它数据库作为存储层，利用Redis支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。 计数器：许多系统都会使用Redis作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。 共享用户Session：用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存Cookie，但是可以利用Redis将用户的Session集中管理，在这种模式只需要保证Redis的高可用，每次用户Session的更新和获取都可以快速完成。大大提高效率。 Hash 这个是类似 Map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 Redis 里，然后每次读写缓存的时候，可以就操作 Hash 里的某个字段。但是这个的场景其实还是多少单一了一些，因为现在很多对象都是比较复杂的，比如你的商品对象可能里面就包含了很多属性，其中也有对象。我自己使用的场景用得不是那么多。 ListList 是有序列表，这个还是可以玩儿出很多花样的。 比如可以通过 List 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 List 实现分页查询，这个是很棒的一个功能，基于 Redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 比如可以搞个简单的消息队列，从 List 头怼进去，从 List 屁股那里弄出来。List本身就是我们在开发过程中比较常用的数据结构了，热点数据更不用说了。 消息队列：Redis的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过Lpush命令从左边插入数据，多个数据消费者，可以使用BRpop命令阻塞的“抢”列表尾部的数据。 文章列表或者数据分页展示的应用。比如，我们常用的博客网站的文章列表，当用户量越来越多时，而且每一个用户都有自己的文章列表，而且当文章多时，都需要分页展示，这时可以考虑使用Redis的列表，列表不但有序同时还支持按照范围内获取元素，可以完美解决分页查询功能。大大提高查询效率。 SetSet 是无序集合，会自动去重的那种。 去重： 直接基于 Set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 JVM 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于Redis进行全局的 Set 去重。 可以基于 Set 玩 交集、并集、差集 的操作。比如交集吧，我们可以把两个人的好友列表整一个交集，看看俩人的共同好友是谁？对吧。反正这些场景比较多，因为对比很快，操作也简单，两个查询一个Set搞定。 Sorted SetSorted set 是排序的 Set，去重并可以排序，写进去的时候给一个分数，自动根据分数排序。 有序集合的使用场景与集合类似，但是set集合不是自动有序的，而Sorted set可以利用分数进行成员间的排序，而且是插入时就排序好。所以当你需要一个有序且不重复的集合列表时，就可以选择Sorted set数据结构作为选择方案。 排行榜：有序集合经典使用场景。例如视频网站需要对用户上传的视频做排行榜，榜单维护可能是多方面：按照时间、按照播放量、按照获得的赞数等。 用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 Redis更多应用场景 Redis 和 Memcached 区别Memcache先来看看 MC 的特点： MC 处理请求时使用多线程异步IO 的方式，可以合理利用 CPU 多核的优势，性能非常优秀； MC 功能简单，使用内存存储数据； MC 的内存结构以及钙化问题； MC 对缓存的数据可以设置失效期，过期后的数据会被清除； 失效的策略采用延迟失效，就是当再次使用数据时检查是否失效； 当容量存满时，会对缓存中的数据进行剔除，剔除时除了会对过期 key 进行清理，还会按 LRU 策略对数据进行剔除。 另外，使用 MC 有一些限制，这些限制在现在的互联网场景下很致命，成为大家选择Redis、MongoDB的重要原因： key 不能超过 250 个字节； value 不能超过 1M 字节； key 的最大失效时间是 30 天； 只支持 K-V 结构，不提供持久化和主从同步功能。 Redis简单说一下 Redis 的特点，方便和 Memcache 比较。 与 MC 不同的是，Redis 采用单线程模式处理请求。这样做的原因有 2 个： 一个是因为采用了非阻塞的异步事件处理机制； 另一个是缓存数据都是内存操作 IO 时间不会太长，单线程可以避免线程上下文切换产生的代价。 Redis 支持持久化，所以 Redis 不仅仅可以用作缓存，也可以用作 NoSQL 数据库。 相比 MC，Redis 还有一个非常大的优势，就是除了 K-V 之外，还支持复杂的数据结构，例如 list、set、sorted set、hash 等。 Redis 提供主从同步机制，以及原生支持集群模式，能够提供高可用服务, 在 redis3.x 版本中，便能支持 Cluster 模式; 而 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 Redis 只使用单核，而 Memcached 可以使用多核，所以平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。而在 100k 以上的数据中，Memcached 性能要高于 Redis，虽然 Redis 最近也在存储大数据的性能上进行优化，但是比起 Remcached，还是稍有逊色。 Redis 线程模型Redis 内部使用 文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 Socket，根据 Socket 上的事件来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 Socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 Socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 Socket，会将 Socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。 高级用法Bitmap位图是支持按 bit 位来存储信息，可以用来实现 布隆过滤器（BloomFilter）； HyperLogLog供不精确的去重计数功能，比较适合用来做大规模数据的去重统计，例如统计 UV； Geospatial可以用来保存地理位置，并作位置距离计算或者根据半径计算位置等。有没有想过用Redis来实现附近的人？或者计算最优地图路径？ pub/sub功能是订阅发布功能，可以用作简单的消息队列。 Pipeline可以批量执行一组指令，一次性返回全部结果，可以减少频繁的请求应答。 LuaRedis 支持提交 Lua 脚本来执行一系列的功能, 利用他的原子性。 事务最后一个功能是事务，但 Redis 提供的不是严格的事务，Redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。 Redis集群 参考[1]《吊打面试官》系列-Redis基础[2]《吊打面试官》系列-缓存雪崩、击穿、穿透[3]《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU[4]《吊打面试官》系列-Redis终章凛冬将至、FPX新王登基[5]《吊打面试官》系列-秒杀系统设计]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>入门</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat系统架构[转载]]]></title>
    <url>%2F2019%2F12%2F06%2FTomcat%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84-%E8%BD%AC%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[Tomcat顶层架构先上一张Tomcat的顶层结构图，如下 简化清晰版 Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，用于具体提供服务。 Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下： Connector用于处理连接相关的事情，并提供Socket与Request和Response相关的转化; Container用于封装和管理Servlet，以及具体处理Request请求； 一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接 示意图如下（Engine、Host、Context下边会说到）： 多个 Connector 和一个 Container 就形成了一个 Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给它生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。 另外，上述的包含关系或者说是父子关系，都可以在tomcat的conf目录下的server.xml配置文件中看出，如下是删除了注释内容之后的一个完整的server.xml配置文件（Tomcat版本为8.0） 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version='1.0' encoding='utf-8'?&gt;&lt;Server port="8005" shutdown="SHUTDOWN"&gt; &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt; &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt; &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt; &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name="UserDatabase" auth="Container" type="org.apache.catalina.UserDatabase" description="User database that can be updated and saved" factory="org.apache.catalina.users.MemoryUserDatabaseFactory" pathname="conf/tomcat-users.xml" /&gt; &lt;/GlobalNamingResources&gt; &lt;Service name="Catalina"&gt; &lt;Connector port="8060" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; &lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; &lt;Engine name="Catalina" defaultHost="localhost"&gt; &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt; &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt; &lt;/Realm&gt; &lt;Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;Context path="" docBase="/xxx/target/bigdata-web" debug="0" reloadable="true" crossContext="true" allowLinking="true"/&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 上边的配置文件，还可以通过下边的一张结构图更清楚的理解： Server标签设置的端口号为8005，shutdown=”SHUTDOWN” ，表示在8005端口监听“SHUTDOWN”命令，如果接收到了就会关闭Tomcat。一个Server有一个Service，当然还可以进行配置。Service左边的内容都属于Container的，Service下边是多个Connector。 Tomcat顶层架构小结 （1）Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container； （2） Server掌管着整个Tomcat的生死大权； （4）Service 是对外提供服务的； （5）Connector用于接受请求并将请求封装成Request和Response来具体处理； （6）Container用于封装和管理Servlet，以及具体处理request请求； 以上是整个Tomcat顶层的分层架构和各个组件之间的关系以及作用，但对于绝大多数的开发人员来说Server和Service对我们来说确实很远，而我们开发中绝大部分进行配置的内容是属于Connector和Container的，所以接下来介绍一下Connector和Container。 Connector和Container的微妙关系 由上述内容我们大致可以知道一个请求发送到Tomcat之后，首先经过Service然后会交给我们的Connector，Connector用于接收请求并将接收的请求封装为Request和Response来具体处理，Request和Response封装完之后再交由Container进行处理，Container处理完请求之后再返回给Connector，最后在由Connector通过Socket将处理的结果返回给客户端，这样整个请求的就处理完了！ Connector最底层使用的是Socket来进行连接的，Request和Response是按照HTTP协议来封装的，所以Connector同时需要实现TCP/IP协议和HTTP协议！ Tomcat既然处理请求，那么肯定需要先接收到这个请求，接收请求这个东西我们首先就需要看一下Connector！ Connector架构分析Connector用于接受请求并将请求封装成Request和Response，然后交给Container进行处理，Container处理完之后在交给Connector返回给客户端。 因此，我们可以把Connector分为四个方面进行理解： （1）Connector如何接受请求的？（2）如何将请求封装成Request和Response的？（3）封装完之后的Request和Response如何交给Container进行处理的？（4）Container处理完之后如何交给Connector并返回给客户端的？ 首先看一下Connector的结构图，如下所示： Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。 其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。 Endpoint 用来处理底层Socket的网络连接 Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的 Endpoint 的抽象实现AbstractEndpoint里面定义的 Acceptor 和 AsyncTimeout 两个内部类和一个 Handler 接口。 Acceptor用于监听请求AsyncTimeout用于检查异步Request的超时Handler 用于处理接收到的Socket，在内部调用Processor进行处理 Processor 用于将Endpoint接收到的Socket封装成Request Processor用来实现HTTP协议的 Adapter 用于将Request交给Container进行具体的处理 Adapter将请求适配到Servlet容器进行具体的处理 至此，我们应该很轻松的回答（1）（2）（3）的问题了，但是（4）还是不知道，那么我们就来看一下Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？ Container架构分析Container用于封装和管理Servlet，以及具体处理Request请求，在Connector内部包含了4个子容器，结构图如下： 4个子容器的作用分别是： Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine； Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点； Context：代表一个应用程序，对应着平时开发的一套程序，或者一个 WEB-INF 目录以及下面的 web.xml 文件； Wrapper：每一Wrapper封装着一个Servlet； 下面找一个Tomcat的文件目录对照一下，如下图所示： Context和Host的区别 Context表示一个应用，Tomcat中默认的配置下 webapps 下的每一个文件夹目录都是一个Context，其中ROOT目录中存放着主应用，其他目录存放着子应用，而整个 webapps 就是一个Host站点。 我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：www.ledouit.com. 如果是Host（webapps）下的其他应用，则可以使用 www.ledouit.com/docs 进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。 看到这里我们知道Container是什么，但是还是不知道Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？别急！下边就开始探讨一下Container是如何进行处理的！ Container如何处理请求的Container处理请求是使用 Pipeline-Valve 管道来处理的！（Valve是阀门之意） Pipeline-Valve 是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的请求返回，再让下一个处理着继续处理。但是！Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点： 每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做 BaseValve，BaseValve是不可删除的； 在上层容器的管道的BaseValve中会调用下层容器的管道。 我们知道 Container 包含四个子容器，而这四个子容器对应的 BaseValve 分别是： StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve Pipeline的处理流程图如下（图D）： （1）Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）； （2）在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。 （3）当执行到StandardWrapperValve的时候，会在 StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！ （4）当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。 Servlet生命周期 Tomcat优化不要保留无用对象 web.xml中无用标签应该删掉； server.xml中无用标签也需要删掉； 原文链接： 四张图带你了解Tomcat系统架构–让面试官颤抖的Tomcat回答系列]]></content>
      <categories>
        <category>Web</category>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Web</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java动态代理]]></title>
    <url>%2F2019%2F11%2F29%2FJava%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是代理我们大家都知道微商代理，简单地说就是代替厂家卖商品，厂家“委托”代理为其销售商品。关于微商代理，首先我们从他们那里买东西时通常不知道背后的厂家究竟是谁，也就是说，“委托者”对我们来说是不可见的;其次，微商代理主要以朋友圈的人为目标客户，这就相当于为厂家做了一次对客户群体的“过滤”。我们把微商代理和厂家进一步抽象，前者可抽象为代理类，后者可抽象为委托类(被代理类)。通过使用代理，通常有两个优点，并且能够分别与我们提到的微商代理的两个特点对应起来： 优点一：可以隐藏委托类的实现; 优点二：可以实现客户与委托类间的解耦，在不修改委托类代码的情况下能够做一些额外的处理。 代理模式代理模式：给某一个对象提供一个代理，并由代理对象来控制对真实对象的访问。代理模式是一种结构型设计模式。 代理模式角色分为 3 种： Subject（抽象主题角色）：定义代理类和真实主题的公共对外方法，也是代理类代理真实主题的方法； RealSubject（真实主题角色）：真正实现业务逻辑的类； Proxy（代理主题角色）：用来代理和封装真实主题； 代理模式的结构比较简单，其核心是代理类，为了让客户端能够一致性地对待真实对象和代理对象，在代理模式中引入了抽象层 如果根据字节码的创建时机来分类，可以分为静态代理和动态代理： 静态代理就是在程序运行前就已经存在代理类的字节码文件，代理类和真实主题角色的关系在运行前就确定了。 动态代理源码是在程序运行期间由JVM根据反射等机制动态的生成，所以在运行前并不存在代理类的字节码文件。 静态代理代码示例我们先通过实例来学习静态代理，然后理解静态代理的缺点，再来学习本文的主角：动态代理 编写一个接口 UserService ，以及该接口的一个实现类 UserServiceImpl： 12345678910111213public interface UserService &#123; public void select(); public void update();&#125;public class UserServiceImpl implements UserService &#123; public void select() &#123; System.out.println("查询 selectById"); &#125; public void update() &#123; System.out.println("更新 update"); &#125;&#125; 我们将通过静态代理对 UserServiceImpl 进行功能增强，在调用 select 和 update 之前记录一些日志。写一个代理类 UserServiceProxy，代理类需要实现 UserService 123456789101112131415161718192021222324public class UserServiceProxy implements UserService &#123; private UserService target; // 被代理的对象 public UserServiceProxy(UserService target) &#123; this.target = target; &#125; public void select() &#123; before(); target.select(); // 这里才实际调用真实主题角色的方法 after(); &#125; public void update() &#123; before(); target.update(); // 这里才实际调用真实主题角色的方法 after(); &#125; private void before() &#123; // 在执行方法之前执行 System.out.printf("log start time [%s] \n", new Date()); &#125; private void after() &#123; // 在执行方法之后执行 System.out.printf("log end time [%s] \n", new Date()); &#125;&#125; 测试; 123456789public class Client1 &#123; public static void main(String[] args) &#123; UserService userServiceImpl = new UserServiceImpl(); UserService proxy = new UserServiceProxy(userServiceImpl); proxy.select(); proxy.update(); &#125;&#125; 输出： 123456log start time [Thu Dec 20 14:13:25 CST 2018] 查询 selectByIdlog end time [Thu Dec 20 14:13:25 CST 2018] log start time [Thu Dec 20 14:13:25 CST 2018] 更新 updatelog end time [Thu Dec 20 14:13:25 CST 2018] 通过静态代理，我们达到了功能增强的目的，而且没有侵入原代码，这是静态代理的一个优点。 静态代理缺点虽然静态代理实现简单，且不侵入原代码，但是，当场景稍微复杂一些的时候，静态代理的缺点也会暴露出来。 1、 当需要代理多个类的时候，由于代理对象要实现与目标对象一致的接口，有两种方式： 只维护一个代理类，由这个代理类实现多个接口，但是这样就导致代理类过于庞大; 新建多个代理类，每个目标对象对应一个代理类，但是这样会产生过多的代理类; 2、 当接口需要增加、删除、修改方法的时候，目标对象与代理类都要同时修改，不易维护。 动态代理生成原理Java虚拟机类加载过程主要分为五个阶段：加载、验证、准备、解析、`初始化。其中加载阶段需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据访问入口 由于虚拟机规范对这3点要求并不具体，所以实际的实现是非常灵活的，关于第1点，获取类的二进制字节流（class字节码）就有很多途径： 从ZIP包获取，这是JAR、EAR、WAR等格式的基础 从网络中获取，典型的应用是 Applet 运行时计算生成，这种场景使用最多的是动态代理技术，在 java.lang.reflect.Proxy 类中，就是用了 ProxyGenerator.generateProxyClass来为特定接口生成形式为 *$Proxy 的代理类的二进制字节流 由其它文件生成，典型应用是JSP，即由JSP文件生成对应的Class类 从数据库中获取等等 所以，动态代理就是想办法，根据接口或目标对象，计算出代理类的字节码，然后再加载到JVM中使用。但是如何计算？如何生成？情况也许比想象的复杂得多，我们需要借助现有的方案。 常见的字节码操作类库 这里有一些介绍：java-source.net/open-source… Apache BCEL (Byte Code Engineering Library)：是Java classworking广泛使用的一种框架，它可以深入到JVM汇编语言进行类操作的细节。 ObjectWeb ASM：是一个Java字节码操作框架。它可以用于直接以二进制形式动态生成stub根类或其他代理类，或者在加载时动态修改类。 CGLIB(Code Generation Library)：是一个功能强大，高性能和高质量的代码生成库，用于扩展JAVA类并在运行时实现接口。 Javassist：是Java的加载时反射系统，它是一个用于在Java中编辑字节码的类库; 它使Java程序能够在运行时定义新类，并在JVM加载之前修改类文件。 为了让生成的代理类与目标对象（真实主题角色）保持一致性，从现在开始将介绍以下两种最常见的方式： 通过实现接口的方式 -&gt; JDK动态代理通过继承类的方式 -&gt; CGLIB动态代理 注：使用ASM对使用者要求比较高，使用Javassist会比较麻烦 JDK动态代理JDK动态代理主要涉及两个类：java.lang.reflect.Proxy 和java.lang.reflect.InvocationHandler，我们仍然通过案例来学习. 编写一个调用逻辑处理器 LogHandler 类，提供日志增强功能，并实现 InvocationHandler 接口；在 LogHandler 中维护一个目标对象，这个对象是被代理的对象（真实主题角色）；在 invoke 方法中编写方法调用的逻辑处理 123456789101112131415161718192021222324252627282930import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.util.Date;public class LogHandler implements InvocationHandler &#123; Object target; // 被代理的对象，实际的方法执行者 public LogHandler(Object target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); // 调用 target 的 method 方法 Object result = method.invoke(target, args); after(); return result; // 返回方法的执行结果 &#125; // 调用invoke方法之前执行 private void before() &#123; System.out.printf("log start time [%s] \n", new Date()); &#125; // 调用invoke方法之后执行 private void after() &#123; System.out.printf("log end time [%s] \n", new Date()); &#125;&#125; 编写客户端，获取动态生成的代理类的对象须借助 Proxy 类的 newProxyInstance 方法: 123456789101112131415161718192021222324252627282930313233343536import proxy.UserService;import proxy.UserServiceImpl;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class UserClientJdkDynamic &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException &#123; // 设置变量可以保存动态代理类，默认名称以 $Proxy0 格式命名 // System.getProperties().setProperty("sun.misc.ProxyGenerator.saveGeneratedFiles", "true"); // 1. 创建被代理的对象，UserService接口的实现类 UserServiceImpl userServiceImpl = new UserServiceImpl(); // 2. 获取对应的 ClassLoader ClassLoader classLoader = userServiceImpl.getClass().getClassLoader(); // 3. 获取所有接口的Class，这里的UserServiceImpl只实现了一个接口UserService， Class[] interfaces = userServiceImpl.getClass().getInterfaces(); // 4. 创建一个将传给代理类的调用请求处理器，处理所有的代理对象上的方法调用 // 这里创建的是一个自定义的日志处理器，须传入实际的执行对象 userServiceImpl InvocationHandler logHandler = new LogHandler(userServiceImpl); /* 5.根据上面提供的信息，创建代理对象 在这个过程中， a.JDK会通过根据传入的参数信息动态地在内存中创建和.class 文件等同的字节码 b.然后根据相应的字节码转换成对应的class， c.然后调用newInstance()创建代理实例 */ UserService proxy = (UserService) Proxy.newProxyInstance(classLoader, interfaces, logHandler); // 调用代理的方法 proxy.select(); proxy.update(); // 6. 保存JDK动态代理生成的代理类，类名保存为 UserServiceProxy ProxyUtils.generateClassFile(userServiceImpl.getClass(), "UserServiceProxy2"); &#125;&#125; 运行结果 123456log start time [Thu Dec 20 16:55:19 CST 2018] 查询 selectByIdlog end time [Thu Dec 20 16:55:19 CST 2018] log start time [Thu Dec 20 16:55:19 CST 2018] 更新 updatelog end time [Thu Dec 20 16:55:19 CST 2018] InvocationHandler和Proxy核心方法介绍java.lang.reflect.InvocationHandlerObject invoke(Object proxy, Method method, Object[] args) 定义了代理对象调用方法时希望执行的动作，用于集中处理在动态代理类对象上的方法调用 java.lang.reflect.Proxystatic InvocationHandler getInvocationHandler(Object proxy) 用于获取指定代理对象所关联的调用处理器 static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces) 返回指定接口的代理类 static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 构造实现指定接口的代理类的一个新实例，所有方法会调用给定处理器对象的 invoke 方法 static boolean isProxyClass(Class&lt;?&gt; cl) 返回 cl 是否为一个代理类 代理类JDK自动生成的代理类到底长什么样子呢？借助上面第6步操作，可以把代理类保存下来一探究竟， target 的类路径下找到 UserServiceProxy2.class，双击后IDEA的反编译插件会将该二进制class文件转变成java文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import proxy.UserService;public final class UserServiceProxy2 extends Proxy implements UserService &#123; private static Method m1; private static Method m2; private static Method m4; private static Method m0; private static Method m3; public UserServiceProxy2(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; // 省略... &#125; public final String toString() throws &#123; // 省略... &#125; public final void select() throws &#123; try &#123; super.h.invoke(this, m4, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; // 省略... &#125; public final void update() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName("java.lang.Object").getMethod("equals", Class.forName("java.lang.Object")); m2 = Class.forName("java.lang.Object").getMethod("toString"); m4 = Class.forName("proxy.UserService").getMethod("select"); m0 = Class.forName("java.lang.Object").getMethod("hashCode"); m3 = Class.forName("proxy.UserService").getMethod("update"); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 从 UserServiceProxy 的代码中我们可以发现： UserServiceProxy2 继承了 Proxy 类，并且实现了被代理的所有接口，以及equals、hashCode、toString等方法 由于UserServiceProxy2继承了 Proxy 类，所以每个代理类都会关联一个 InvocationHandler 方法调用处理器 类和所有方法都被 public final 修饰，所以代理类只可被使用，不可以再被继承 每个方法都有一个 Method 对象来描述，Method 对象在static静态代码块中创建，以 m+ 数字 的格式命名 调用方法的时候通过 super.h.invoke(this, m1, (Object[])null) 调用，其中的 super.h.invoke 实际上是在创建代理的时候传递给 Proxy.newProxyInstance 的 LogHandler 对象，它继承 InvocationHandler 类，负责实际的调用处理逻辑 而 LogHandler 的 invoke 方法接收到 method、args 等参数后，进行一些处理，然后通过反射让被代理的对象 target 执行方法 12345678@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); // 调用 target 的 method 方法 Object result = method.invoke(target, args); after(); return result; // 返回方法的执行结果&#125; JDK动态代理执行方法调用的过程简图如下： CGLIB动态代理maven引入CGLIB包，然后编写一个UserDao类，它没有接口，只有两个方法，select() 和 update() 12345678public class UserDao &#123; public void select() &#123; System.out.println("UserDao 查询 selectById"); &#125; public void update() &#123; System.out.println("UserDao 更新 update"); &#125;&#125; 编写一个 LogInterceptor ，继承了 MethodInterceptor，用于方法的拦截回调 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class LogInterceptor implements MethodInterceptor &#123; private Object target; //目标类 public LogInterceptor(Object target) &#123; this.target = target; &#125; /** * 返回代理对象 * 具体实现，暂时先不追究。 */ public Object createProxy() &#123; Enhancer enhancer = new Enhancer(); enhancer.setCallback(this); // 回调函数 拦截器 // 设置代理对象的父类,可以看到代理对象是目标对象的子类。所以这个接口类就可以省略了 enhancer.setSuperclass(this.target.getClass()); return enhancer.create(); &#125; /** * @param object 表示要进行增强的对象 * @param method 表示拦截的方法 * @param objects 数组表示参数列表，基本数据类型需要传入其包装类型，如int--&gt;Integer、long-Long、double--&gt;Double * @param methodProxy 表示对方法的代理，invokeSuper方法表示对被代理对象方法的调用 * * @return 执行结果 * * @throws Throwable */ public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; before(); // 注意这里是调用 invokeSuper 而不是 invoke，否则死循环， // methodProxy.invokesuper执行的是原始类的方法，method.invoke执行的是子类的方法 Object result = methodProxy.invokeSuper(object, objects); after(); return result; &#125; private void before() &#123; System.out.println(String.format("log start time [%s] ", new Date())); &#125; private void after() &#123; System.out.println(String.format("log end time [%s] ", new Date())); &#125;&#125; 测试： 12345678910111213public class CglibClient &#123; public static void main(String[] args)&#123; //目标对象 UserDao userDao = new UserDao(); LogInterceptor interceptor = new LogInterceptor(userDao); // 代理对象，调用cglib系统方法自动生成 // 注意：代理类是目标类的子类。 UserDao proxy = (UserDao) interceptor.createProxy(); proxy.select(); proxy.update(); &#125;&#125; 结果： 123456log start time [Fri Nov 29 10:51:17 CST 2019] UserDao 查询 selectByIdlog end time [Fri Nov 29 10:51:17 CST 2019] log start time [Fri Nov 29 10:51:17 CST 2019] UserDao 更新 updatelog end time [Fri Nov 29 10:51:17 CST 2019] CGLIB 创建动态代理类的模式是： 查找目标类上的所有非final 的public类型的方法定义； 将这些方法的定义转换成字节码； 将组成的字节码转换成相应的代理的class对象； 实现 MethodInterceptor 接口，用来处理对代理类上所有方法的请求 JDK与CGLIB动态代理对比 JDK动态代理：基于Java反射机制实现，必须要实现了接口的业务类才能用这种办法生成代理对象。 Cglib动态代理：基于ASM机制实现，通过生成业务类的子类作为代理类。 JDK动态代理优势 最小化依赖关系，减少依赖意味着简化开发和维护，JDK 本身的支持，可能比 cglib 更加可靠。 平滑进行 JDK 版本升级，而字节码类库通常需要进行更新以保证在新版 Java 上能够使用。 代码实现简单。 基于类似 cglib 框架的优势 无需实现接口，达到代理类无侵入 只操作我们关心的类，而不必为其他相关类增加工作量。 高性能 参考[1] Java 动态代理详解 [2] Java动态代理 [3] Java反射机制详解 [4] 从代理模式再出发！Proxy.newProxyInstance的秘密 [5] JDK动态代理实现原理(jdk8) 附件文中代码： https://github.com/austin-brant/dynamic-proxy-demo]]></content>
      <categories>
        <category>Java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤器(Bloom Filter)]]></title>
    <url>%2F2019%2F11%2F27%2F%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-Bloom-Filter%2F</url>
    <content type="text"><![CDATA[概念布隆过滤器（英语：Bloom Filter）是1970年由一个叫布隆的小伙子提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。 优点： 空间效率和查询时间都远远超过一般的算法 缺点： 有一定的误识别率和删除困难。 原理布隆过滤器的原理是，当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了： 如果这些点有任何一个0，则被检元素一定不在； 如果都是1，则被检元素很可能在 这就是布隆过滤器的基本思想。 Bloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。 缓存穿透 每次查询都会直接打到DB 简而言之，言而简之就是我们先把我们数据库的数据都加载到我们的过滤器中，比如数据库的id现在有：1、2、3 那就用id：1 为例子, 他在上图中经过三次hash之后，把三次原本值0的地方改为1下次数据进来查询的时候如果id的值是1，那么我就把1拿去三次hash 发现三次hash的值，跟上面的三个位置完全一样，那就能证明过滤器中有1的, 反之如果不一样就说明不存在了 那应用的场景在哪里呢？一般我们都会用来防止缓存击穿 简单来说就是你数据库的id都是1开始然后自增的，那我知道你接口是通过id查询的，我就拿负数去查询，这个时候，会发现缓存里面没这个数据，我又去数据库查也没有，一个请求这样，100个，1000个，10000个呢？你的DB基本上就扛不住了，如果在缓存里面加上这个，是不是就不存在了，你判断没这个数据就不去查了，直接return一个数据为空不就好了嘛。 Bloom Filter缺点bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性 存在误判，可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。如果bloom filter中存储的是黑名单，那么可以通过建立一个白名单来存储可能会误判的元素。 删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。可以采用Counting Bloom Filter Guava本地实现布隆过滤器有许多实现与优化，Guava中就提供了一种Bloom Filter的实现。 在使用bloom filter时，绕不过的两点是预估数据量n 以及 期望的误判率fpp， 在实现bloom filter时，绕不过的两点就是hash函数的选取 以及 bit数组的大小。 对于一个确定的场景，我们预估要存的数据量为n，期望的误判率为fpp，然后需要计算我们需要的Bit数组的大小m，以及hash函数的个数k，并选择hash函数 Bit数组大小选择根据预估数据量n以及误判率fpp，bit数组大小的m的计算方式： 哈希函数选择​由预估数据量n以及bit数组长度m，可以得到一个hash函数的个数k：​​​​哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。 哈希函数个数k、位数组大小m、加入的字符串数量n的关系可以参考Bloom Filters - the math，Bloom_filter-wikipedia 要使用BloomFilter，需要引入guava包： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;23.0&lt;/version&gt;&lt;/dependency&gt; 测试分两步： 1、往过滤器中放一百万个数，然后去验证这一百万个数是否能通过过滤器 2、另外找一万个数，去检验漏网之鱼的数量 1234567891011121314151617181920212223242526272829303132333435/** * 测试布隆过滤器(可用于redis缓存穿透) * * @author 敖丙 */public class TestBloomFilter &#123; private static int total = 1000000; private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total);// private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.001); public static void main(String[] args) &#123; // 初始化1000000条数据到过滤器中 for (int i = 0; i &lt; total; i++) &#123; bf.put(i); &#125; // 匹配已在过滤器中的值，是否有匹配不上的 for (int i = 0; i &lt; total; i++) &#123; if (!bf.mightContain(i)) &#123; System.out.println("有坏人逃脱了~~~"); &#125; &#125; // 匹配不在过滤器中的10000个值，有多少匹配出来 int count = 0; for (int i = total; i &lt; total + 10000; i++) &#123; if (bf.mightContain(i)) &#123; count++; &#125; &#125; System.out.println("误伤的数量：" + count); &#125;&#125; 运行结果： 运行结果表示，遍历这一百万个在过滤器中的数时，都被识别出来了。一万个不在过滤器中的数，误伤了320个，错误率是0.03左右。 看下BloomFilter的源码： 1234567891011121314151617public static &lt;T&gt; BloomFilter&lt;T&gt; create(Funnel&lt;? super T&gt; funnel, int expectedInsertions) &#123; return create(funnel, (long) expectedInsertions);&#125; public static &lt;T&gt; BloomFilter&lt;T&gt; create(Funnel&lt;? super T&gt; funnel, long expectedInsertions) &#123; return create(funnel, expectedInsertions, 0.03); // FYI, for 3%, we always get 5 hash functions&#125;public static &lt;T&gt; BloomFilter&lt;T&gt; create( Funnel&lt;? super T&gt; funnel, long expectedInsertions, double fpp) &#123; return create(funnel, expectedInsertions, fpp, BloomFilterStrategies.MURMUR128_MITZ_64);&#125;static &lt;T&gt; BloomFilter&lt;T&gt; create( Funnel&lt;? super T&gt; funnel, long expectedInsertions, double fpp, Strategy strategy) &#123; ......&#125; BloomFilter一共四个create方法，不过最终都是走向第四个。看一下每个参数的含义： funnel：数据类型(一般是调用Funnels工具类中的) expectedInsertions：期望插入的值的个数 fpp 错误率(默认值为0.03) strategy 哈希算法(我也不懂啥意思)Bloom Filter的应用 在最后一个create方法中，设置一个断点： 上面的numBits，表示存一百万个int类型数字，需要的位数为7298440，700多万位。理论上存一百万个数，一个int是4字节32位，需要481000000=3200万位。如果使用HashMap去存，按HashMap50%的存储效率，需要6400万位。可以看出BloomFilter的存储空间很小，只有HashMap的1/10左右 上面的numHashFunctions，表示需要5个函数去存这些数字 使用第三个create方法，我们设置下错误率： 1private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.0003); 再运行看看： 此时误伤的数量为4，错误率为0.04%左右。 当错误率设为0.0003时，所需要的位数为16883499，1600万位，需要12个函数和上面对比可以看出，错误率越大，所需空间和时间越小，错误率越小，所需空间和时间越大。 Redis实现RedisBloom实现Redis的布隆过滤器不是原生自带的，而是要通过module加载进去。Redis在4.0的版本中加入了module功能。 RedisBloom github 主页地址： https://github.com/RedisBloom/RedisBloom RedisBloom客户端 主页地址： https://github.com/RedisBloom/JRedisBloom 上面有docker一键启动命令，可以很方便地实验。也有几种主流语言的客户端库的链接，比如Java语言的JReBloom。 RedisBloom模块还实现了布谷鸟过滤器，它算是对布隆过滤器的增强版。解决了布隆过滤器的一些比较明显的缺点，比如：不能删除元素，不能计数等。除此之外，布谷鸟过滤器不用使用多个hash函数，所以查询性能更高。除此之外，在相同的误判率下，布谷鸟过滤器的空间利用率要明显高于布隆，空间上大概能节省40%多。 安装Rebloom插件 1 下载并编译 123$ git clone git://github.com/RedisLabsModules/rebloom$ cd rebloom$ make 将Rebloom加载到Redis中，在redis.conf里面添加 1loadmodule /path/to/rebloom.so 命令操作 123BF.ADD bloom redisBF.EXISTS bloom redisBF.EXISTS bloom nonxist 命令行加载rebloom插件,并且设定每个bloomfilter key的容量和错误率： 123cd /usr/redis-4.0.11# 容量100万, 容错率万分之一./src/redis-server redis.conf --loadmodule /usr/rebloom/rebloom.so INITIAL_SIZE 1000000 ERROR_RATE 0.0001 java-lua版操作(java代码不提供了，自己把脚本执行就行) bloomFilterAdd.lua 123456local bloomName = KEYS[1]local value = KEYS[2]-- bloomFilterlocal result_1 = redis.call('BF.ADD', bloomName, value)return result_1 bloomFilterExist.lua 123456local bloomName = KEYS[1]local value = KEYS[2]-- bloomFilterlocal result_1 = redis.call('BF.EXISTS', bloomName, value)return result_1 Bitmap简单实现-原理版Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32 长度的位数组。 位操作方法可以被分为两组：一、对单一位的操作，比如设置某一位为1或0，或者得到这一位的值；二、对一组位的操作，比方说计算一定范围内的1的个数（比如计数） bitmap一个最大的优势是它通常能在存储信息的时候节省大量空间。比方说一个用增量ID来辨别用户的系统，可以用仅仅512MB的空间来标识40亿个用户是否想要接受通知。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class BloomFilter &#123; public static double size = Math.pow(2, 32); //第一次加载的时候将数据加载到redis中 public static void saveDataToRedis(Jedis jedis) &#123; List&lt;String&gt; baseList = new ArrayList&lt;String&gt;(); baseList.add("000"); baseList.add("111"); baseList.add("222"); for (int i = 0; i &lt; baseList.size(); i++) &#123; // 只做一次hash，实际中会计算多个hash位置，降低误差率 long index = hashIndex(baseList.get(i)); jedis.setbit("orderId", index, true); &#125; &#125; private static long hashIndex(String target) &#123; return Math.abs((long) (target.hashCode() % size)); &#125; //判断传入的数据是否在redis中 public static boolean checkEleIsContainBloomFilter(String target, Jedis jedis) &#123; long index = hashIndex(target); System.out.println("index: " + index + " size: " + size); boolean checkResult = jedis.getbit("orderId", index); return checkResult; &#125; public static void main(String[] args) &#123; //获取redis链接 Jedis jedis = new Jedis("xxxx", 6379); jedis.auth("xxxxx"); //第一次运行的时候调用，只运行一次 saveDataToRedis(jedis); //获取比较后的值 System.out.println(checkEleIsContainBloomFilter("000", jedis)); //释放redis链接 jedis.close(); &#125;&#125; 常见应用场景 cerberus在收集监控数据的时候, 有的系统的监控项量会很大, 需要检查一个监控项的名字是否已经被记录到db过了, 如果没有的话就需要写入db. 爬虫过滤已抓到的url就不再抓，可用bloom filter过滤 垃圾邮件过滤。如果用哈希表，每存储一亿个 email地址，就需要 1.6GB的内存（用哈希表实现的具体办法是将每一个 email地址对应成一个八字节的信息指纹，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email地址需要占用十六个字节。一亿个地址大约要 1.6GB，即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB的内存。而Bloom Filter只需要哈希表 1/8到 1/4 的大小就能解决同样的问题。 参考[1] Redis-避免缓存穿透的利器之BloomFilter [2] Java redis 模拟布隆过滤器 [3] redis-分布式布隆过滤器（Bloom Filter）详解（初版）]]></content>
      <categories>
        <category>基础知识</category>
        <category>Bloom</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>布隆过滤器</tag>
        <tag>Bloom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis过期键处理策略]]></title>
    <url>%2F2019%2F11%2F22%2FRedis%E8%BF%87%E6%9C%9F%E9%94%AE%E5%A4%84%E7%90%86%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Redis Expire Key基础redis数据库在数据库服务器中使用了 redisDb 数据结构，结构如下： 12345678910typedef struct redisDb &#123; dict *dict; /* 键空间 key space */ dict *expires; /* 过期字典 */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP) */ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ struct evictionPoolEntry *eviction_pool; /* Eviction pool of keys */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */&#125; redisDb; 其中: 键空间(key space):dict字典用来保存数据库中的所有键值对 过期字典(expires):保存数据库中所有键的过期时间，过期时间用UNIX时间戳表示，且值为long long整数 设置过期时间命令 EXPIRE &lt;key&gt; &lt;ttl&gt; 命令用于将键key的过期时间设置为ttl秒之后 PEXPIRE &lt;key&gt; &lt;ttl&gt; 命令用于将键key的过期时间设置为ttl毫秒之后 EXPIREAT &lt;key&gt; &lt;timesramp&gt; 命令用于将key的过期时间设置为timrestamp所指定的秒数时间戳 PEXPIREAT &lt;key&gt; &lt;timesramp&gt; 命令用于将key的过期时间设置为timrestamp所指定的毫秒数时间戳 设置过期时间： 1234redis&gt; set Ccww 5 2 0 ok redis&gt; expire Ccww 5 ok 使用redisDb结构存储数据图表示： 过期时间保存以及判定过期键的判定，其实通过过期字典进行判定，步骤： 检查给定键是否存在于过期字典，如果存在，取出键的过期时间 通过判断当前UNIX时间戳是否大于键的过期时间，是的话，键已过期，相反则键未过期。 过期键删除策略定时删除在设置键的过期时间的同时，创建一个定时任务，当键达到过期时间时，立即执行对键的删除操作. 优点对内存友好，定时删除策略可以保证过期键会尽可能快地被删除，并释放国期间所占用的内存 缺点对cpu时间不友好，在过期键比较多时，删除任务会占用很大一部分cpu时间，在内存不紧张但cpu时间紧张的情况下，将cpu时间用在删除和当前任务无关的过期键上，影响服务器的响应时间和吞吐量 惰性删除放任键过期不管，但在每次从键空间获取键时，都检查取得的键是否过期，如果过期的话，就删除该键，如果没有过期，就返回该键 优点对cpu时间友好，在每次从键空间获取键时进行过期键检查并是否删除，删除目标也仅限当前处理的键，这个策略不会在其他无关的删除任务上花费任何cpu时间。 缺点对内存不友好，过期键过期也可能不会被删除，导致所占的内存也不会释放。甚至可能会出现内存泄露的现象，当存在很多过期键，而这些过期键又没有被访问到，这会可能导致它们会一直保存在内存中，造成内存泄露。 定期删除由于定时删除会占用太多cpu时间，影响服务器的响应时间和吞吐量, 而惰性删除浪费太多内存，有内存泄露的危险，所以出现一种整合和折中这两种策略的定期删除策略: 定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响; 至于要删除多少过期键，以及要检查多少个数据库，则由算法决定； 定时删除策略有效地减少了因为过期键带来的内存浪费; 定时删除策略难点就是确定删除操作执行的时长和频率： 删除操作执行得太频繁。或者执行时间太长，定期删除策略就会退化成为定时删除策略，以至于将cpu时间过多地消耗在删除过期键上。 相反，则与惰性删除策略一样，出现浪费内存的情况。 所以使用定期删除策略，需要根据服务器的情况合理地设置删除操作的执行时长和执行频率。 过期键删除策略实现 Redis服务器结合惰性删除和定期删除两种策略一起使用，通过这两种策略之间的配合使用，使得服务器可以在合理使用CPU时间和浪费内存空间取得平衡点。 惰性删除策略的实现 Redis在执行任何读写命令时都会先找到这个key，惰性删除就作为一个切入点放在查找key之前，如果key过期了就删除这个key。 12345678910robj *lookupKeyRead(redisDb *db, robj *key) &#123; robj *val; expireIfNeeded(db,key); // 切入点 val = lookupKey(db,key); if (val == NULL) server.stat_keyspace_misses++; else server.stat_keyspace_hits++; return val;&#125; 通过expireIfNeeded函数对输入键进行检查是否删除: 123456789101112131415161718192021222324252627282930int expireIfNeeded(redisDb *db, robj *key) &#123; /* 取出键的过期时间 */ mstime_t when = getExpire(db,key); mstime_t now; /* 没有过期时间返回0*/ if (when &lt; 0) return 0; /* No expire for this key */ /* 服务器loading时*/ if (server.loading) return 0; /* 根据一定规则获取当前时间*/ now = server.lua_caller ? server.lua_time_start : mstime(); /* 如果当前的是从(Slave)服务器 * 0 认为key为无效 * 1 if we think the key is expired at this time. * */ if (server.masterhost != NULL) return now &gt; when; /* key未过期，返回 0 */ if (now &lt;= when) return 0; /* 删除键 */ server.stat_expiredkeys++; propagateExpire(db,key,server.lazyfree_lazy_expire); notifyKeyspaceEvent(NOTIFY_EXPIRED, "expired",key,db-&gt;id); return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) : dbSyncDelete(db,key);&#125; 定期删除策略的实现 key的定期删除会在Redis的周期性执行任务（serverCron，默认每100ms执行一次）中进行，而且是发生Redis的master节点，因为slave节点会通过主节点的DEL命令同步过来达到删除key的目的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465for (j = 0; j &lt; dbs_per_call; j++) &#123; int expired; redisDb *db = server.db+(current_db % server.dbnum); current_db++; /* 超过25％的key已过期，则继续. */ do &#123; unsigned long num, slots; long long now, ttl_sum; int ttl_samples; /* 如果该db没有设置过期key，则继续看下个db*/ if ((num = dictSize(db-&gt;expires)) == 0) &#123; db-&gt;avg_ttl = 0; break; &#125; slots = dictSlots(db-&gt;expires); now = mstime(); /*但少于1%时，需要调整字典大小*/ if (num &amp;&amp; slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (num*100/slots &lt; 1)) break; expired = 0; ttl_sum = 0; ttl_samples = 0; if (num &gt; ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP) num = ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP;// 20 while (num--) &#123; dictEntry *de; long long ttl; if ((de = dictGetRandomKey(db-&gt;expires)) == NULL) break; ttl = dictGetSignedIntegerVal(de)-now; if (activeExpireCycleTryExpire(db,de,now)) expired++; if (ttl &gt; 0) &#123; /* We want the average TTL of keys yet not expired. */ ttl_sum += ttl; ttl_samples++; &#125; &#125; /* Update the average TTL stats for this database. */ if (ttl_samples) &#123; long long avg_ttl = ttl_sum/ttl_samples; /样本获取移动平均值 */ if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl; db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50); &#125; iteration++; if ((iteration &amp; 0xf) == 0) &#123; /* 每迭代16次检查一次 */ long long elapsed = ustime()-start; latencyAddSampleIfNeeded("expire-cycle",elapsed/1000); if (elapsed &gt; timelimit) timelimit_exit = 1; &#125; /* 超过时间限制则退出*/ if (timelimit_exit) return; /* 在当前db中，如果少于25%的key过期，则停止继续删除过期key */ &#125; while (expired &gt; ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4);&#125; 依次遍历每个db（默认配置数是16），针对每个db，每次循环随机选择20个（ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP）key判断是否过期，如果一轮所选的key少于25%过期，则终止迭代，此外在迭代过程中如果超过了一定的时间限制则终止过期删除这一过程。 Redis采用的过期策略redis 过期策略是：定期删除 + 惰性删除 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。所以，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就需要结合惰性删除。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？ 答案是：走内存淘汰机制 内存淘汰机制redis 内存淘汰机制有以下几个： noeviction 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了 allkeys-lru 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）； allkeys-random 当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊； volatile-lru 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）； volatile-random 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key； volatile-ttl 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除； 设置方式： 1config set maxmemory-policy volatile-lru AOF、RDB和复制功能对过期键的处理RDB 生成RDB文件程序会数据库中的键进行检查，已过期的键不会保存到新创建的RDB文件中 载入RDB文件 主服务载入RDB文件，会对文件中保存的键进行检查会忽略过期键加载未过期键 从服务器载入RDB文件，会加载文件所保存的所有键（过期和未过期的），但从主服务器同步数据同时会清空从服务器的数据库。 AOF AOF文件写入当过期键被删除后，会在AOF文件增加一条DEL命令，来显式地记录该键已被删除。 AOF重写已过期的键不会保存到重写的AOF文件中 复制 当服务器运行在复制模式下时，从服务器的过期键删除动作由主服务器控制的，这样的好处主要为了保持主从服务器数据一致性： 主服务器在删除一个过期键之后，会显式地向所有的从服务器发送一个DEL命令，告知从服务器删除这个过期键； 从服务器在执行客户端发送的读取命令时，即使碰到过期键也不会将过期键删除，不作任何处理。只有接收到主服务器 DEL命令后，从服务器进行删除处理。 参考文档[1] 当遇到美女面试官之如何理解Redis的Expire Key(过期键) [2] Redis的过期策略及内存淘汰机制]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>入门</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty性能优化]]></title>
    <url>%2F2019%2F11%2F20%2FNetty%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[共享Handler代码：https://github.com/austin-brant/netty-im 在使用 Netty 完成了一个 IM 系统的核心功能之后，我们再来仔细看一下服务端 NettyServer.java 1234567891011121314151617serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new AuthHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new CreateGroupRequestHandler()); ch.pipeline().addLast(new JoinGroupRequestHandler()); ch.pipeline().addLast(new QuitGroupRequestHandler()); ch.pipeline().addLast(new ListGroupMembersRequestHandler()); ch.pipeline().addLast(new GroupMessageRequestHandler()); ch.pipeline().addLast(new LogoutRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125; &#125;); 我们看到，服务端的 pipeline 链里面已经有 12 个 handler，其中，与指令相关的 handler 有 9 个。 Netty 在这里的逻辑是：每次有新连接到来的时候，都会调用 ChannelInitializer 的 initChannel() 方法，然后这里 9 个指令相关的 handler 都会被 new 一次。 其实这里的每一个指令 handler，他们内部都是没有成员变量的，也就是说是无状态的，我们完全可以使用单例模式，即调用 pipeline().addLast() 方法的时候，都直接使用单例，不需要每次都 new，提高效率，也避免了创建很多小的对象。 比如，我们拿 LoginRequestHandler 举例，来看一下如何改造 LoginRequestHandler.java 1234567891011// 1. 加上注解标识，表明该 handler 是可以多个 channel 共享的@ChannelHandler.Sharablepublic class LoginRequestHandler extends SimpleChannelInboundHandler&lt;LoginRequestPacket&gt; &#123; // 2. 构造单例 public static final LoginRequestHandler INSTANCE = new LoginRequestHandler(); protected LoginRequestHandler() &#123; &#125;&#125; 首先，非常重要的一点，如果一个 handler 要被多个 channel 进行共享，必须要加上 @ChannelHandler.Sharable 显示地告诉 Netty，这个 handler 是支持多个 channel 共享的，否则会报错，读者可以自行尝试一下。 然后，我们仿照 Netty 源码里面单例模式的写法，构造一个单例模式的类。 接着，我们在服务端的代理里面就可以这么写 NettyServer.java 12345678serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; // ...单例模式，多个 channel 共享同一个 handler ch.pipeline().addLast(LoginRequestHandler.INSTANCE); // ... &#125; &#125;); 这样的话，每来一次新的连接，添加 handler 的时候就不需要每次都 new 了。 压缩 handler - 合并编解码器当我们改造完了之后，我们再来看一下服务端代码 NettyServer.java 1234567891011121314151617serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(MessageRequestHandler.INSTANCE); ch.pipeline().addLast(CreateGroupRequestHandler.INSTANCE); ch.pipeline().addLast(JoinGroupRequestHandler.INSTANCE); ch.pipeline().addLast(QuitGroupRequestHandler.INSTANCE); ch.pipeline().addLast(ListGroupMembersRequestHandler.INSTANCE); ch.pipeline().addLast(GroupMessageRequestHandler.INSTANCE); ch.pipeline().addLast(LogoutRequestHandler.INSTANCE); ch.pipeline().addLast(new PacketEncoder()); &#125; &#125;); pipeline 中第一个 handler - Spliter，我们是无法改动它的，因为他内部实现是与每个 channel 有关，每个 Spliter 需要维持每个 channel 当前读到的数据，也就是说他是有状态的。 而 PacketDecoder 与 PacketEncoder 我们是可以继续改造的，Netty 内部提供了一个类，叫做 MessageToMessageCodec，使用它可以让我们的编解码操作放到一个类里面去实现，首先我们定义一个 PacketCodecHandler: PacketCodecHandler.java 1234567891011121314151617181920@ChannelHandler.Sharablepublic class PacketCodecHandler extends MessageToMessageCodec&lt;ByteBuf, Packet&gt; &#123; public static final PacketCodecHandler INSTANCE = new PacketCodecHandler(); private PacketCodecHandler() &#123; &#125; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf byteBuf, List&lt;Object&gt; out) &#123; out.add(PacketCodec.INSTANCE.decode(byteBuf)); &#125; @Override protected void encode(ChannelHandlerContext ctx, Packet packet, List&lt;Object&gt; out) &#123; ByteBuf byteBuf = ctx.channel().alloc().ioBuffer(); PacketCodec.INSTANCE.encode(byteBuf, packet); out.add(byteBuf); &#125;&#125; 首先，这里 PacketCodecHandler，他是一个无状态的 handler，因此，同样可以使用单例模式来实现。 需要实现 decode() 和 encode() 方法，decode 是将二进制数据 ByteBuf 转换为 java 对象 Packet，而 encode 操作是一个相反的过程，在 encode() 方法里面，我们调用了 channel 的 内存分配器手工分配了 ByteBuf。 接着，PacketDecoder 和 PacketEncoder都可以删掉，我们的 server 端代码就成了如下的样子 12345678910111213141516serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(PacketCodecHandler.INSTANCE); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(MessageRequestHandler.INSTANCE); ch.pipeline().addLast(CreateGroupRequestHandler.INSTANCE); ch.pipeline().addLast(JoinGroupRequestHandler.INSTANCE); ch.pipeline().addLast(QuitGroupRequestHandler.INSTANCE); ch.pipeline().addLast(ListGroupMembersRequestHandler.INSTANCE); ch.pipeline().addLast(GroupMessageRequestHandler.INSTANCE); ch.pipeline().addLast(LogoutRequestHandler.INSTANCE); &#125; &#125;); 可以看到，除了拆包器，所有的 handler 都写成了单例，当然，如果你的 handler 里有与 channel 相关成员变量，那就不要写成单例的，不过，其实所有的状态都可以绑定在 channel 的属性上，依然是可以改造成单例模式。 缩短事件传播路径如果我们再仔细观察我们的服务端代码，发现，我们的 pipeline 链中，绝大部分都是与指令相关的 handler，我们把这些 handler 编排在一起，是为了逻辑简洁，但是随着指令相关的 handler 越来越多，handler 链越来越长，在事件传播过程中性能损耗会被逐渐放大，因为解码器解出来的每个 Packet 对象都要在每个 handler 上经过一遍，我们接下来来看一下如何缩短这个事件传播的路径。 压缩handler - 合并平行handler对我们这个应用程序来说，每次 decode 出来一个指令对象之后，其实只会在一个指令 handler 上进行处理，因此，我们其实可以把这么多的指令 handler 压缩为一个 handler，我们来看一下如何实现 我们定义一个 IMHandler，实现如下： IMHandler.java 1234567891011121314151617181920212223@ChannelHandler.Sharablepublic class IMHandler extends SimpleChannelInboundHandler&lt;Packet&gt; &#123; public static final IMHandler INSTANCE = new IMHandler(); private Map&lt;Byte, SimpleChannelInboundHandler&lt;? extends Packet&gt;&gt; handlerMap; private IMHandler() &#123; handlerMap = new HashMap&lt;&gt;(); handlerMap.put(MESSAGE_REQUEST, MessageRequestHandler.INSTANCE); handlerMap.put(CREATE_GROUP_REQUEST, CreateGroupRequestHandler.INSTANCE); handlerMap.put(JOIN_GROUP_REQUEST, JoinGroupRequestHandler.INSTANCE); handlerMap.put(QUIT_GROUP_REQUEST, QuitGroupRequestHandler.INSTANCE); handlerMap.put(LIST_GROUP_MEMBERS_REQUEST, ListGroupMembersRequestHandler.INSTANCE); handlerMap.put(GROUP_MESSAGE_REQUEST, GroupMessageRequestHandler.INSTANCE); handlerMap.put(LOGOUT_REQUEST, LogoutRequestHandler.INSTANCE); &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, Packet packet) throws Exception &#123; handlerMap.get(packet.getCommand()).channelRead(ctx, packet); &#125;&#125; 首先，IMHandler 是无状态的，依然是可以写成一个单例模式的类。 我们定义一个 map，存放指令到各个指令处理器的映射。 每次回调到 IMHandler 的 channelRead0() 方法的时候，我们通过指令找到具体的 handler，然后调用指令 handler 的 channelRead，他内部会做指令类型转换，最终调用到每个指令 handler 的 channelRead0() 方法。 接下来，我们来看一下，如此压缩之后，我们的服务端代码 NettyServer.java 12345678910serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(PacketCodecHandler.INSTANCE); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(IMHandler.INSTANCE); &#125; &#125;); 可以看到，现在，我们服务端的代码已经变得很清爽了，所有的平行指令处理 handler，我们都压缩到了一个 IMHandler，并且 IMHandler 和指令 handler 均为单例模式，在单机十几万甚至几十万的连接情况下，性能能得到一定程度的提升，创建的对象也大大减少了。 当然，如果你对性能要求没这么高，大可不必搞得这么复杂，还是按照我们前面小节的方式来实现即可，比如，我们的客户端多数情况下是单连接的，其实并不需要搞得如此复杂，还是保持原样即可。 更改事件传播源另外，关于缩短事件传播路径，除了压缩 handler，还有一个就是，如果你的 outBound 类型的 handler 较多，在写数据的时候能用 ctx.writeAndFlush() 就用这个方法。 ctx.writeAndFlush() 事件传播路径 ctx.writeAndFlush() 是从 pipeline 链中的 当前节点开始往前找到第一个 outBound 类型的 handler 把对象往前进行传播，如果这个对象确认不需要经过其他 outBound 类型的 handler 处理，就使用这个方法。 如上图，在某个 inBound 类型的 handler 处理完逻辑之后，调用 ctx.writeAndFlush() 可以直接一口气把对象送到 codec 中编码，然后写出去。 ctx.channel().writeAndFlush() 事件传播路径 ctx.channel().writeAndFlush() 是 从pipeline链中的最后一个outBound类型的 handler开始，把对象往前进行传播，如果你确认当前创建的对象需要经过后面的 outBound 类型的 handler，那么就调用此方法。 如上图，在某个 inBound 类型的 handler 处理完逻辑之后，调用 ctx.channel().writeAndFlush()，对象会从最后一个 outBound 类型的 handler 开始，逐个往前进行传播，路径是要比 ctx.writeAndFlush() 要长的。 由此可见，在我们的应用程序中，当我们没有改造编解码之前，我们必须调用 ctx.channel().writeAndFlush(), 而经过改造之后，我们的编码器（既属于 inBound, 又属于 outBound 类型的 handler）已处于 pipeline 的最前面，因此，可以大胆使用 ctx.writeAndFlush()。 减少阻塞主线程的操作通常我们的应用程序会涉及到数据库或者网络，比如以下这个例子 123456protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush() // 4. balabala 其他的逻辑&#125; 我们看到，在 channelRead0() 这个方法里面，第二个过程中，我们有一些耗时的操作，这个时候，我们万万不能将这个操作直接就在这个方法中处理了，为什么？ 默认情况下，Netty 在启动的时候会开启 2 倍的 cpu 核数个 NIO 线程，而通常情况下我们单机会有几万或者十几万的连接，因此，一条 NIO 线程会管理着几千或几万个连接，在传播事件的过程中，单条 NIO 线程的处理逻辑可以抽象成以下一个步骤，我们就拿 channelRead0() 举例 单个 NIO 线程执行的抽象逻辑 123456List&lt;Channel&gt; channelList = 已有数据可读的 channelfor (Channel channel in channelist) &#123; for (ChannelHandler handler in channel.pipeline()) &#123; handler.channelRead0(); &#125; &#125; 从上面的抽象逻辑中可以看到，其中只要有一个 channel 的一个 handler 中的 channelRead0() 方法阻塞了 NIO 线程，最终都会拖慢绑定在该 NIO 线程上的其他所有的 channel，当然，这里抽象的逻辑已经做了简化，想了解细节可以参考我关于 Netty 中 NIO 线程（即 reactor 线程）文章的分析， 「netty 源码分析之揭开 reactor 线程的面纱（一）」， 「netty 源码分析之揭开 reactor 线程的面纱（二）」， 「netty 源码分析之揭开 reactor 线程的面纱（三）」。 而我们需要怎么做？对于耗时的操作，我们需要把这些耗时的操作丢到我们的业务线程池中去处理，下面是解决方案的伪代码 12345678910ThreadPool threadPool = xxx;protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; threadPool.submit(new Runnable() &#123; // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush() // 4. balabala 其他的逻辑 &#125;)&#125; 这样，就可以避免一些耗时的操作影响 Netty 的 NIO 线程，从而影响其他的 channel。 如何准确统计处理时长通常，应用程序都有统计某个操作响应时间的需求，比如，基于我们上面的栗子，我们会这么做 12345678910protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; threadPool.submit(new Runnable() &#123; long begin = System.currentTimeMillis(); // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush() // 4. balabala 其他的逻辑 long time = System.currentTimeMillis() - begin; &#125;)&#125; 这种做法其实是不推荐的，为什么？ 因为 writeAndFlush() 这个方法如果在非NIO线程（这里，我们其实是在业务线程中调用了该方法）中执行，它是一个异步的操作，调用之后，其实是会立即返回的，剩下的所有的操作，都是 Netty 内部有一个任务队列异步执行的，想了解底层细节的可以阅读一下我的这篇文章 「netty 源码分析之 writeAndFlush 全解析」. 因此，这里的 writeAndFlush() 执行完毕之后，并不能代表相关的逻辑，比如事件传播、编码等逻辑执行完毕，只是表示 Netty 接收了这个任务，那么如何才能判断 writeAndFlush() 执行完毕呢？我们可以这么做 123456789101112131415protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; threadPool.submit(new Runnable() &#123; long begin = System.currentTimeMillis(); // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush xxx.writeAndFlush().addListener(future -&gt; &#123; if (future.isDone()) &#123; // 4. balabala 其他的逻辑 long time = System.currentTimeMillis() - begin; &#125; &#125;); &#125;)&#125; writeAndFlush() 方法会返回一个 ChannelFuture 对象，我们给这个对象添加一个监听器，然后在回调方法里面，我们可以监听这个方法执行的结果，进而再执行其他逻辑，最后统计耗时，这样统计出来的耗时才是最准确的。 最后，需要提出的一点就是，Netty 里面很多方法都是异步的操作，在业务线程中如果要统计这部分操作的时间，都需要使用监听器回调的方式来统计耗时，如果在 NIO 线程中调用，就不需要这么干。 参考摘自：Netty 入门与实战：仿写微信 IM 即时通讯系统 参考文章：[1] netty 源码分析之揭开 reactor 线程的面纱（一）[2] netty 源码分析之揭开 reactor 线程的面纱（二）[3] netty 源码分析之揭开 reactor 线程的面纱（三）[4] netty 源码分析之 writeAndFlush 全解析]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty心跳与空闲检测]]></title>
    <url>%2F2019%2F11%2F15%2FNetty%E5%BF%83%E8%B7%B3%E4%B8%8E%E7%A9%BA%E9%97%B2%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[代码：https://github.com/austin-brant/netty-im 网络问题下图是网络应用程序普遍会遇到的一个问题：连接假死 连接假死的现象是： 在某一端（服务端或者客户端）看来，底层的 TCP 连接已经断开了，但是应用程序并没有捕获到，因此会认为这条连接仍然是存在的，从 TCP 层面来说，只有收到四次握手数据包或者一个 RST 数据包，连接的状态才表示已断开。 连接假死会带来以下两大问题: 对于服务端来说，因为每条连接都会耗费 cpu 和内存资源，大量假死的连接会逐渐耗光服务器的资源，最终导致性能逐渐下降，程序奔溃。 对于客户端来说，连接假死会造成发送数据超时，影响用户体验。 通常，连接假死由以下几个原因造成的 应用程序出现线程堵塞，无法进行数据的读写。 客户端或者服务端网络相关的设备出现故障，比如网卡，机房故障。 公网丢包。公网环境相对内网而言，非常容易出现丢包，网络抖动等现象，如果在一段时间内用户接入的网络连续出现丢包现象，那么对客户端来说数据一直发送不出去，而服务端也是一直收不到客户端来的数据，连接就一直耗着。 如果我们的应用是面向用户的，那么公网丢包这个问题出现的概率是非常大的。对于内网来说，内网丢包，抖动也是会有一定的概率发生。一旦出现此类问题，客户端和服务端都会受到影响，接下来，我们分别从服务端和客户端的角度来解决连接假死的问题。 服务端空闲检测对于服务端来说，客户端的连接如果出现假死，那么服务端将无法收到客户端的数据，也就是说，如果能一直收到客户端发来的数据，那么可以说明这条连接还是活的，因此，服务端对于连接假死的应对策略就是空闲检测。 何为空闲检测？ 空闲检测指的是每隔一段时间，检测这段时间内是否有数据读写，简化一下，我们的服务端只需要检测一段时间内，是否收到过客户端发来的数据即可，Netty 自带的 IdleStateHandler 就可以实现这个功能。 接下来，我们写一个类继承自 IdleStateHandler，来定义检测到假死连接之后的逻辑。 IMIdleStateHandler.java 1234567891011121314public class IMIdleStateHandler extends IdleStateHandler &#123; private static final int READER_IDLE_TIME = 15; public IMIdleStateHandler() &#123; super(READER_IDLE_TIME, 0, 0, TimeUnit.SECONDS); &#125; @Override protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) &#123; System.out.println(READER_IDLE_TIME + "秒内未读到数据，关闭连接"); ctx.channel().close(); &#125;&#125; 首先，我们观察一下 IMIdleStateHandler 的构造函数，他调用父类 IdleStateHandler的构造函数，有四个参数，其中: 第一个表示读空闲时间，指的是在这段时间内如果没有数据读到，就表示连接假死； 第二个是写空闲时间，指的是 在这段时间如果没有写数据，就表示连接假死； 第三个参数是读写空闲时间，表示在这段时间内如果没有产生数据读或者写，就表示连接假死。写空闲和读写空闲为0，表示我们不关心者两类条件； 最后一个参数表示时间单位。在我们的例子中，表示的是：如果 15 秒内没有读到数据，就表示连接假死。 连接假死之后会回调 channelIdle() 方法，我们这个方法里面打印消息，并手动关闭连接。 接下来，我们把这个 handler 插入到服务端 pipeline 的最前面 NettyServer.java 123456789serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; // 空闲检测 ch.pipeline().addLast(new IMIdleStateHandler()); ch.pipeline().addLast(new Spliter()); // ... &#125; &#125;); 为什么要插入到最前面？ 因为如果插入到最后面的话，如果这条连接读到了数据，但是在 inBound 传播的过程中出错了或者数据处理完完毕就不往后传递了（我们的应用程序属于这类），那么最终 IMIdleStateHandler 就不会读到数据，最终导致误判。 服务端的空闲检测时间完毕之后，接下来我们再思考一下，在一段时间之内没有读到客户端的数据，是否一定能判断连接假死呢？并不能，如果在这段时间之内客户端确实是没有发送数据过来，但是连接是 ok 的，那么这个时候服务端也是不能关闭这条连接的，为了防止服务端误判，我们还需要在客户端做点什么。 客户端定时发送心跳服务端在一段时间内没有收到客户端的数据，这个现象产生的原因可以分为以下两种： 连接假死。 非假死状态下确实没有发送数据。 我们只需要排除掉第二种可能性，那么连接自然就是假死的。要排查第二种情况，我们可以在客户端定期发送数据到服务端，通常这个数据包称为心跳数据包，接下来，我们定义一个 handler，定期发送心跳给服务端 HeartBeatTimerHandler.java 12345678910111213141516171819public class HeartBeatTimerHandler extends ChannelInboundHandlerAdapter &#123; private static final int HEARTBEAT_INTERVAL = 5; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; scheduleSendHeartBeat(ctx); super.channelActive(ctx); &#125; private void scheduleSendHeartBeat(ChannelHandlerContext ctx) &#123; ctx.executor().scheduleAtFixedRate(() -&gt; &#123; if (ctx.channel().isActive()) &#123; System.out.println("发送心跳信息 : " + new Date()); ctx.writeAndFlush(new HeartBeatRequestPacket()); &#125; &#125;, 0, HEARTBEAT_INTERVAL, TimeUnit.SECONDS); &#125;&#125; ctx.executor() 返回的是当前的 channel 绑定的 NIO 线程，不理解没关系，只要记住就行，然后，NIO线程有一个方法，schedule()，类似 jdk 的延时任务机制，可以隔一段时间之后执行一个任务，而我们这边是实现了每隔 5 秒，向服务端发送一个心跳数据包，这个时间段通常要比服务端的空闲检测时间的一半要短一些，我们这里直接定义为空闲检测时间的三分之一，主要是为了排除公网偶发的秒级抖动。 实际在生产环境中，我们的发送心跳间隔时间和空闲检测时间可以略长一些，可以设置为几分钟级别，具体应用可以具体对待，没有强制的规定。 我们上面其实解决了服务端的空闲检测问题，服务端这个时候是能够在一定时间段之内关掉假死的连接，释放连接的资源了，但是对于客户端来说，我们也需要检测到假死的连接。 服务端回复心跳与客户端空闲检测客户端的空闲检测其实和服务端一样，依旧是在客户端 pipeline 的最前方插入 IMIdleStateHandler NettyClient.java 12345678bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; public void initChannel(SocketChannel ch) &#123; // 空闲检测 ch.pipeline().addLast(new IMIdleStateHandler()); ch.pipeline().addLast(new Spliter()); // ... &#125; &#125; 然后为了排除是否是因为服务端在非假死状态下确实没有发送数据，服务端也要定期发送心跳给客户端。 而其实在前面我们已经实现了客户端向服务端定期发送心跳，服务端这边其实只要在收到心跳之后回复客户端，给客户端发送一个心跳响应包即可。如果在一段时间之内客户端没有收到服务端发来的数据，也可以判定这条连接为假死状态。 因此，服务端的 pipeline 中需要再加上如下一个 handler - HeartBeatRequestHandler，由于这个 handler 的处理其实是无需登录的，所以，我们将该 handler 放置在 AuthHandler 前面 NettyServer.java 1234567891011serverBootstrap ch.pipeline().addLast(new IMIdleStateHandler()); ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(PacketCodecHandler.INSTANCE); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); // 加在这里 ch.pipeline().addLast(HeartBeatRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(IMHandler.INSTANCE); &#125; &#125;); HeartBeatRequestHandler 相应的实现为 1234567891011@ChannelHandler.Sharablepublic class HeartBeatRequestHandler extends SimpleChannelInboundHandler&lt;HeartBeatRequestPacket&gt; &#123; public static final HeartBeatRequestHandler INSTANCE = new HeartBeatRequestHandler(); private HeartBeatRequestHandler() &#123;&#125; @Override protected void channelRead0(ChannelHandlerContext ctx, HeartBeatRequestPacket requestPacket) &#123; ctx.writeAndFlush(new HeartBeatResponsePacket()); &#125;&#125; 实现非常简单，只是简单地回复一个 HeartBeatResponsePacket 数据包。客户端在检测到假死连接之后，断开连接，然后可以有一定的策略去重连，重新登录等等。 总结 首先讨论了连接假死相关的现象以及产生的原因 要处理假死问题首先我们要实现客户端与服务端定期发送心跳，在这里，其实服务端只需要对客户端的定时心跳包进行回复 客户端与服务端如果都需要检测假死，那么直接在 pipeline 的最前方插入一个自定义 IdleStateHandler，在 channelIdle() 方法里面自定义连接假死之后的逻辑 通常空闲检测时间要比发送心跳的时间的两倍要长一些，这也是为了排除偶发的公网抖动，防止误判 参考摘自：Netty 入门与实战：仿写微信 IM 即时通讯系统]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty ChannelHandler生命周期]]></title>
    <url>%2F2019%2F11%2F14%2FNetty-ChannelHandler%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[ChannelHandler有很多回调方法，这些回调方法的执行是有顺序的，而这个执行顺序可以称为 ChannelHandler 的生命周期。 代码：https://github.com/austin-brant/netty-im 生命周期详解对于服务端应用程序来说，我们这里讨论 ChannelHandler 更多的指的是 ChannelInboundHandler，在本小节，我们基于 ChannelInboundHandlerAdapter，自定义了一个 handler: LifeCyCleTestHandler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class LifeCyCleTestHandler extends ChannelInboundHandlerAdapter &#123; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("逻辑处理器被添加：handlerAdded()"); super.handlerAdded(ctx); &#125; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 绑定到线程(NioEventLoop)：channelRegistered()"); super.channelRegistered(ctx); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 准备就绪：channelActive()"); super.channelActive(ctx); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("channel 有数据可读：channelRead()"); super.channelRead(ctx, msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 某次数据读完：channelReadComplete()"); super.channelReadComplete(ctx); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 被关闭：channelInactive()"); super.channelInactive(ctx); &#125; @Override public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 取消线程(NioEventLoop) 的绑定: channelUnregistered()"); super.channelUnregistered(ctx); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("逻辑处理器被移除：handlerRemoved()"); super.handlerRemoved(ctx); &#125;&#125; 上面的代码可以看到，我们在每个方法被调用的时候都会打印一段文字，然后把这个事件继续往下传播。最后，我们把这个 handler 添加到构建的 pipeline 中. 1234567891011// 前面代码略.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; // 添加到第一个 ch.pipeline().addLast(new LifeCyCleTestHandler()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125;&#125;); 接着，我们先运行 NettyServer.java，然后再运行 NettyClient.java，这个时候，Server 端 控制台的输出为 可以看到 ChannelHandler 回调方法的执行顺序为 handlerAdded() -&gt; channelRegistered() -&gt; channelActive() -&gt; channelRead() -&gt; channelReadComplete() 下面，我们来逐个解释一下每个回调方法的含义 handlerAdded()指的是当检测到新连接之后，调用 ch.pipeline().addLast(new LifeCyCleTestHandler()); 之后的回调，表示在当前的 channel 中，已经成功添加了一个 handler 处理器。 channelRegistered()这个回调方法，表示当前的 channel 的所有的逻辑处理已经和某个 NIO 线程建立了绑定关系，类似我们在Netty 是什么？这小节中 BIO 编程中，accept 到新的连接，然后创建一个线程来处理这条连接的读写，只不过 Netty 里面是使用了线程池的方式，只需要从线程池里面去抓一个线程绑定在这个 channel 上即可，这里的 NIO 线程通常指的是 NioEventLoop,不理解没关系，后面我们还会讲到。 channelActive()当 channel 的所有的业务逻辑链准备完毕（也就是说 channel 的 pipeline 中已经添加完所有的 handler）以及绑定好一个 NIO 线程之后，这条连接算是真正激活了，接下来就会回调到此方法。 channelRead()客户端向服务端发来数据，每次都会回调此方法，表示有数据可读。 channelReadComplete()服务端每次读完一次完整的数据之后，回调该方法，表示数据读取完毕。 接下来，我们再把客户端关闭，这个时候对于服务端来说，其实就是 channel 被关闭， ChannelHandler 回调方法的执行顺序为 channelInactive() -&gt; channelUnregistered() -&gt; handlerRemoved() 这里的回调方法的执行顺序是新连接建立时候的逆操作，下面我们还是来解释一下每个方法的含义: channelInactive(): 表面这条连接已经被关闭了，这条连接在 TCP 层面已经不再是 ESTABLISH 状态了 channelUnregistered(): 既然连接已经被关闭，那么与这条连接绑定的线程就不需要对这条连接负责了，这个回调就表明与这条连接对应的 NIO 线程移除掉对这条连接的处理 handlerRemoved()：最后，我们给这条连接上添加的所有的业务逻辑处理器都给移除掉。 最后，我们用一幅图来标识 ChannelHandler 的生命周期 ChannelHandler 生命周期各回调方法用法举例Netty 对于一条连接的在各个不同状态下回调方法的定义还是蛮细致的，这个好处就在于我们能够基于这个机制写出扩展性较好的应用程序。 ChannelInitializer 的实现原理仔细翻看一下我们的服务端启动代码，我们在给新连接定义 handler 的时候，其实只是通过 childHandler() 方法给新连接设置了一个 handler，这个 handler 就是 ChannelInitializer，而在 ChannelInitializer 的 initChannel() 方法里面，我们通过拿到 channel 对应的 pipeline，然后往里面塞 handler NettyServer.java 123456789.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new LifeCyCleTestHandler()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125;&#125;); 这里的 ChannelInitializer 其实就利用了 Netty 的 handler 生命周期中 channelRegistered() 与 handlerAdded() 两个特性，我们简单翻一翻 ChannelInitializer 这个类的源代码： ChannelInitializer.java 123456789101112131415161718192021222324protected abstract void initChannel(C ch) throws Exception;public final void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // ... initChannel(ctx); // ...&#125;public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; // ... if (ctx.channel().isRegistered()) &#123; initChannel(ctx); &#125; // ...&#125;private boolean initChannel(ChannelHandlerContext ctx) throws Exception &#123; if (initMap.putIfAbsent(ctx, Boolean.TRUE) == null) &#123; initChannel((C) ctx.channel()); // ... return true; &#125; return false;&#125; 这里，我把非重点代码略去，逻辑会更加清晰一些 ChannelInitializer 定义了一个抽象的方法 initChannel()，这个抽象方法由我们自行实现，我们在服务端启动的流程里面的实现逻辑就是往 pipeline 里面塞我们的 handler 链 handlerAdded() 和 channelRegistered() 方法，都会尝试去调用 initChannel() 方法，initChannel() 使用 putIfAbsent() 来防止 initChannel() 被调用多次 如果你 debug 了 ChannelInitializer 的上述两个方法，你会发现，在 handlerAdded() 方法被调用的时候，channel 其实已经和某个线程绑定上了，所以，就我们的应用程序来说，这里的 channelRegistered() 其实是多余的，那为什么这里还要尝试调用一次呢？ 猜测应该是担心我们自己写了个类继承自 ChannelInitializer，然后覆盖掉了 handlerAdded() 方法，这样即使覆盖掉，在 channelRegistered() 方法里面还有机会再调一次 initChannel()，把我们自定义的 handler 都添加到 pipeline 中去。 handlerAdded() 与 handlerRemoved()这两个方法通常可以用在一些资源的申请和释放 channelActive() 与 channelInActive()对我们的应用程序来说，这两个方法表明的含义是 TCP 连接的建立与释放，通常我们在这两个回调里面统计单机的连接数，channelActive() 被调用，连接数加一，channelInActive() 被调用，连接数减一 另外，我们也可以在 channelActive() 方法中，实现对客户端连接 ip 黑白名单的过滤，具体这里就不展开了 channelRead()我们在前面小节讲拆包粘包原理，服务端根据自定义协议来进行拆包，其实就是在这个方法里面，每次读到一定的数据，都会累加到一个容器里面，然后判断是否能够拆出来一个完整的数据包，如果够的话就拆了之后，往下进行传递，这里就不过多展开，感兴趣的同学可以阅读一下: netty源码分析之拆包器的奥秘 channelReadComplete()每次向客户端写数据的时候，都通过 writeAndFlush() 的方法写并刷新到底层，其实这种方式不是特别高效，我们可以在之前调用 writeAndFlush() 的地方都调用 write() 方法，然后在这个方面里面调用 ctx.channel().flush() 方法，相当于一个批量刷新的机制，当然，如果你对性能要求没那么高，writeAndFlush() 足矣。 参考摘自：Netty 入门与实战：仿写微信 IM 即时通讯系统 参考：netty源码分析之拆包器的奥秘]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty数据传输载体ByteBuf简介]]></title>
    <url>%2F2019%2F11%2F12%2FNetty%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E8%BD%BD%E4%BD%93ByteBuf%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[代码：https://github.com/austin-brant/netty-im ByteBuf结构首先，我们先来了解一下 ByteBuf 的结构 以上就是一个 ByteBuf 的结构图，从上面这幅图可以看到 ByteBuf 是一个字节容器，容器里面的的数据分为三个部分: 第一个部分是已经丢弃的字节，这部分数据是无效的； 第二部分是可读字节，这部分数据是 ByteBuf 的主体数据， 从 ByteBuf 里面读取的数据都来自这一部分;最后一部分的数据是可写字节，所有写到 ByteBuf 的数据都会写到这一段。 第三部分虚线表示的是该 ByteBuf 最多还能扩容多少容量 以上三段内容是被两个指针给划分出来的，从左到右，依次是读指针（readerIndex）、写指针（writerIndex），然后还有一个变量 capacity，表示 ByteBuf 底层内存的总容量; 从 ByteBuf 中每读取一个字节，readerIndex 自增1，ByteBuf 里面总共有 writerIndex-readerIndex 个字节可读, 由此可以推论出当 readerIndex 与 writerIndex 相等的时候，ByteBuf 不可读 写数据是从 writerIndex 指向的部分开始写，每写一个字节，writerIndex 自增1，直到增到 capacity，这个时候，表示 ByteBuf 已经不可写了 ByteBuf 里面其实还有一个参数 maxCapacity，当向 ByteBuf 写数据的时候，如果容量不足，那么这个时候可以进行扩容，直到 capacity 扩容到 maxCapacity，超过 maxCapacity 就会报错 Netty使用ByteBuf这个数据结构可以有效地区分可读数据和可写数据，读写之间相互没有冲突，当然，ByteBuf只是对二进制数据的抽象, Netty关于数据读写只认ByteBuf，下面，我们就来学习一下 ByteBuf 常用的 API. 常用API容量 APIcapacity()表示 ByteBuf 底层占用了多少字节的内存（包括丢弃的字节、可读字节、可写字节），不同的底层实现机制有不同的计算方式，后面我们讲 ByteBuf 的分类的时候会讲到 maxCapacity()表示 ByteBuf 底层最大能够占用多少字节的内存，当向 ByteBuf 中写数据的时候，如果发现容量不足，则进行扩容，直到扩容到 maxCapacity，超过这个数，就抛异常 readableBytes() 与 isReadable()readableBytes() 表示 ByteBuf 当前可读的字节数，它的值等于 writerIndex-readerIndex，如果两者相等，则不可读，isReadable() 方法返回 false writableBytes()、 isWritable() 与 maxWritableBytes()writableBytes() 表示 ByteBuf 当前可写的字节数，它的值等于 capacity - writerIndex，如果两者相等，则表示不可写，isWritable() 返回 false，但是这个时候，并不代表不能往 ByteBuf 中写数据了， 如果发现往 ByteBuf 中写数据写不进去的话，Netty 会自动扩容 ByteBuf，直到扩容到底层的内存大小为 maxCapacity，而 maxWritableBytes() 就表示可写的最大字节数，它的值等于 maxCapacity - writerIndex。 读写指针相关的APIreaderIndex() 与 readerIndex(int)前者表示返回当前的读指针 readerIndex, 后者表示设置读指针 writeIndex() 与 writeIndex(int)前者表示返回当前的写指针 writerIndex, 后者表示设置写指针 markReaderIndex() 与 resetReaderIndex()前者表示把当前的读指针保存起来，后者表示把当前的读指针恢复到之前保存的值，下面两段代码是等价的 12345678910// 代码片段1int readerIndex = buffer.readerIndex();// .. 其他操作buffer.readerIndex(readerIndex);// 代码片段二buffer.markReaderIndex();// .. 其他操作buffer.resetReaderIndex(); 希望大家多多使用代码片段二这种方式，不需要自己定义变量，无论 buffer 当作参数传递到哪里，调用 resetReaderIndex() 都可以恢复到之前的状态，在解析自定义协议的数据包的时候非常常见，推荐大家使用这一对API. markWriterIndex() 与 resetWriterIndex()这一对 API 的作用与上述一对 API 类似，这里不再赘述. 读写API本质上，关于ByteBuf的读写都可以看作从指针开始的地方开始读写数据 writeBytes(byte[] src) 与 buffer.readBytes(byte[] dst)writeBytes() 表示把字节数组 src 里面的数据全部写到 ByteBuf，而 readBytes() 指的是把 ByteBuf 里面的数据全部读取到 dst，这里 dst 字节数组的大小通常等于 readableBytes()，而 src 字节数组大小的长度通常小于等于 writableBytes() writeByte(byte b) 与 buffer.readByte()writeByte() 表示往 ByteBuf 中写一个字节，而 buffer.readByte() 表示从 ByteBuf 中读取一个字节，类似的 API 还有 writeBoolean()、writeChar()、writeShort()、writeInt()、writeLong()、writeFloat()、writeDouble() 与 readBoolean()、readChar()、readShort()、readInt()、readLong()、readFloat()、readDouble() 这里就不一一赘述. 与读写 API 类似的 API 还有 getBytes、getByte() 与 setBytes()、setByte() 系列，唯一的区别就是 get/set 不会改变读写指针，而 read/write 会改变读写指针，这点在解析数据的时候千万要注意 release() 与 retain()由于 Netty 使用了堆外内存，而堆外内存是不被 jvm 直接管理的，也就是说申请到的内存无法被垃圾回收器直接回收，所以需要我们手动回收。有点类似于c语言里面，申请到的内存必须手工释放，否则会造成内存泄漏。 Netty 的 ByteBuf 是通过 引用计数 的方式管理的，如果一个 ByteBuf 没有地方被引用到，需要回收底层内存。默认情况下，当创建完一个 ByteBuf，它的引用为1，然后每次调用 retain() 方法， 它的引用就加一， release() 方法原理是将引用计数减一，减完之后如果发现引用计数为0，则直接回收 ByteBuf 底层的内存。 slice()、duplicate()、copy()这三个方法通常情况会放到一起比较，这三者的返回值都是一个新的 ByteBuf 对象 slice() 方法从原始 ByteBuf 中截取一段，这段数据是从 readerIndex 到 writeIndex，同时，返回的新的 ByteBuf 的最大容量 maxCapacity 为原始 ByteBuf 的 readableBytes() duplicate() 方法把整个 ByteBuf 都截取出来，包括所有的数据，指针信息 slice() 方法与 duplicate() 方法比较： 相同点： 底层内存以及引用计数与原始的 ByteBuf 共享，也就是说经过 slice() 或者 duplicate() 返回的 ByteBuf 调用 write 系列方法都会影响到 原始的 ByteBuf，但是它们都维持着与原始 ByteBuf 相同的内存引用计数和不同的读写指针 不同点：slice() 只截取从 readerIndex 到 writerIndex 之间的数据，它返回的 ByteBuf 的最大容量被限制到 原始 ByteBuf 的 readableBytes(), 而 duplicate() 是把整个 ByteBuf 都与原始的 ByteBuf 共享 slice() 方法与 duplicate() 方法不会拷贝数据，它们只是通过改变读写指针来改变读写的行为，而 copy() 会直接从原始的 ByteBuf 中拷贝所有的信息，包括读写指针以及底层对应的数据，因此， copy() 返回的 ByteBuf 中写数据不会影响到原始的 ByteBuf slice() 和 duplicate() 不会改变 ByteBuf 的引用计数，所以原始的 ByteBuf 调用 release() 之后发现引用计数为零，就开始释放内存，调用这两个方法返回的 ByteBuf 也会被释放，这个时候如果再对它们进行读写，就会报错。因此，我们可以通过调用一次 retain() 方法 来增加引用，表示它们对应的底层的内存多了一次引用，引用计数为2，在释放内存的时候，需要调用两次 release() 方法，将引用计数降到零，才会释放内存 这三个方法均维护着自己的读写指针，与原始的 ByteBuf 的读写指针无关，相互之间不受影响 retainedSlice() 与 retainedDuplicate() 它们的作用是在截取内存片段的同时，增加内存的引用计数，分别与下面两段代码等价 12345// retainedSlice 等价于slice().retain();// retainedDuplicate() 等价于duplicate().retain() 使用到 slice 和 duplicate 方法的时候，千万要理清 内存共享，引用计数共享，读写指针不共享 几个概念，下面举两个常见的易犯错的例子 多次释放 12345678910111213141516171819202122Buffer buffer = xxx;doWith(buffer);// 一次释放buffer.release();public void doWith(Bytebuf buffer) &#123; // ... // 没有增加引用计数 Buffer slice = buffer.slice(); foo(slice);&#125;public void foo(ByteBuf buffer) &#123; // read from buffer // 重复释放 buffer.release();&#125; 这里的 doWith 有的时候是用户自定义的方法，有的时候是 Netty 的回调方法，比如 channelRead() 等等 不释放造成内存泄漏 1234567891011121314151617Buffer buffer = xxx;doWith(buffer);// 引用计数为2，调用 release 方法之后，引用计数为1，无法释放内存 buffer.release();public void doWith(Bytebuf buffer) &#123; // ... // 增加引用计数 Buffer slice = buffer.retainedSlice(); foo(slice); // 没有调用 release&#125;public void foo(ByteBuf buffer) &#123; // read from buffer&#125; 想要避免以上两种情况发生，大家只需要记得一点，在一个函数体里面，只要增加了引用计数（包括 ByteBuf 的创建和手动调用 retain() 方法），就必须调用 release() 方法. 实战了解了以上 API 之后，最后我们使用上述 API 来 写一个简单的 demo。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ByteBufTest &#123; public static void main(String[] args) &#123; ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(9, 100); print("allocate ByteBuf(9, 100)", buffer); // write 方法改变写指针，写完之后写指针未到 capacity 的时候，buffer 仍然可写 buffer.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;); print("writeBytes(1,2,3,4)", buffer); // write 方法改变写指针，写完之后写指针未到 capacity 的时候，buffer 仍然可写, 写完 int 类型之后，写指针增加4 buffer.writeInt(12); print("writeInt(12)", buffer); // write 方法改变写指针, 写完之后写指针等于 capacity 的时候，buffer 不可写 buffer.writeBytes(new byte[]&#123;5&#125;); print("writeBytes(5)", buffer); // write 方法改变写指针，写的时候发现 buffer 不可写则开始扩容，扩容之后 capacity 随即改变 buffer.writeBytes(new byte[]&#123;6&#125;); print("writeBytes(6)", buffer); // get 方法不改变读写指针 System.out.println("getByte(3) return: " + buffer.getByte(3)); System.out.println("getShort(3) return: " + buffer.getShort(3)); System.out.println("getInt(3) return: " + buffer.getInt(3)); print("getByte()", buffer); // set 方法不改变读写指针 buffer.setByte(buffer.readableBytes() + 1, 0); print("setByte()", buffer); // read 方法改变读指针 byte[] dst = new byte[buffer.readableBytes()]; buffer.readBytes(dst); print("readBytes(" + dst.length + ")", buffer); &#125; private static void print(String action, ByteBuf buffer) &#123; System.out.println("after ===========" + action + "============"); System.out.println("capacity(): " + buffer.capacity()); System.out.println("maxCapacity(): " + buffer.maxCapacity()); System.out.println("readerIndex(): " + buffer.readerIndex()); System.out.println("readableBytes(): " + buffer.readableBytes()); System.out.println("isReadable(): " + buffer.isReadable()); System.out.println("writerIndex(): " + buffer.writerIndex()); System.out.println("writableBytes(): " + buffer.writableBytes()); System.out.println("isWritable(): " + buffer.isWritable()); System.out.println("maxWritableBytes(): " + buffer.maxWritableBytes()); System.out.println(); &#125;&#125; 控制台输出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990after ===========allocate ByteBuf(9, 100)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 0isReadable(): falsewriterIndex(): 0writableBytes(): 9isWritable(): truemaxWritableBytes(): 100after ===========writeBytes(1,2,3,4)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 4isReadable(): truewriterIndex(): 4writableBytes(): 5isWritable(): truemaxWritableBytes(): 96after ===========writeInt(12)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 8isReadable(): truewriterIndex(): 8writableBytes(): 1isWritable(): truemaxWritableBytes(): 92after ===========writeBytes(5)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 9isReadable(): truewriterIndex(): 9writableBytes(): 0isWritable(): falsemaxWritableBytes(): 91after ===========writeBytes(6)============capacity(): 64maxCapacity(): 100readerIndex(): 0readableBytes(): 10isReadable(): truewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90getByte(3) return: 4getShort(3) return: 1024getInt(3) return: 67108864after ===========getByte()============capacity(): 64maxCapacity(): 100readerIndex(): 0readableBytes(): 10isReadable(): truewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90after ===========setByte()============capacity(): 64maxCapacity(): 100readerIndex(): 0readableBytes(): 10isReadable(): truewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90after ===========readBytes(10)============capacity(): 64maxCapacity(): 100readerIndex(): 10readableBytes(): 0isReadable(): falsewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90 总结 Netty 对二进制数据的抽象 ByteBuf 的结构，本质原理就是，它引用了一段内存，这段内存可以是堆内也可以是堆外的，然后用引用计数来控制这段内存是否需要被释放，使用读写指针来控制对 ByteBuf 的读写，可以理解为是外观模式的一种使用 基于读写指针和容量、最大可扩容容量，衍生出一系列的读写方法，要注意 read/write 与 get/set 的区别 多个 ByteBuf 可以引用同一段内存，通过引用计数来控制内存的释放，遵循谁 retain() 谁 release() 的原则 参考[转载] 数据传输载体 ByteBuf 介绍]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql性能分析工具]]></title>
    <url>%2F2019%2F10%2F09%2FMysql%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Explain简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化. EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了, 例如: 1EXPLAIN SELECT * from user_info WHERE id &lt; 300; 数据准备12345678910111213141516171819202122232425262728293031323334353637CREATE TABLE `user_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(50) NOT NULL DEFAULT &apos;&apos;, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `user_id` bigint(20) DEFAULT NULL, `product_name` varchar(50) NOT NULL DEFAULT &apos;&apos;, `productor` varchar(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`,`product_name`,`productor`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出格式1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. table: 表示查询涉及的表或衍生表 partitions: 匹配的分区 type: join 类型 possible_keys: 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key: 此次查询中真正使用到的索引. ref: 哪个字段或常数与 key 一起被使用 rows: MySQL查询优化器根据统计信息, 估算SQL要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. filtered: 表示此查询条件所过滤的数据的百分比 extra: 额外的信息 参数说明select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二个或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型, 例如: 1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) 如果我们使用了 UNION 查询, 那么 EXPLAIN 输出 的结果类似如下: 123456789mysql&gt; EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) UNION (SELECT * FROM user_info WHERE id IN (3, 4, 5));+----+--------------+------------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+-------+---------------+---------+---------+------+------+-------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | |+----+--------------+------------+-------+---------------+---------+---------+------+------+-------------+3 rows in set (0.00 sec) typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+-------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+-------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5; +----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using where; Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+--------------------------+2 rows in set (0.01 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中. 当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 123456 mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+ index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据. index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index.例如: 123456 mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+ 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. 下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 123456 mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+ type 类型的性能比较 通常来说, 不同的 type 类型的性能关系如下: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的. 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快. 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n字节长度 varchar(n): 如果是 utf8 编码, 则是 (3n + 2) 字节; 如果是 utf8mb4 编码, 则是 (4n + 2） 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的例子: 123456mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = 'p1' AND productor = 'WHH';+----+-------------+------------+-------+---------------------------+---------------------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+------+------+--------------------------+| 1 | SIMPLE | order_info | range | user_product_detail_index | user_product_detail_index | 9 | NULL | 4 | Using where; Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+------+------+--------------------------+ 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: KEY user_product_detail_index (user_id, product_name, productor) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT ‘0’, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 123456mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = 'p1';+----+-------------+------------+------+---------------------------+---------------------------+---------+-------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------------------+---------------------------+---------+-------------+------+--------------------------+| 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 161 | const,const | 2 | Using where; Using index |+----+-------------+------------+------+---------------------------+---------------------------+---------+-------------+------+--------------------------+ 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 123456 mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name;+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-----------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-----------------------------+| 1 | SIMPLE | order_info | index | NULL | user_product_detail_index | 254 | NULL | 9 | Using index; Using filesort |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-----------------------------+ 我们的索引是 KEY user_product_detail_index (user_id, product_name, productor) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort.如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 123456mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name;+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-------------+| 1 | SIMPLE | order_info | index | NULL | user_product_detail_index | 254 | NULL | 9 | Using index |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-------------+ Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. Profiling简介当我们要对某一条sql的性能进行分析时，可以使用它。 Profiling是从 mysql5.0.3版本以后才开放的。启动profile之后，所有查询包括错误的语句都会记录在内。关闭会话或者set profiling=0 就关闭了。（如果将profiling_history_size参数设置为0，同样具有关闭MySQL的profiling效果。） 此工具可用来查询SQL执行状态，System lock和Table lock 花多少时间等等， 对定位一条语句的I/O、CPU、IPC，Memory消耗 非常重要。(SQL 语句执行所消耗的最大两部分资源就是IO和CPU) 在mysql5.7之后，profile信息将逐渐被废弃，mysql推荐使用performance schema 用法简易流程大概如下： 12345678910111213set profiling=1; //打开分析，默认值为0（off），可以通过设置profiling为1或ON开启 run your sql1; run your sql2; show profiles; //查看sql1, sql2的语句分析 show profile for query 1; //查看sql1的具体分析 show profile ALL for query 1; //查看sql1相关的所有分析【主要看i/o与cpu,下边分析中有各项意义介绍】set profiling=0; //关闭分析 语法： 123SHOW PROFILE [type [, type] ... ] [FOR QUERY n] [LIMIT row_count [OFFSET offset]] type: ALL | BLOCK IO | CONTEXT SWITCHES | CPU | IPC | MEMORY | PAGE FAULTS | SOURCE | SWAPS SOURCE：显示和Source_function,Source_file,Source_line相关的开销信息 注意：profiling被应用在每一个会话中，当前会话关闭后，profiling统计的信息将丢失。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155mysql&gt; show variables like '%profil%'; ## 查看mysql profiling相关配置+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | OFF | ## 是否开启| profiling_history_size | 15 | ## 记录历史条数+------------------------+-------+3 rows in set (0.00 sec)mysql&gt; set profiling = ON; ## 开启分析Query OK, 0 rows affected (0.01 sec)mysql&gt; show variables like '%profil%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | ON || profiling_history_size | 15 |+------------------------+-------+3 rows in set (0.00 sec)mysql&gt; show tables; +-------------------------------------+| Tables_in_bigdata_new |+-------------------------------------+| TblDataShoppingRecord || TblDataViewRecord || TblEmailNotifyRecord |+-------------------------------------+3 rows in set (0.00 sec)mysql&gt; select * from TblDataShoppingRecord order by id desc limit 1\G;*************************** 1. row *************************** id: 1263 status: off name: mapdata_meta_27 creator: zhangsan createTime: 2019-10-08 15:20:201 row in set (0.00 sec)ERROR: No query specifiedmysql&gt; select * from TblDataShoppingRecord where creator = 'austin' order by id desc limit 1\G;*************************** 1. row *************************** id: 1285 status: off name: metadata_30 creator: austin createTime: 2019-10-08 15:20:201 row in set (0.01 sec)ERROR: No query specifiedmysql&gt; show profiles;+----------+------------+---------------------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+---------------------------------------------------------------------------------------------+| 1 | 0.00045400 | show variables like '%profil%' || 2 | 0.00086200 | show tables || 3 | 0.00034900 | select * from TblDataShoppingRecord order by id desc limit 1 || 4 | 0.00040000 | select * from TblDataShoppingRecord where creator = 'austin' order by id desc limit 1 |+----------+------------+---------------------------------------------------------------------------------------------+4 rows in set (0.00 sec)mysql&gt; show profile for query 4;+--------------------------------+----------+| Status | Duration |+--------------------------------+----------+| starting | 0.000039 || checking query cache for query | 0.000052 || Opening tables | 0.000015 || System lock | 0.000005 || Table lock | 0.000037 || init | 0.000031 || optimizing | 0.000009 || statistics | 0.000073 || preparing | 0.000016 || executing | 0.000005 || Sorting result | 0.000006 || Sending data | 0.000050 || end | 0.000007 || query end | 0.000004 || freeing items | 0.000027 || storing result in query cache | 0.000015 || logging slow query | 0.000004 || cleaning up | 0.000005 |+--------------------------------+----------+18 rows in set (0.00 sec)mysql&gt; show profile cpu for query 4;+--------------------------------+----------+----------+------------+| Status | Duration | CPU_user | CPU_system |+--------------------------------+----------+----------+------------+| starting | 0.000039 | 0.000000 | 0.000000 || checking query cache for query | 0.000052 | 0.000000 | 0.000000 || Opening tables | 0.000015 | 0.000000 | 0.000000 || System lock | 0.000005 | 0.000000 | 0.000000 || Table lock | 0.000037 | 0.000000 | 0.000000 || init | 0.000031 | 0.000000 | 0.000000 || optimizing | 0.000009 | 0.000000 | 0.000000 || statistics | 0.000073 | 0.000000 | 0.000000 || preparing | 0.000016 | 0.000000 | 0.000000 || executing | 0.000005 | 0.000000 | 0.000000 || Sorting result | 0.000006 | 0.000000 | 0.000000 || Sending data | 0.000050 | 0.000000 | 0.000000 || end | 0.000007 | 0.000000 | 0.000000 || query end | 0.000004 | 0.000000 | 0.000000 || freeing items | 0.000027 | 0.000000 | 0.000000 || storing result in query cache | 0.000015 | 0.000000 | 0.000000 || logging slow query | 0.000004 | 0.000000 | 0.000000 || cleaning up | 0.000005 | 0.000000 | 0.000000 |+--------------------------------+----------+----------+------------+18 rows in set (0.00 sec)mysql&gt; show profile BLOCK IO, CPU, IPC, MEMORY, SOURCE, SWAPS for query 4; +--------------------------------+----------+----------+------------+--------------+---------------+---------------+-------------------+-------+------------------+---------------+-------------+| Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out | Messages_sent | Messages_received | Swaps | Source_function | Source_file | Source_line |+--------------------------------+----------+----------+------------+--------------+---------------+---------------+-------------------+-------+------------------+---------------+-------------+| starting | 0.000039 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | NULL | NULL | NULL || checking query cache for query | 0.000052 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_cache.cc | 1523 || Opening tables | 0.000015 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_base.cc | 4618 || System lock | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | lock.cc | 260 || Table lock | 0.000037 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | lock.cc | 271 || init | 0.000031 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2528 || optimizing | 0.000009 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 833 || statistics | 0.000073 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 1024 || preparing | 0.000016 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 1046 || executing | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 1780 || Sorting result | 0.000006 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2205 || Sending data | 0.000050 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2338 || end | 0.000007 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2574 || query end | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 5118 || freeing items | 0.000027 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 6142 || storing result in query cache | 0.000015 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_cache.cc | 985 || logging slow query | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 1735 || cleaning up | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 1703 |+--------------------------------+----------+----------+------------+--------------+---------------+---------------+-------------------+-------+------------------+---------------+-------------+18 rows in set (0.00 sec)mysql&gt; set profiling=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like '%profil%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | OFF || profiling_history_size | 15 |+------------------------+-------+3 rows in set (0.00 sec) 结果参数说明其中标题含义： 12345678910111213141516&quot;Status&quot;: &quot;query end&quot;, 状态&quot;Duration&quot;: &quot;1.751142&quot;, 持续时间&quot;CPU_user&quot;: &quot;0.008999&quot;, cpu用户&quot;CPU_system&quot;: &quot;0.003999&quot;, cpu系统&quot;Context_voluntary&quot;: &quot;98&quot;, 上下文主动切换&quot;Context_involuntary&quot;: &quot;0&quot;, 上下文被动切换&quot;Block_ops_in&quot;: &quot;8&quot;, 阻塞的输入操作&quot;Block_ops_out&quot;: &quot;32&quot;, 阻塞的输出操作&quot;Messages_sent&quot;: &quot;0&quot;, 消息发出&quot;Messages_received&quot;: &quot;0&quot;, 消息接受&quot;Page_faults_major&quot;: &quot;0&quot;, 主分页错误&quot;Page_faults_minor&quot;: &quot;0&quot;, 次分页错误&quot;Swaps&quot;: &quot;0&quot;, 交换次数&quot;Source_function&quot;: &quot;mysql_execute_command&quot;, 源功能&quot;Source_file&quot;: &quot;sql_parse.cc&quot;, 源文件&quot;Source_line&quot;: &quot;4465&quot; 源代码行 不同阶段： 12345678910111213141516starting：开始checking permissions：检查权限Opening tables：打开表init ： 初始化System lock ：系统锁optimizing ： 优化statistics ： 统计preparing ：准备executing ：执行Sending data ：发送数据Sorting result ：排序end ：结束query end ：查询 结束closing tables ： 关闭表 ／去除TMP 表freeing items ： 释放物品cleaning up ：清理 配置显示的记录数由变量“profiling_history_size”控制,默认15条，最大值为100，可以手动设置该参数值。 12345678910111213mysql&gt; set profiling_history_size = 30; Query OK, 0 rows affected (0.01 sec)mysql&gt; show variables like '%profil%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | OFF || profiling_history_size | 30 |+------------------------+-------+3 rows in set (0.00 sec) 参考博客[1] MySQL 性能优化神器 Explain 使用分析[2] Mysql分析-profile详解[3] MySQL性能分析工具profiling]]></content>
      <categories>
        <category>Mysql</category>
        <category>性能</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引底层原理]]></title>
    <url>%2F2019%2F09%2F28%2FMysql%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[常用引擎InnoDB（聚集索引） InnoDB的存储文件有两个，后缀名分别是.frm 和.idb，其中.frm是表的定义文件，而.idb是索引和数据文件。 InnoDB 中存在表锁和行锁，不过行锁是在命中索引的情况下才会起作用。 InnoDB 支持事务，且支持四种隔离级别（读未提交、读已提交、可重复读、串行化），默认的为可重复读；而在 Oracle 数据库中，只支持串行化级别和读已提交这两种级别，其中默认的为读已提交级别。 MyISAM（非聚集索引） Myisam 的存储文件有三个，后缀名分别是.frm、.MYD、MYI，其中.frm是表的定义文件，.MYD是数据文件，.MYI是索引文件。 Myisam只支持表锁，且不支持事务。Myisam 由于有单独的索引文件，在读取数据方面的性能很高 。 存储结构可以用来优化查询的数据结构有哈希表，完全平衡二叉树，B树，B+树。我们使用最多的是B+树，InnoDB和Myisam都是用 B+Tree 来存储数据的。 数据结构可视化网站： https://www.cs.usfca.edu/~galles/visualization/Algorithms.html Hash为什么很少使用hash? 优点：直接计算下标，查询单一数据非常快。 缺点：如果是进行的范围查询的话，哈希索引就必须全表遍历，获得age数据，然后再依次进行比较，也就是相当于没有索引了。这样就不能优化查询效率了。 B树 d 为大于1的一个正整数，称为B-Tree的度，表示节点的数据存储个数； h 为一个正整数，称为B-Tree的高度； 每个非叶子节点由n-1个key和n个指针组成，其中 d &lt;= n &lt;= 2d； 每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null； 所有叶节点具有相同的深度，等于树高h; key和指针互相间隔，节点两端是指针; 一个节点中的key从左到右非递减排列; 所有节点组成树结构; 每个指针要么为null，要么指向另外一个节点; 关于B-Tree有一系列有趣的性质，例如一个度为 d 的B-Tree，设其索引 N 个key，则其树高h的上限为 logd((N+1)/2) ，检索一个key，其查找节点个数的线性复杂度为 O(logdN) 。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 B+树B-Tree有许多变种，其中最常见的是B+Tree。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1; 非叶子节点不存储data，只存储key，可节省空间，增大 度; 叶子节点不存储指针; 带有顺序访问指针，提高了区间访问性能; 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。 为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的最小逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 mysql页文件配置查看mysql页文件大小 SHOW GLOBAL STATUS like ‘Innodb_page_size’; 为什么Mysql页文件默认16kb就够了呢 假设我们一行数据大小为1K,那么一页就能存16条数据，也就是一个叶子节点能存16条数据; 再看非叶子节点，假设主键ID为bigint类型, 那么长度为8B，指针大小在Innodb源码中为6B，-共就是14B,那么一页里就可以存储16K/14=1170个(主键+指针)，那么： 一颗高度 为2的B+树能存储的数据为: 1170 * 16 = 18720条 一 颗高度为3的B+树能存储的数据为: 1170 * 1170 * 16 = 21902400 (千万级条)。 所以在InnoDB中B+树高度一般为1-3层， 它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次I/O, 所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。所以也就回答了我们的问题，1 页=16k这么设置是比较合适的，是适用大多数的企业的，当然这个值是可以修改的，所以也能根据业务的时间情况进行调整。 B-/+Tree索引的性能分析一般以使用磁盘I/O次数评价索引结构的优劣。 先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。 为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。B+Tree之所以更适合外存索引，原因和内节点度d有关。从上面分析可以看到，d越大索引的性能越好，而度的上限取决于节点内key和data的大小： 1dmax = floor(pagesize/(keysize+datasize+pointsize)) floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的度，拥有更好的性能。 MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的。 MyISAM索引实现 索引文件仅仅保存数据记录的地址，索引文件和数据文件是分离的； 主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复； 索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 数据文件本身就是索引文件，都存储在后缀为.idb的文件中； 叶节点data域保存了完整的数据记录, 而不是行地址； 因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形； 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录； InnodeDB主键选择与插入优化基于以上特点就很容易理解为什么不建议使用过长的字段作为主键？而且推荐使用整形自增主键？ 所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大； InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择； 联合索引有如下数据表： 1234567CREATE TABLE People ( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum('m','f') not null, key(last_name,first_name,dob)) 这个建表语句在last_name、first_name、dob列上建立了一个联合索引，下图展示了该索引的存储结构。 可以看到，联合索引中的索引项会先根据第一个索引列进行排序，第一个索引列相同的情况下，会再按照第二个索引列进行排序，依次类推。根据这种存储特点，B-Tree索引对如下类型的查找有效： 全值匹配：查找条件和索引中的所有列相匹配 匹配最左前缀：查找条件只有索引中的第一列 匹配列前缀：只匹配某一列值的开头部分。这里并不一定只能匹配第一个索引列的前缀。例如在确定第一个索引列的值时，也可以在第二个索引列上匹配列前缀。在上面例子中，对于查找姓为Allen，名为J开头的人，也可以应用到索引。 匹配范围值，或者精确匹配某一列并范围匹配另外一列：例如查找姓在Allen和Barrymore之间的人，或者查找姓为Allen，名字在某一个范围内的人。 只访问索引的查询，即要查询的值在索引中都包含，只需要访问索引就行了，无需访问数据行。这种索引被称作覆盖索引。 对于上面列出的查询类型，索引除了可以用来查询外，还可以用来排序。 下面是B-Tree索引的一些限制： 如果不是从索引的最左列开始查找，则无法使用索引。例如直接查找名字为Bill的人，或查找某个生日的人都无法应用到上面的索引，因为都跳过了索引的第一个列。此外查找姓以某个字母结尾的人，也无法使用到上面的索引。 不能在中间跳过索引中的某个列，例如不能查找姓为Smith，生日为某个特定日期的类。这样的查询只能使用到索引的第一列。 如果查询中有某个列的范围查询，则该列右边的所有列都无法使用索引优化查找。例如有查询WHERE last_name=’Smith’ AND first_name LIKE ‘J%’ AND dob=’1976-12-23’，这个查询只能使用到索引的前两列，而不能使用整个索引。 通过上面列出的这些条件，可见对于一个B-TREE联合索引，索引列的顺序非常重要。 InnoDB中有一个功能叫“自适应哈希索引”，当InnoDB注意到某些索引值使用的非常频繁时，会在B-Tree索引之上再建立一层哈希索引，以加速查找效率。这是完全自动的内部行为，用户无法干预。 索引查询多列索引当出现对多个索引列做相交(AND)操作的查询时，代表需要一个包含所有相关列的联合索引，而不是多个独立的单列索引。 在MySql官方提供的示例数据库sakila中，表film_actor在字段film_id和actor_id上各有一个单列索引，对于下面这条查询语句，这两个单列索引都不是很好的选择： 1SELECT film_id,actor_id FROM film_actor WHERE actor_id=1 OR film_id=1; 在老的MySql版本中，这个查询会使用全表扫描。但在MySql5.0之后，查询能够同时使用这两个单列索引进行扫描，然后将结果合并，相当于转换成下面这条查询： 123SELECT film_id,actor_id FROM film_actor WHERE actor_id=1 UNIONSELECT film_id,actor_id FROM film_actor WHERE film_id=1; 在MySql5.7中，执行上面查询的执行计划如下图所示： 从执行计划的type字段可以看到，MySql同时使用了两个索引，并将各自的查询结果合并。并且Extra字段描述了使用索引的详细信息。 虽然MySql在背后对查询进行了优化，使其可以同时利用两个单列索引。但是这需要耗费大量的CPU和内存资源，所以直接将查询改写成UNION的方式会更好。像这种两个列上都有索引的情况，用union代替or会得到更好的效果(注意要求两个列上都建有索引，如果没有索引，用union代替or反而会降低效率)。 如果在EXPLAIN中看到有索引合并，那就应该好好检查一下查询和表的结构，看看是不是已经是最优的。 覆盖索引如果一个索引包含所有需要查询的字段，就称之为“覆盖索引”。由于在索引的叶子节点中已经包含了要查询的全部数据，所以就可以从索引中直接获取查询结果，而没必要再回表查询。 索引一般远远小于数据行的大小，如果只需要访问索引，就会极大减少数据访问量。而且索引是按照顺序存储，所以在进行范围查询时会比随机从磁盘读取每一条数据的I/O要少的多。由此看出，覆盖索引能够极大的提高查询性能。 sakila数据库中包含了由store_id和film_id组成的一个联合索引，如下图所示： 如果只查询store_id和film_id这两列，就可以使用这个索引做覆盖索引。 EXPLAIN的Extra列如果是Using index，则代表这个查询使用到了覆盖索引。注意type字段和是否为覆盖索引毫无关系。 参考博客[1] MySQL索引背后的数据结构及算法原理 [2] MySql索引]]></content>
      <categories>
        <category>Mysql</category>
        <category>索引</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>索引</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka快速入门]]></title>
    <url>%2F2019%2F09%2F23%2FKafka%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[介绍Apache Kafka® 是 一个分布式流处理平台.我们知道流处理平台有以下三种特性: 可以让你发布和订阅流式的记录。这一方面与消息队列或者企业消息系统类似。 可以储存流式的记录，并且有较好的容错性。 可以在流式记录产生时就进行处理。 Kafka适合什么样的场景? 它可以用于两大类别的应用: 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue) 构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化) 首先是一些概念: Kafka作为一个集群，运行在一台或者多台服务器上. Kafka 通过 topic 对存储的流数据进行分类。 每条记录中包含一个key，一个value和一个timestamp（时间戳）。 Topics和日志让我们首先深入了解下Kafka的核心概念:提供一串流式的记录— topic 。 Topic 就是数据主题，是数据记录发布的地方,可以用来区分业务系统。Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。 对于每一个topic， Kafka集群都会维持一个分区日志，如下所示： 每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的commit log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，offset用来唯一的标识分区中每一条记录。 Kafka 集群保留所有发布的记录—无论他们是否已被消费—并通过一个可配置的参数——保留期限来控制. 举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。Kafka的性能和数据大小无关，所以长时间存储数据没有什么问题. 日志中的 partition（分区）有以下几个用途。 第一，当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。 第二，可以作为并行的单元集 分布式日志的分区partition （分布）在Kafka集群的服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性. 每个分区都有一台 server 作为 “leader”，零台或者多台server作为 follwers 。leader server 处理一切对 partition （分区）的读写请求，而follwers只需被动的同步leader上的数据。当leader宕机了，followers 中的一台服务器会自动成为新的 leader。每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的。 消费者如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例. 如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程. 在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。 Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。 Kafka的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer 组中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。 这种方式还有一个附加的好处。consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了， 那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。 持久化Kafka 对消息的存储和缓存严重依赖于文件系统。现代操作系统提供了 read-ahead 和 write-behind 技术，read-ahead 是以大的 data block 为单位预先读取数据，而 write-behind 是将多个小型的逻辑写合并成一次大型的物理磁盘写入。关于该问题的进一步讨论可以参考 ACM Queue article，他们发现实际上顺序磁盘访问在某些情况下比随机内存访问还要快！ 这里给出了一个非常简单的设计：相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将数据 flush 到文件系统，我们把这个过程倒过来。所有数据一开始就被写入到文件系统的持久化日志中，而不用在 cache 空间不足的时候 flush 到磁盘。实际上，这表明数据被转移到了内核的 pagecache 中。 使用文件系统和 pagecache 显得更有优势–我们可以通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番。 这样使得32GB的机器缓存容量可以达到28-30GB,并且不会产生额外的 GC 负担。此外，即使服务重新启动，缓存依旧可用，而 in-process cache 则需要在内存中重建(重建一个10GB的缓存可能需要10分钟)，否则进程就要从 cold cache 的状态开始(这意味着进程最初的性能表现十分糟糕)。 这同时也极大的简化了代码，因为所有保持 cache 和文件系统之间一致性的逻辑现在都被放到了 OS 中，这样做比一次性的进程内缓存更准确、更高效。如果你的磁盘使用更倾向于顺序读取，那么 read-ahead 可以有效的使用每次从磁盘中读取到的有用数据预先填充 cache。 持久化队列可以建立在简单的读取和向文件后追加两种操作之上，这和日志解决方案相同。这种架构的优点在于所有的操作复杂度都是O(1)，而且读操作不会阻塞写操作，读操作之间也不会互相影响。这有着明显的性能优势，在不产生任何性能损失的情况下能够访问几乎无限的硬盘空间，这意味着我们可以提供一些其它消息系统不常见的特性。例如：在 Kafka 中，我们可以让消息保留相对较长的一段时间(比如一周)，而不是试图在被消费后立即删除。正如我们后面将要提到的，这给消费者带来了很大的灵活性。 优化 减少数据拷贝使用 sendfile 方法，可以允许操作系统将数据从 pagecache 直接发送到网络，这样避免重新复制数据。所以这种优化方式，只需要最后一步的copy操作，将数据复制到 NIC 缓冲区。pagecache 和 sendfile 的组合使用意味着，在一个kafka集群中，大多数 consumer 消费时，您将看不到磁盘上的读取活动，因为数据将完全由缓存提供。 端到端批量压缩Kafka 以高效的批处理格式支持一批消息可以压缩在一起发送到服务器。这批消息将以压缩格式写入，并且在日志中保持压缩，只会在 consumer 消费时解压缩。 生产者负载均衡生产者直接发送数据到主分区的服务器上，不需要经过任何中间路由。为了让生产者实现这个功能，所有的 kafka 服务器节点都能响应这样的元数据请求： 哪些服务器是活着的，主题的哪些分区是主分区，分配在哪个服务器上，这样生产者就能适当地直接发送它的请求到服务器上。 客户端控制消息发送数据到哪个分区，这个可以实现随机的负载均衡方式, 或者使用一些特定语义的分区函数。 我们有提供特定分区的接口让用于根据指定的键值进行hash分区(当然也有选项可以重写分区函数)，例如，如果使用用户ID作为key，则用户相关的所有数据都会被分发到同一个分区上。 异步发送批处理是提升性能的一个主要驱动，为了允许批量处理，kafka 生产者会尝试在内存中汇总数据，并用一次请求批次提交信息。 批处理，不仅仅可以配置指定的消息数量，也可以指定等待特定的延迟时间(如64k 或10ms)，这允许汇总更多的数据后再发送，在服务器端也会减少更多的IO操作。 该缓冲是可配置的，并给出了一个机制，通过权衡少量额外的延迟时间获取更好的吞吐量。 消息交互语义Kafka可以提供的消息交付语义保证有多种： At most once——消息可能会丢失但绝不重传。 At least once——消息可以重传但绝不丢失。在 0.11.0.0 之前的版本中, 如果 producer 没有收到表明消息已经被提交的响应, 那么 producer 除了将消息重传之外别无选择。 这里提供的是 at-least-once 的消息交付语义，因为如果最初的请求事实上执行成功了，那么重传过程中该消息就会被再次写入到 log 当中。 Exactly once——这正是人们想要的, 每一条消息只被传递一次. 从 0.11.0.0 版本开始，Kafka producer新增了幂等性的传递选项，该选项保证重传不会在 log 中产生重复条目。 为实现这个目的, broker 给每个 producer 都分配了一个 ID ，并且 producer 给每条被发送的消息分配了一个序列号来避免产生重复的消息。 同样也是从 0.11.0.0 版本开始, producer 新增了使用类似事务性的语义将消息发送到多个 topic partition 的功能： 也就是说，要么所有的消息都被成功的写入到了 log，要么一个都没写进去。这种语义的主要应用场景就是 Kafka topic 之间的 exactly-once 的数据传递。 并非所有使用场景都需要这么强的保证。对于延迟敏感的应用场景，我们允许生产者指定它需要的持久性级别。如果 producer 指定了它想要等待消息被提交，则可以使用10ms的量级。然而， producer 也可以指定它想要完全异步地执行发送，或者它只想等待直到 leader 节点拥有该消息（follower 节点有没有无所谓）。 现在让我们从 consumer 的视角来描述语义。 假设 consumer 要读取一些消息——它有几个处理消息和更新位置的选项。 Consumer 可以先读取消息，然后将它的位置保存到 log 中，最后再对消息进行处理。在这种情况下，消费者进程可能会在保存其位置之后，带还没有保存消息处理的输出之前发生崩溃。而在这种情况下，即使在此位置之前的一些消息没有被处理，接管处理的进程将从保存的位置开始。在 consumer 发生故障的情况下，这对应于“at-most-once”的语义，可能会有消息得不到处理。 Consumer 可以先读取消息，然后处理消息，最后再保存它的位置。在这种情况下，消费者进程可能会在处理了消息之后，但还没有保存位置之前发生崩溃。而在这种情况下，当新的进程接管后，它最初收到的一部分消息都已经被处理过了。在 consumer 发生故障的情况下，这对应于“at-least-once”的语义。 在许多应用场景中，消息都设有一个主键，所以更新操作是幂等的（相同的消息接收两次时，第二次写入会覆盖掉第一次写入的记录）。 多副本创建副本的单位是 topic 的 partition ，正常情况下， 每个分区都有一个 leader 和零或多个 followers 。 总的副本数是包含 leader 的总和。 所有的读写操作都由 leader 处理，一般 partition 的数量都比 broker 的数量多的多，各分区的 leader 均 匀的分布在brokers 中。所有的 followers 节点都同步 leader 节点的日志，日志中的消息和偏移量都和 leader 中的一致。（当然, 在任何给定时间, leader 节点的日志末尾时可能有几个消息尚未被备份完成）。 Followers 节点就像普通的 consumer 那样从 leader 节点那里拉取消息并保存在自己的日志文件中。Followers 节点可以从 leader 节点那里批量拉取消息日志到自己的日志文件中。 与大多数分布式系统一样，自动处理故障需要精确定义节点 “alive” 的概念。Kafka 判断节点是否存活有两种方式。 节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连接。 如果节点是个 follower ，它必须能及时的同步 leader 的写操作，并且延时不能太久。 我们认为满足这两个条件的节点处于 “in sync” 状态，区别于 “alive” 和 “failed” 。 Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本列表中移除。 同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。 在所有时间里，Kafka 保证只要有至少一个同步中的节点存活，提交的消息就不会丢失。 Kafka分配Replica的算法如下： 将所有Broker（假设共n个Broker）和待分配的Partition排序 将第i个Partition分配到第（i mod n）个Broker上 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上 可用性和持久性保证向 Kafka 写数据时，producers 设置 ack 是否提交完成， 0：不等待broker返回确认消息, 1: leader保存成功返回或, -1(all): 所有备份都保存成功返回. 请注意. 设置 “ack = all” 并不能保证所有的副本都写入了消息。默认情况下，当 acks = all 时，只要 ISR 副本同步完成，就会返回消息已经写入。例如，一个 topic 仅仅设置了两个副本，那么只有一个 ISR 副本，那么当设置acks = all时返回写入成功时，剩下了的那个副本数据也可能数据没有写入。 尽管这确保了分区的最大可用性，但是对于偏好数据持久性而不是可用性的一些用户，可能不想用这种策略，因此，我们提供了两个topic 配置，可用于优先配置消息数据持久性： 禁用 unclean leader 选举机制 - 如果所有的备份节点都挂了,分区数据就会不可用，直到最近的 leader 恢复正常。这种策略优先于数据丢失的风险， 参看上一节的 unclean leader 选举机制。 指定最小的 ISR 集合大小，只有当 ISR 的大小大于最小值，分区才能接受写入操作，以防止仅写入单个备份的消息丢失造成消息不可用的情况，这个设置只有在生产者使用 acks = all 的情况下才会生效，这至少保证消息被 ISR 副本写入。此设置是一致性和可用性 之间的折衷，对于设置更大的最小ISR大小保证了更好的一致性，因为它保证将消息被写入了更多的备份，减少了消息丢失的可能性。但是，这会降低可用性，因为如果 ISR 副本的数量低于最小阈值，那么分区将无法写入。 高可用ISRKafka 动态维护了一个同步状态的备份的集合 （a set of in-sync replicas）， 简称 ISR ，在这个集合中的节点都是和 leader 保持高度一致的，只有这个集合的成员才 有资格被选举为 leader，一条消息必须被这个集合 所有 节点读取并追加到日志中了，这条消息才能视为提交。这个 ISR 集合发生变化会在 ZooKeeper 持久化，正因为如此，这个集合中的任何一个节点都有资格被选为 leader 。因为 ISR 模型和 f+1 副本，一个 Kafka topic 冗余 f 个节点故障而不会丢失任何已经提交的消息。 Kafka 对于数据不会丢失的保证，是基于至少一个节点在保持同步状态，一旦分区上的所有备份节点都挂了，就无法保证了。但是，实际在运行的系统需要去考虑假设一旦所有的备份都挂了，怎么去保证数据不会丢失，这里有两种实现的方法 等待一个 ISR 的副本重新恢复正常服务，并选择这个副本作为领 leader （它有极大可能拥有全部数据）。 选择第一个重新恢复正常服务的副本（不一定是 ISR 中的）作为leader。 kafka 默认选择第二种策略，当所有的 ISR 副本都挂掉时，会选择一个可能不同步的备份作为 leader ，可以配置属性 unclean.leader.election.enable 禁用此策略，那么就会使用第 一种策略即停机时间优于不同步。 如何选举Leader 最简单最直观的方案是，所有Follower都在Zookeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（Zookeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。 但是该方法会有3个问题： split-brain 这是由Zookeeper的特性引起的，虽然Zookeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致 herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整 Zookeeper负载过重 每个Replica都要为此在Zookeeper上注册一个Watch，当集群规模增加到几千个Partition时Zookeeper负载会过重。 Kafka 0.8.* 的Leader Election方案解决了上述问题，它 在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。如果 controller 节点挂了，其他 存活的 broker 都可能成为新的 controller 节点。 【更详细的分析可看】Kafka设计解析（二）- Kafka High Availability （上） Custom RebalanceConsumer Rebalance 的算法如下： 将目标 Topic 下的所有 Partirtion 排序，存于 PT 对某 Consumer Group 下所有 Consumer 排序，存于 CG，第 i 个 Consumer 记为 Ci N=size(PT)/size(CG)，向上取整 解除 Ci 对原来分配的 Partition 的消费权（i 从 0 开始） 将第 i * N 到（i+1）* N−1 个 Partition 分配给 Ci 根据 Kafka 社区 wiki，Kafka 作者正在考虑在还未发布的 0.9.x 版本中使用中心协调器 (Coordinator) 。大体思想是为所有 Consumer Group 的子集选举出一个 Broker 作为 Coordinator，由它 Watch Zookeeper，从而判断是否有 Partition 或者 Consumer 的增减，然后生成 Rebalance 命令，并检查是否这些 Rebalance 在所有相关的 Consumer 中被执行成功，如果不成功则重试，若成功则认为此次 Rebalance 成功（这个过程跟 Replication Controller 非常类似）: Consumer 1) Consumer 启动时，先向 Broker 列表中的任意一个 Broker 发送 ConsumerMetadataRequest，并通过 ConsumerMetadataResponse 获取它所在 Group 的 Coordinator 信息。 2）Consumer 连接到 Coordinator 并发送 HeartbeatRequest: 如果返回的 HeartbeatResponse 没有任何错误码，Consumer 继续 fetch 数据。 若其中包含 IllegalGeneration 错误码，即说明 Coordinator 已经发起了 Rebalance 操作，此时 Consumer 停止 fetch 数据，commit offset，并发送 JoinGroupRequest 给它的 Coordinator，并在 JoinGroupResponse 中获得它应该拥有的所有 Partition 列表和它所属的 Group 的新的 Generation ID。此时 Rebalance 完成，Consumer 开始 fetch 数据。 故障检测机制Consumer 成功加入 Group 后，Consumer 和相应的 Coordinator 同时开始故障探测程序。 Consumer 向 Coordinator 发起周期性的 Heartbeat（HeartbeatRequest）并等待响应，该周期为 session.timeout.ms/heartbeat.frequency。 若 Consumer 在 session.timeout.ms 内未收到 HeartbeatResponse，或者发现相应的 Socket channel 断开，它即认为 Coordinator 已宕机并启动 Coordinator 探测程序。 若 Coordinator 在 session.timeout.ms 内没有收到一次 HeartbeatRequest，则它将该 Consumer 标记为宕机状态并为其所在 Group 触发一次 Rebalance 操作。 Coordinator Failover 过程中，Consumer 可能会在新的 Coordinator 完成 Failover 过程之前或之后发现新的 Coordinator 并向其发送 HeatbeatRequest。 对于后者，新的 Cooodinator 可能拒绝该请求，致使该 Consumer 重新探测 Coordinator 并发起新的连接请求。 如果该 Consumer 向新的 Coordinator 发送连接请求太晚，新的 Coordinator 可能已经在此之前将其标记为宕机状态而将之视为新加入的 Consumer 并触发一次 Rebalance 操作。 Coordinator1）稳定状态下，Coordinator 通过上述故障探测机制跟踪其所管理的每个 Group 下的每个 Consumer 的健康状态。 2）刚启动时或选举完成后，Coordinator 从 Zookeeper 读取它所管理的 Group 列表及这些 Group 的成员列表。如果没有获取到 Group 成员信息，它不会做任何事情直到某个 Group 中有成员注册进来。 3）在 Coordinator 完成加载其管理的 Group 列表及其相应的成员信息之前，它将为 HeartbeatRequest，OffsetCommitRequest 和 JoinGroupRequests 返回 CoordinatorStartupNotComplete 错误码。此时，Consumer 会重新发送请求。 4）Coordinator 会跟踪被其所管理的任何 Consumer Group 注册的 Topic 的 Partition 的变化，并为该变化触发 Rebalance 操作。创建新的 Topic 也可能触发 Rebalance，因为 Consumer 可以在 Topic 被创建之前就已经订阅它了。 Coordinator 发起 Rebalance 操作流程如下所示。 Coordinator Failover如前文所述，Rebalance 操作需要经历如下几个阶段 1）Topic/Partition 的改变或者新 Consumer 的加入或者已有 Consumer 停止，触发 Coordinator 注册在 Zookeeper 上的 watch，Coordinator 收到通知准备发起 Rebalance 操作。 2）Coordinator 通过在 HeartbeatResponse 中返回 IllegalGeneration 错误码发起 Rebalance 操作。 3）Consumer 发送 JoinGroupRequest 4）Coordinator 在 Zookeeper 中增加 Group 的 Generation ID 并将新的 Partition 分配情况写入 Zookeeper 5）Coordinator 发送 JoinGroupResponse 【摘自】Kafka 设计解析（四）：Kafka Consumer 解析]]></content>
      <categories>
        <category>中间件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql入门]]></title>
    <url>%2F2019%2F09%2F22%2FMysql%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[重识Count()COUNT(列名)、COUNT(常量)和COUNT(*)之间区别前面我们提到过 COUNT(expr) 用于做行数统计，统计的是expr不为NULL的行数，那么 COUNT(列名)、 COUNT(常量) 和 COUNT(*) 这三种语法中，expr分别是列名、 常量 和 *。 那么列名、 常量 和 * 这三个条件中，常量 是一个固定值，肯定不为NULL。* 可以理解为查询整行，所以肯定也不为NULL，那么就只有列名的查询结果有可能是NULL了。 所以， COUNT(常量) 和 COUNT(*)表示的是直接查询符合条件的数据库表的行数。而COUNT(列名)表示的是查询符合条件的列的值不为NULL的行数。 除了查询得到结果集有区别之外，COUNT(*)相比COUNT(常量) 和 COUNT(列名)来讲，COUNT(*)是SQL92定义的标准统计行数的语法，因为他是标准语法，所以MySQL数据库对他进行过很多优化。 COUNT(*)的优化MyISAM和InnoDB有很多区别，其中有一个关键的区别和我们接下来要介绍的COUNT(*)有关，那就是: MyISAM不支持事务，MyISAM中的锁是表级锁； 而InnoDB支持事务，并且支持行级锁; 因为MyISAM的锁是表级锁，同一张表上面的操作需要串行进行，所以，MyISAM做了一个简单的优化: 它可以把表的总行数单独记录下来，如果从一张表中使用COUNT(*)进行查询的时候，可以直接返回这个记录下来的数值就可以了，当然，前提是不能有where条件。 MyISAM之所以可以把表中的总行数记录下来供COUNT(*)查询使用，那是因为MyISAM数据库是表级锁，不会有并发的数据库行数修改，所以查询得到的行数是准确的。 但是，对于InnoDB来说，就不能做这种缓存操作了，因为InnoDB支持事务，其中大部分操作都是行级锁，所以可能表的行数可能会被并发修改，那么缓存记录下来的总行数就不准确了。 在InnoDB中，使用COUNT(*)查询行数的时候，不可避免的要进行扫表了，那么，就可以在扫表过程中下功夫来优化效率了。从MySQL 8.0.13开始，针对InnoDB的 SELECT COUNT(*) FROM tbl_name 语句，确实在扫表的过程中做了一些优化。前提是查询语句中不包含WHERE或GROUP BY等条件。 COUNT(*)的目的只是为了统计总行数，所以，他根本不关心自己查到的具体值，所以，他如果能够在扫表的过程中，选择一个成本较低的索引进行的话，那就可以大大节省时间。 我们知道，InnoDB中索引分为聚簇索引（主键索引）和非聚簇索引（非主键索引），聚簇索引的叶子节点中保存的是整行记录，而非聚簇索引的叶子节点中保存的是该行记录的主键的值。 相比之下，非聚簇索引要比聚簇索引小很多，所以MySQL会优先选择最小的非聚簇索引来扫表。所以，当我们建表的时候，除了主键索引以外，创建一个非主键索引还是有必要的。 至此，我们介绍完了MySQL数据库对于COUNT(*)的优化，这些优化的前提都是查询语句中不包含WHERE以及GROUP BY条件。 COUNT(*)和COUNT(1)看下MySQL官方文档是怎么说的： InnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference. 画重点：same way , no performance difference。所以，对于COUNT(1)和COUNT(*)，MySQL的优化是完全一样的，根本不存在谁比谁快！ 建议使用COUNT(*)！因为这个是SQL92定义的标准统计行数的语法。 COUNT(字段)最后，就是我们一直还没提到的COUNT(字段)，他的查询就比较简单粗暴了，就是进行全表扫描，然后判断指定字段的值是不是为NULL，不为NULL则累加。相比COUNT(*)，COUNT(字段)多了一个步骤就是判断所查询的字段是否为NULL，所以他的性能要比COUNT(*)慢。 事务及原理Mysql事务及其原理 事务的 ACID 属性 原子性（Atomicity）：作为逻辑工作单元，一个事务里的所有操作的执行，要么全部成功，要么全部失败。 一致性（Consistency）：数据库从一个一致性状态变换到另外一个一致性状态，数据库的完整性不会受到破坏。 隔离性（Isolation）：通常来说，一个事务所做的修改在最终提交前，对其他事务是不可见的。为什么是通常来说，为了提高事务的并发引出不同的隔离级别，具体参考下一章节。 持久性（Durability）：一旦事务提交，则其所做的修改就会永久保存到数据库中，即使系统故障，修改的数据也不会丢失。 事务的隔离级别为了尽可能的高并发，事务的隔离性被分为四个级别：读未提交、读已提交、可重复读和串行化。用户可以根据需要选择不同的级别。 未提交读（READ UNCOMMITTED）：一个事务还未提交，它的变更就能被别的事务看到。 例：事务 A 可以读到事务 B 修改的但还未提交的数据，会导致脏读（可能事务 B 在提交后失败了，事务 A 读到的数据是脏的）。 提交读（READ COMMITTED）：一个事务提交后，它的变更才能被其他事务看到。大多数据库系统的默认级别，但 Mysql 不是。 例：事务 A 只能读到事务 B 修改并提交后的数据，会导致不可重复读（事务 A 中执行两次查询，一次在事务 B 提交过程中，一次在事务 B 提交之后，会导致两次读取的结果不一致）。 可重复读（REPEATABLE READ）：未提交的事务的变更不能被其他事务看到，同时一次事务过程中多次读取同样记录的结果是一致的。 例：事务 A 在执行过程中多次获取某范围内的记录，事务 B 提交后在此范围内插入或者删除 N条记录，事务 A 执行过程中多次范围读会存在不一致，即幻读（Mysql 的默认级别，InnoDB 通过 MVVC 解决了幻读的问题）。 可串行化（SERIALIZABLE）：当两个事务间存在读写冲突时，数据库通过加锁强制事务串行执行，解决了前面所说的所有问题（脏读、不可重复读、幻读）。是最高隔离的隔离级别。 用表格可以更清晰的描述四种隔离级别的定义和可能存在的问题： 以上是对四种隔离级别的定义和初步认识，看《十分钟搞懂MySQL四种事务隔离级别》这篇文章可以彻底弄清楚他们之间的区别。 分库分表概念 分表 - 解决单表数据过大比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库 - 解决单库并发压力太大一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 水平拆分水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 拆表不拆库还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 分库分表策略 垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分； 水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好； 分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都 ok 了，但是每个库的表还是太大了，那么你就分表不分库，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用。 优点： 扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了 缺点：很容易产生热点问题，大量的流量都打在最新的数据上。实际生产用 range，要看场景； 按照某个字段 hash 一下均匀分散，这个较为常用。 优点：可以平均分配每个库的数据量和请求压力； 缺点：扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。 分库分表中间件比较常见的包括： Cobar阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 Cobar 集群，Cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 Atlas360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 Sharding-jdbc当当开源的，属于 client 层方案，目前已经更名为 ShardingSphere（后文所提到的 Sharding-jdbc，等同于 ShardingSphere）。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且截至 2019.4，已经推出到了 4.0.0-RC1 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 Mycat基于 Cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 Sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结综上，现在其实建议考量的，就是 Sharding-jdbc 和 Mycat，这两个都可以去考虑使用。 Sharding-jdbc 优点： 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，建议中小型公司选用; 缺点： 如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 Sharding-jdbc 的依赖； Mycat 优点： 对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞定就行； 优点： 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，最好是专门弄个人来研究和维护 Mycat，然后大量项目直接透明使用即; Mysql主从复制读写分离其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 主从复制原理主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 半同步复制也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，强制立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了； 并行复制从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 主从同步延时问题一般来说，如果主从延迟较为严重，有以下解决方案： 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计； 打开MySQL支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义； 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到; 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，如果这么做，读写分离的意义就丧失了。]]></content>
      <categories>
        <category>Mysql</category>
        <category>入门</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis入门教程]]></title>
    <url>%2F2019%2F09%2F19%2FRedis%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[redis 的线程模型redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。 来看客户端与 redis 的一次通信过程： 要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 这样便完成了一次通信。关于 Redis 的一次通信过程，推荐读者阅读《Redis 设计与实现——黄健宏》进行系统学习。 redis单线程模型为什么快 1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路I/O复用模型，非阻塞IO； 5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 6、C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。 以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨： （1）多路 I/O 复用模型 多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。 这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。 redis持久化持久化的两种方式 RDB：内存快照，全量；是对 redis 中的数据执行周期性的快照。 AOF：对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。类似于mysql的binlog, 存储紧张时会进行压缩，指令合并 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点优点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 缺点 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点优点 AOF可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF日志文件以append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 缺点 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF开启后，支持的写QPS会比RDB支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的 基于命令日志 merge 回放 的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 持久化方案 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis支持 同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 redis高并发和高可用redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。 redis主从架构单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication 的核心机制 redis 采用异步方式（master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node）复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 redis 主从复制的核心原理当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制： （1）master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中； （2）RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave； （3）slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中； （4）接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。 slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 主从复制的断点续传从 redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。 master node会 在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization。 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。 无磁盘化复制master 在内存中直接创建 RDB，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。 1234repl-diskless-sync yes# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来repl-diskless-sync-delay 5 复制的完整流程slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的host和ip，但是复制流程没开始。 slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。 全量复制 master 执行 bgsave ，在本地生成一份 rdb 快照文件。 master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s) master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。 1client-output-buffer-limit slave 256MB 64MB 60 slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务。 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。 增量复制 如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。 master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。 master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。 heartbeat主从节点互相都会发送 heartbeat 信息。 master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。 redis基于哨兵实现高可用哨兵的介绍sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很很不合理了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 哨兵集群必须部署 2 个以上节点, 经典的 3 节点哨兵集群是这样的 123456789 +----+ | M1 | | S1 | +----+ |+----+ | +----+| R2 |----+----| R3 || S2 | | S3 |+----+ +----+ 配置 quorum=2，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移. redis 哨兵主备切换的数据丢失问题 异步复制导致的数据丢失 因为 master -&gt; slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。 脑裂导致的数据丢失脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。 此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。 数据丢失问题的解决方案进行如下配置： 12min-slaves-to-write 1 ## 表示要求至少有1个slavemin-slaves-max-lag 10 ## 表示数据复制和同步的延迟不能超过10秒 减少异步复制数据的丢失有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，超过10s，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。 减少脑裂的数据丢失如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。 sdown 和 odown 转换机制 sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 （1）sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds 指定的毫秒数之后，就主观认为 master 宕机了； （2）如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。 哨兵集群的自动发现机制 哨兵互相之间的发现，是通过 redis 的 pub/sub 系统实现的，每个哨兵都会往 __sentinel__:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。 每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的__sentinel__:hello channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。 每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。 每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。 slave 配置的自动纠正哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。 slave-&gt;master 选举算法如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息： 跟 master 断开连接的时长 slave 优先级 复制 offset run id 如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对 slave 进行排序： 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。 quorum 和 majority每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。 如果 quorum &lt; majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。 但是如果 quorum &gt;= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。 configuration epoch哨兵会对一套 redis master+slaves 进行监控，有相应的监控的配置。 执行切换的那个哨兵，会从要切换到的新 master（salve-&gt;master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。 configuration 传播哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。 这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。 redis集群集群介绍Redis集群是一个提供在多个Redis节点间共享数据的程序集。主要是针对海量数据+高并发+高可用的场景。 Redis集群并不支持处理多个keys的命令, 因为这需要在不同的节点间移动数据, 从而达不到像Redis那样的性能, 在高负载的情况下可能会导致不可预料的错误. Redis 集群通过分区来提供一定程度的可用性,在实际环境中当某个节点宕机或者不可达的情况下继续处理命令. Redis 集群的优势: 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。 cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本原理集群元数据的维护采用Gossip协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 gossip好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力； 不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后； 10000端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。 gossip 协议gossip 协议包含多种消息，包含 ping,pong,meet,fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。1redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机了。 ping 消息深入ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。 每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。 Redis 集群的数据分片Redis 集群引入了 哈希槽的概念. Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,举个例子,比如当前集群有3个节点,那么: 123节点 A 包含 0 到 5500号哈希槽.节点 B 包含5501 到 11000 号哈希槽.节点 C 包含11001 到 16384号哈希槽. 这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我想移除节点A,需要将A中的槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态. redis cluster 的高可用与主备切换原理redis cluster 的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。 从节点过滤对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。 从节点选举每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。 从节点执行主备切换，从节点切换为主节点。]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>入门</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用消息队列简介]]></title>
    <url>%2F2019%2F09%2F05%2F%E5%B8%B8%E7%94%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[消息队列常见场景消息队列常见的使用场景比较核心的有 4 个：解耦、异步、削峰、分布式事务。 解耦 通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 异步看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了。 削峰每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 分布式事务 发送消息开启确认发布机制，MQ收到后会返回回执，超时未收到回执，发送方可重新发送；消费者开启手动ack模式，控制消息的重发、清楚、丢弃，保证数据可靠处理。 队列高可用RabbitMq入门以及使用教程 RabbitMQ 的应用场景以及基本原理介绍 RabbitMQ两种集群模式 普通模式默认的集群模式。 场景1、客户端直接连接队列所在节点如果有一个消息生产者或者消息消费者通过amqp-client的客户端连接至节点1进行消息的发布或者订阅，那么此时的集群中的消息收发只与节点1相关，这个没有任何问题； 场景2、客户端连接的是非队列数据所在节点如果消息生产者所连接的是节点2或者节点3，此时队列1的完整数据不在该两个节点上，那么在发送消息过程中这两个节点主要起了一个路由转发作用，根据这两个节点上的元数据（也就是上文提到的：指向queue的owner node的指针）转发至节点1上，最终发送的消息还是会存储至节点1的队列1上。同样，如果消息消费者所连接的节点2或者节点3，那这两个节点也会作为路由节点起到转发作用，将会从节点1的队列1中拉取消息进行消费。 特点： 非高可用 主要是提高吞吐量的 镜像模式把需要的队列做成镜像队列，存在于多个节点，属于RabbitMQ的HA方案跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。 高可用分布式集群 基于镜像模式，对于消息的生产和消费者可以通过HAProxy的软负载将请求分发至RabbitMQ集群中的Node1～Node7节点，其中Node8～Node10的三个节点作为磁盘节点保存集群元数据和配置信息。 消息不丢失RabbitMQ Kafaka 消费者:关闭自动提交 offset，在处理完之后自己手动提交offset； kafka:设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 生产者:如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 消息顺序性场景描述举个例子，数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。 常见消息队列对比 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。]]></content>
      <categories>
        <category>中间件</category>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot集成Mybatis]]></title>
    <url>%2F2019%2F08%2F29%2FSpring-Boot%E9%9B%86%E6%88%90Mybatis%2F</url>
    <content type="text"><![CDATA[SpringBoot 整合 Mybatis 有两种常用的方式，一种就是我们常见的 xml 的方式 ，还有一种是全注解的方式。我觉得这两者没有谁比谁好，在 SQL 语句不太长的情况下，我觉得全注解的方式一定是比较清晰简洁的。但是，复杂的 SQL 确实不太适合和代码写在一起。 下面记录一下配置过程： 创建工程创建一个spring boot的maven工程, pom核心内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;mysql.version&gt;5.1.46&lt;/mysql.version&gt; &lt;mybatis.version&gt;1.3.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--msyql核心驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;!--这个是官方的mybatis依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;mybatis-generator&lt;/id&gt; &lt;phase&gt;deploy&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;!-- Mybatis-Generator 工具配置文件的位置 --&gt; &lt;configurationFile&gt;src/main/resources/mybatis-generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;!-- 这个是自动生成mapper等的依赖，必须得加--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!--这个是自动生成mapper等的依赖，必须得加--&gt; &lt;!--https://mvnrepository.com/artifact/org.mybatis.generator/mybatis-generator-core--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置文件application.properties 12345678910111213141516171819## mapper xml 文件地址mybatis.mapper-locations=classpath*:mapper/*Mapper.xml#数据库设置spring.datasource.type=com.alibaba.druid.pool.DruidDataSource##数据库urlspring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?characterEncoding=utf8&amp;useSSL=false##数据库用户名spring.datasource.username=xxxx##数据库密码spring.datasource.password=xxxx##数据库驱动spring.datasource.driver-class-name=com.mysql.jdbc.Driver# Mybatis Generator configuration# dao类和实体类的位置mybatis.project =src/main/java# mapper文件的位置mybatis.resources=src/main/resources 自动生成xml方式generatorConfig配置在src/main/resource下创建mybatis-generator文件夹，在文件夹下创建generatorConfig.xml文件，制定generator生成规则： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;!-- 配置生成器 --&gt;&lt;generatorConfiguration&gt; &lt;!--执行generator插件生成文件的命令： call mvn mybatis-generator:generate -e --&gt; &lt;!-- 引入配置文件 --&gt; &lt;properties resource="application.properties"/&gt; &lt;!--classPathEntry:数据库的JDBC驱动,换成你自己的驱动位置 可选 --&gt; &lt;!--&lt;classPathEntry location="~/Downloads/mysql-connector-java-5.1.24-bin.jar"/&gt;--&gt; &lt;!-- 一个数据库一个context --&gt; &lt;!--defaultModelType="flat" 大数据字段，不分表 --&gt; &lt;context id="MysqlTables" targetRuntime="MyBatis3Simple" defaultModelType="flat"&gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name="autoDelimitKeywords" value="true"/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name="javaFileEncoding" value="utf-8"/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name="beginningDelimiter" value="`"/&gt; &lt;property name="endingDelimiter" value="`"/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name="javaFormatter" value="org.mybatis.generator.api.dom.DefaultJavaFormatter"/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name="xmlFormatter" value="org.mybatis.generator.api.dom.DefaultXmlFormatter"/&gt; &lt;plugin type="org.mybatis.generator.plugins.SerializablePlugin"/&gt; &lt;plugin type="org.mybatis.generator.plugins.ToStringPlugin"/&gt; &lt;!-- 为了防止生成的代码中有很多注释，比较难看，加入下面的配置控制 --&gt; &lt;commentGenerator&gt; &lt;property name="suppressAllComments" value="true"/&gt;&lt;!-- 是否取消注释 --&gt; &lt;property name="suppressDate" value="true"/&gt; &lt;!-- 是否生成注释带时间戳--&gt; &lt;/commentGenerator&gt; &lt;!-- jdbc连接 --&gt; &lt;jdbcConnection driverClass="$&#123;spring.datasource.driver-class-name&#125;" connectionURL="$&#123;spring.datasource.url&#125;" userId="$&#123;spring.datasource.username&#125;" password="$&#123;spring.datasource.password&#125;"/&gt; &lt;!-- 类型转换 --&gt; &lt;javaTypeResolver&gt; &lt;!-- 是否使用bigDecimal， false可自动转化以下类型（Long, Integer, Short, etc.） --&gt; &lt;property name="forceBigDecimals" value="false"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成实体类地址 --&gt; &lt;javaModelGenerator targetPackage="com.austin.entity" targetProject="$&#123;mybatis.project&#125;"&gt; &lt;!-- 是否允许子包，即targetPackage.schemaName.tableName --&gt; &lt;property name="enableSubPackages" value="false"/&gt; &lt;!-- 是否对model添加 构造函数 --&gt; &lt;property name="constructorBased" value="true"/&gt; &lt;!-- 是否对类CHAR类型的列的数据进行trim操作 --&gt; &lt;property name="trimStrings" value="true"/&gt; &lt;!-- 建立的Model对象是否 不可改变 即生成的Model对象不会有 setter方法，只有构造方法 --&gt; &lt;property name="immutable" value="false"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成maperxml文件, targetPackage表示xml文件存放地址 --&gt; &lt;sqlMapGenerator targetPackage="mapper" targetProject="$&#123;mybatis.resources&#125;"&gt; &lt;property name="enableSubPackages" value="false"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成mapxml对应client，也就是接口dao --&gt; &lt;javaClientGenerator targetPackage="com.austin.dao" targetProject="$&#123;mybatis.project&#125;" type="XMLMAPPER"&gt; &lt;property name="enableSubPackages" value="false"/&gt; &lt;/javaClientGenerator&gt; &lt;!--table可以有多个,每个数据库中的表都可以写一个table， tableName表示要匹配的数据库表名,也可以在tableName属性中通过使用%通配符来匹配所有数据库表,只有匹配的表才会自动生成文件 domainObjectName是生成的实体类名称,可以不写，默认会用表名的驼峰格式 --&gt; &lt;table tableName="tbl_user" domainObjectName="UserInfo" enableCountByExample="true" enableUpdateByExample="true" enableDeleteByExample="true" enableSelectByExample="true" selectByExampleQueryId="true"&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;!-- 数据库表主键 --&gt; &lt;generatedKey column="id" sqlStatement="Mysql" identity="true"/&gt; &lt;/table&gt; &lt;table tableName="tbl_role" domainObjectName="RoleInfo" enableCountByExample="true" enableUpdateByExample="true" enableDeleteByExample="true" enableSelectByExample="true" selectByExampleQueryId="true"&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;!-- 数据库表主键 --&gt; &lt;generatedKey column="id" sqlStatement="Mysql" identity="true"/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 其中，如果在pom中配置了红框中所示的依赖，则不需要额外指定classPathEntry路径。 工程结构 执行生成执行可以采用两种方式 命令行pom文件所在路径执行： 1mvn mybatis-generator:generate IDE 生成结果 注解方式全注解的方式，这种方式和后面提到的xml的方式的区别仅仅在于 一个将 sql 语句写在 java 代码中，一个写在 xml 配置文件中。 123456789@Mapperpublic interface StudentMapper &#123; @Insert("insert into tbl_student(name, class_name, age) values (#&#123;name&#125;, #&#123;className&#125;, #&#123;age&#125;)") void insert(@Param("name") String name, @Param("className") String className, @Param("age") int age); @Select("select * from tbl_student") List&lt;Student&gt; findAllStudent();&#125; 常见问题（1）mapper文件无法注入出错原因： 启动类上需要通过@MapperScan指定mpper文件路径。 代码路径https://github.com/austin-brant/mybatis-spring-boot-demo]]></content>
      <categories>
        <category>数据库中间件</category>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[多线程]Volatile和Synchronization详解]]></title>
    <url>%2F2019%2F08%2F14%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B-Volatile%E5%92%8CSynchronization%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在多线程并发编程中synchronized和volatile都扮演着重要的角色，volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。本文将深入分析volatile和synchronize的原理，通过深入分析帮助我们正确地使用volatile和synchronize关键字。首先先了解下并发编程的三大概念。 并发编程的三大概念可见性 可见性的意思是当一个线程修改一个共享变量时，另外一个线程能立马读到这个修改的值。 原子性 原子（atomic）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为“不可被中断的一个或一系列操作”。即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。但在多处理器上实现原子操作就变得有点复杂。 比如 a=0；（a非long和double类型） 这个操作是不可分割的，那么我们说这个操作时原子操作。再比如：a++； 这个操作实际是a = a + 1；是可分割的，所以他不是一个原子操作。非原子操作都会存在线程安全问题，需要我们使用同步技术（sychronized）来让它变成一个原子操作。一个操作是原子操作，那么我们称它具有原子性。java的concurrent包下提供了一些原子类，我们可以通过阅读API来了解这些原子类的用法。比如：AtomicInteger、AtomicLong、AtomicReference等。 有序性 有序性就是程序执行的顺序按照代码的先后顺序执行。 什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。在Java里面，可以通过volatile关键字来保证一定的“有序性”。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 Java内存模型 Java内存模型规定了所有的变量都存储在主内存中。为了提高处理速度，处理器不直接和主内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他，俗称工作内存），线程对变量的所有操作（读取，赋值）都必须在工作内存中进行，但操作完不知道何时会写到内存。 不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。 Volatile的定义与实现原理Java语言规范第3版中对volatile的定义如下：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过 排他锁 单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 让我们在X86处理器下通过工具获取JIT编译器生成的汇编指令来查看对volatile进行写操作时，CPU会做什么事情。 Java代码如下。 1instance = new Singleton(); // instance是volatile变量 转变成汇编代码，如下。 10x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); 有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事情: 1）将当前处理器缓存行的数据写回到系统内存。 2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过 嗅探在总线上传播的数据 来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会 重新从系统内存中把数据读到处理器缓存里 。 具体讲解volatile的两条实现原则。 1）Lock前缀指令会引起处理器缓存回写到内存 Lock前缀指令导致在执行指令期间，声言处理器的LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以独占任何共享内存。但是，在新的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销的比较大，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为 “缓存锁定” ，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。 2）一个处理器的缓存回写到内存会导致其他处理器的缓存无效 IA-32处理器和Intel 64处理器使用 MESI（修改、独占、共享、无效）控制协议 去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能 嗅探其他处理器访问系统内存和它们的内部缓存。 处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。 例如，在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它自己的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。 Synchronized的实现在多线程并发编程中synchronized一直是元老级角色，很多人都会称呼它为重量级锁。但是，随着Java SE 1.6对synchronized进行了各种优化之后，有些情况下它就并不那么重了。下面详细介绍Java SE 1.6中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁，以及锁的存储结构和升级过程。 先来看下利用synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现为以下3种形式。 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象。 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。 从JVM规范中可以看到Synchonized在JVM里的实现原理，JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。代码块同步是使用 monitorenter 和monitorexit 指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是，方法的同步同样可以使用这两个指令来实现。 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 Java对象头synchronized用的锁是存在Java对象头里的。如果对象是数组类型，则虚拟机用3个字宽（Word）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，1字宽等于4字节，即32bit。 Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如下表所示： 在运行期间，Mark Word里存储的数据会随着锁标志位的变化而变化。Mark Word可能变化为存储以下4种数据，如下表： 在64位虚拟机下，Mark Word是64bit大小的，其存储结构如下表所示： 锁的升级Java SE 1.6为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”，在Java SE 1.6中，锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率，下文会详细分析。 偏向锁HotSpot的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。 如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 (1) 偏向锁的撤销 偏向锁使用了一种 等到竞争出现才释放锁 的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。 偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。 它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着; 如果线程不处于活动状态，则将对象头设置成无锁状态； 如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 下图中的线程1演示了偏向锁初始化的流程，线程2演示了偏向锁撤销的流程。 (2) 关闭偏向锁 偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟： 1-XX:BiasedLockingStartupDelay=0 如果你确定应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁： 1-XX:-UseBiasedLocking=false 那么程序默认会进入轻量级锁状态。 轻量级锁（1）轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并 将对象头中的Mark Word复制到锁记录中，官方称 为Displaced Mark Word 。然后线程尝试 使用CAS将对象头中的Mark Word替换为指向锁记录的指针。 如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 （2）轻量级锁解锁 轻量级解锁时，会 使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。下图是两个线程同时争夺锁，导致锁膨胀的流程图。 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 锁的对比 锁 优点 缺点 适用场景 无锁 高性能 基本上无并发，不存在线程竞争 偏向锁 加锁解锁不需要额外消耗，和执行非同步方法相比仅存在纳秒级差距 若线程间存在锁竞争，会带来额外的锁撤销消耗 只有一个线程访问的同步块场景 轻量级锁 竞争的线程一直自旋不会阻塞，提高了程序响应速度 未得到锁的线程自旋消耗CPU 追求响应时间，同步块执行速度非常快 重量级锁 线程竞争不使用自旋，不消耗CPU 线程竞争会直接阻塞，进入内核态，响应时间慢 追求吞吐量，同步块执行时间较长 Volatile vs Synchronize volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的; volatile仅能实现变量的修改可见性，并不能保证原子性；synchronized则可以保证变量的修改可见性和原子性; volatile不会造成线程的阻塞; synchronized可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化(禁止指令重排)；synchronized标记的变量可以被编译器优化 volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 本文主要摘抄于 《Java并发编程的艺术》（第二章） 只作为个人读书记录。]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>Synchronization</tag>
        <tag>Volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal源码解析]]></title>
    <url>%2F2019%2F08%2F13%2FThreadLocal%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[ThreadLocal的作用ThreadLocal的作用是提供线程内的局部变量，说白了，就是在各线程内部创建一个变量的副本，相比于使用各种锁机制访问变量，ThreadLocal的思想就是用空间换时间，使各线程都能访问属于自己这一份的变量副本，变量值不互相干扰，减少同一个线程内的多个函数或者组件之间一些公共变量传递的复杂度。 ThreadLocal特性及使用场景 1、方便同一个线程使用某一对象，避免不必要的参数传递； 2、线程间数据隔离（每个线程在自己线程里使用自己的局部变量，各线程间的ThreadLocal对象互不影响）； 3、获取数据库连接、Session、关联ID（比如日志的uniqueID，方便串起多个日志）； ThreadLocal应注意 1、ThreadLocal并未解决多线程访问共享对象的问题； 2、ThreadLocal并不是每个线程拷贝一个对象，而是直接new（新建）一个； 3、如果ThreadLocal.set()的对象是多线程共享的，那么还是涉及并发问题。 图解TreadLocal 每个线程可能有多个ThreadLocal，同一线程的各个ThreadLocal存放于同一个ThreadLocalMap中。 图解ThreadLocal(JDK8).vsdx原图下载地址：https://github.com/zxiaofan/JDK-Study/tree/master/src/java1/lang/threadLocal 内部类ThreadLocalMap1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; /** * ThreadLocalMap的key是ThreadLocal * value是Object（即我们所谓的“线程本地数据”） */ Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; /** * 初始容量，2的幂等次方 */ private static final int INITIAL_CAPACITY = 16; /** * 实际保存数据的数组，超过threshold会2倍扩容 */ private Entry[] table; /** * 实际存储的entry数量 */ private int size = 0; /** * 下次扩容的阈值 */ private int threshold; // Default to 0 /** * Set the resize threshold to maintain at worst a 2/3 load factor. */ private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; /** * 往后移动一位 */ private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0); &#125; /** * 往前移动一位 */ private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1); &#125; /** * Construct a new map initially containing (firstKey, firstValue). * ThreadLocalMaps懒汉模式, 等第一个entry被放入时才初始化. */ ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125; /** * 将父线程的ThreadLocalMaps内容复制过来 * Called only by createInheritedMap. */ private ThreadLocalMap(ThreadLocalMap parentMap) &#123; Entry[] parentTable = parentMap.table; int len = parentTable.length; setThreshold(len); table = new Entry[len]; for (int j = 0; j &lt; len; j++) &#123; Entry e = parentTable[j]; if (e != null) &#123; @SuppressWarnings("unchecked") ThreadLocal&lt;Object&gt; key = (ThreadLocal&lt;Object&gt;) e.get(); if (key != null) &#123; Object value = key.childValue(e.value); Entry c = new Entry(key, value); int h = key.threadLocalHashCode &amp; (len - 1); while (table[h] != null) h = nextIndex(h, len); table[h] = c; size++; &#125; &#125; &#125; &#125;&#125; ThreadLocalMap是定制的hashMap，仅用于维护当前线程的本地变量值。仅ThreadLocal类对其有操作权限，是Thread的私有属性。为避免占用空间较大或生命周期较长的数据常驻于内存引发一系列问题，hash table的key是弱引用WeakReferences。当空间不足时，会清理未被引用的entry。这时Entry里的key为null了，那么直到线程结束前，Entry中的value都是无法回收的，这里可能产生内存泄露。 SuppliedThreadLocal12345678910111213static final class SuppliedThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; private final Supplier&lt;? extends T&gt; supplier; SuppliedThreadLocal(Supplier&lt;? extends T&gt; supplier) &#123; this.supplier = Objects.requireNonNull(supplier); &#125; @Override protected T initialValue() &#123; return supplier.get(); &#125;&#125; SuppliedThreadLocal是JDK8新增的内部类，只是扩展了ThreadLocal的初始化值的方法而已，允许使用JDK8新增的Lambda表达式赋值。需要注意的是，函数式接口Supplier不允许为null。 初始化123456789101112131415161718192021222324252627282930313233public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocal初始化时会调用nextHashCode()方法初始化 * threadLocalHashCode，且threadLocalHashCode初始化后不可变。 * threadLocalHashCode可用来标记不同的ThreadLocal实例。 */ private final int threadLocalHashCode = nextHashCode(); private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; protected T initialValue() &#123; return null; &#125; /** * JDK8新增，支持Lambda表达式，和ThreadLocal重写的initialValue()效果一样。 */ public static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123; return new SuppliedThreadLocal&lt;&gt;(supplier); &#125; public ThreadLocal() &#123; &#125;&#125; ThreadLocal类变量有3个，其中2个是静态变量（包括一个常量），实际作为作为ThreadLocal实例的变量只有threadLocalHashCode这1个，而且已经初始化就不可变了。 其中withInitial()方法使用示例： 123456789101112131415161718192021public void jdk8Test()&#123; Supplier&lt;String&gt; supplier =new Supplier&lt;String&gt;()&#123; @Override public String get()&#123; return"supplier_new"; &#125; &#125;; threadLocal= ThreadLocal.withInitial(supplier); System.out.println(threadLocal.get()); // supplier_new // Lambda表达式 threadLocal= ThreadLocal.withInitial(()-&gt;"sup_new_2"); System.out.println(threadLocal.get()); // sup_new_2 ThreadLocal&lt;DateFormat&gt; localDate = ThreadLocal.withInitial(()-&gt;new SimpleDateFormat("yyyy-MM-dd")); System.out.println(localDate.get().format(new Date())); // 2017-01-22 ThreadLocal&lt;String&gt; local =new ThreadLocal&lt;&gt;().withInitial(supplier); System.out.println(local.get()); // supplier_new&#125; 源码分析get方法12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 直接看代码，可以分析主要有以下几步： 获取当前的Thread对象，通过getMap获取Thread内的ThreadLocalMap 如果map已经存在，以当前的ThreadLocal为键，获取Entry对象，并从从Entry中取出值 否则，调用setInitialValue进行初始化。 getMap123ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; getMap很简单，就是返回线程中ThreadLocalMap，跳到Thread源码里看，ThreadLocalMap是这么定义的： 1ThreadLocal.ThreadLocalMap threadLocals = null; 所以ThreadLocalMap还是定义在ThreadLocal里面的，我们前面已经说过ThreadLocalMap中的Entry定义，下面为了先介绍ThreadLocalMap的定义我们把setInitialValue放在前面说。 setInitialValue12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; setInititialValue在Map不存在的时候调用。 首先是调用initialValue生成一个初始的value值，深入initialValue函数，我们可知它就是返回一个null，如果创建ThreadLocal时调用withInitial() 方法指定了初始方法，则返回自定义值； 还是在get()一下Map，如果map存在，则直接map.set(), 这个函数会放在后文说； 如果map不存在，则会调用createMap()创建ThreadLocalMap。 createMap123void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 比较简单，就是调用了ThreadLocalMap内部类的构造函数而已。 map.getEntry12345678private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e);&#125; 首先是计算索引位置i，通过计算key的hash%(table.length-1)得出； 根据获取Entry，如果Entry存在且Entry的key恰巧等于ThreadLocal，那么直接返回Entry对象； 否则，也就是在此位置上找不到对应的Entry，那么就调用getEntryAfterMiss。 getEntryAfterMiss12345678910111213141516private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125; 这个方法我们还得结合上一步看，上一步是因为不满足 1e != null &amp;&amp; e.get() == key 才沦落到调用getEntryAfterMiss的，所以: 首先e如果为null的话，证明不存在value, 那么getEntryAfterMiss还是直接返回null的 如果是不满足e.get() == key，那么进入while循环，这里是不断循环，如果e一直不为空，那么就调用nextIndex，不断递增i，在此过程中一直会做两个判断： 如果 k == key, 那么代表找到了这个所需要的Entry，直接返回； 如果 k == null，那么证明这个Entry中key已经为null, 那么这个Entry就是一个过期对象，这里调用expungeStaleEntry清理该Entry。这里解答了前面留下的一个坑，即ThreadLocal Ref销毁时，ThreadLocal实例由于只有Entry中的一条弱引用指着，那么就会被GC掉，Entry的key没了，value可能会内存泄露的，其实在每一个get，set操作时都会不断清理掉这种key为null的Entry的。 为什么循环查找？ 这里你可以直接跳到下面的set方法，主要是因为处理哈希冲突的方法，我们都知道HashMap采用拉链法处理哈希冲突，即在一个位置已经有元素了，就采用链表把冲突的元素链接在该元素后面，而ThreadLocal采用的是开放地址法，即有冲突后，把要插入的元素放在要插入的位置后面为null的地方 具体关于这两种方法的区别可以参考：解决哈希（HASH）冲突的主要方法。 所以上面的循环就是因为我们在第一次计算出来的i位置不一定存在key与我们想查找的key恰好相等的Entry，所以只能不断在后面循环，来查找是不是被插到后面了，直到找到为null的元素，因为若是插入也是到null为止的。 expungeStaleEntry12345678910111213141516171819202122232425262728293031323334private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; // （1）删掉staleSlot位置value值 tab[staleSlot].value = null; tab[staleSlot] = null; size--; // （2）Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; // 删除元素后，需要重新移动存活的元素，因为查找时遇到null会终止 int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125; 看上面这段代码主要有两部分： (1) 这段主要是将i位置上的Entry的value设为null，Entry的引用也设为null，那么系统GC的时候自然会清理掉这块内存； (2) 这段就是扫描位置staleSlot之后，null之前的Entry数组，清除每一个key为null的Entry，同时若是key不为空，做rehash，调整其位置。 为什么要做rehash呢？ 因为我们在清理的过程中会把某个值设为null，那么这个值后面的区域如果之前是连着前面的，那么下次循环查找时，就会只查到null为止。 举个例子就是： …, &lt;key1(hash1), value1&gt;, &lt;key2(hash1), value2&gt;,… 即key1和key2的hash值相同, 此时，若插入 &lt;key3(hash2), value3&gt; 其hash计算的目标位置被 &lt;key2(hash1), value2&gt; 占了，于是往后寻找可用位置，hash表可能变为： …, &lt;key1(hash1), value1&gt;, &lt;key2(hash1), value2&gt;, &lt;key3(hash2), value3&gt;, … 此时，若 &lt;key2(hash1), value2&gt; 被清理，显然 &lt;key3(hash2), value3&gt;应该往前移(即通过rehash调整位置)，否则若以key3查找hash表，将会找不到key3。 set方法我们在get方法的循环查找那里也大概描述了set方法的思想，即开放地址法,下面看具体代码： 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 首先也是获取当前线程，根据线程获取到ThreadLocalMap，若是有ThreadLocalMap，则调用 1map.set(ThreadLocal&lt;?&gt; key, Object value) 若是没有则调用createMap创建。 map.set1234567891011121314151617181920212223242526private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 看上面这段代码： 首先还是根据key计算出位置i，然后查找i位置上的Entry， 若是Entry已经存在并且key等于传入的key，那么这时候直接给这个Entry赋新的value值。 若是Entry (e != null) 存在，但是key为null，则调用replaceStaleEntry来更换这个key为空的Entry 不断循环检测，直到遇到为null的地方，这时候要是还没在循环过程中return，那么就在这个null的位置新建一个Entry，并且插入，同时size增加1。 最后调用cleanSomeSlots，这个函数就不细说了，你只要知道内部还是调用了上面提到的expungeStaleEntry函数清理key为null的Entry就行了，最后返回是否清理了Entry，接下来再判断 sz&gt;thresgold ,这里就是判断是否达到了rehash的条件，达到的话就会调用rehash函数。 上面这段代码有两个函数还需要分析下，首先是: replaceStaleEntry123456789101112131415161718192021222324252627282930313233343536373839404142434445private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; // 向前找到key为null的位置 int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // staleSlot节点key为空，属于应该清理节点 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; // 更新value值 tab[i] = tab[staleSlot]; // i指向key为空节点 tab[staleSlot] = e; // staleSlot前面全不为空，i节点指向最新key为null位置 if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; // 更新key为空节点位置 if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; &#125; // If key not found, put new entry in stale slot tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); // If there are any other stale entries in run, expunge them if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 首先我们回想上一步是因为这个位置的Entry的key为null才调用replaceStaleEntry。 第1个for循环：我们向前找到key为null的位置，记录为slotToExpunge,这里是为了后面的清理过程，可以不关注了； 第2个for循环：我们从staleSlot起到下一个null为止，若是找到key和传入key相等的Entry，就给这个Entry赋新的value值，并且把它和staleSlot位置的Entry交换，然后调用CleanSomeSlots清理key为null的Entry。 若是一直没有key和传入key相等的Entry，那么就在staleSlot处新建一个Entry。函数最后再清理一遍空key的Entry。 说完replaceStaleEntry，还有个重要的函数是rehash以及rehash的条件： 首先是sz &gt; threshold时调用rehash rehash12345678private void rehash() &#123; // 清理全部空节点 expungeStaleEntries(); // Use lower threshold for doubling to avoid hysteresis if (size &gt;= threshold - threshold / 4) resize();&#125; 清理完空key的Entry后，如果size大于3/4的threshold，则调用resize函数： resize123456789101112131415161718192021222324252627private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; // Help the GC 下次gc会被回收 &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; setThreshold(newLen); size = count; table = newTab;&#125; 由源码我们可知每次扩容大小扩展为原来的2倍，然后再一个for循环里，清除空key的Entry，同时重新计算key不为空的Entry的hash值，把它们放到正确的位置上，再更新ThreadLocalMap的所有属性。 remove最后一个需要探究的就是remove函数，它用于在map中移除一个不用的Entry。也是先计算出hash值，若是第一次没有命中，就循环直到null，在此过程中也会调用expungeStaleEntry清除空key节点。代码如下： 1234567891011121314private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; e.clear(); expungeStaleEntry(i); return; &#125; &#125;&#125; 使用ThreadLocal的最佳实践我们发现无论是set,get还是remove方法，过程中key为null的Entry都会被擦除，那么Entry内的value也就没有强引用链，GC时就会被回收。那么怎么会存在内存泄露呢？但是以上的思路是假设你调用get或者set方法了，很多时候我们都没有调用过，所以最佳实践就是: 使用者需要手动调用remove函数，删除不再使用的ThreadLocal. 尽量将ThreadLocal设置成private static的，这样ThreadLocal会尽量和线程本身一起消亡。 问题与思考（1）如果有多个ThreadLocal都对同一个线程ThreadLocalMap写数据时，可能存在hash位置冲突，导致set()和get()效率显著下降； （2）ThreadLocal不能读取父线程的ThradLocalMap内容，需要使用InheritableThreadLocal；]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-入门]]></title>
    <url>%2F2019%2F08%2F12%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[数组数组，将元素存储到内存的连续位置中，是最基本的数据结构。在任何和编程相关的面试中，都会被问到和数组相关的问题，可以说是非常热门的考题之一。比如：将数组反转、对数组进行排序、搜索数组中的元素等。 数组数据结构的主要优点是如果知道索引就可以通过 O(l) 进行快速搜索，但是在数组中添加和删除元素的速度会很慢，因为数组一旦被创建，就无法更改其大小。如果需要创建更长或更短的数组，得先创建一个新数组，再把原数组中的所有元素复制到新创建的数组中。 解决数组相关问题的关键是要熟悉数组的数据结构和基本的构造，如循环、递归等等；下面给出了 10 道热门面试题帮助大家掌握知识并进行练习。 1. 给定一个 1-100 的整数数组，请找到其中缺少的数字。解决方法与代码：https://javarevisited.blogspot.com/2014/11/how-to-find-missing-number-on-integer-array-java.html 两种思路：（1）如果只缺少一个数字，n*(n+1)/2 - sum 就是缺失的数字；（2）缺失多个数字或是某些数字重复出现，则只能遍历一遍，记录哪些数字出现过，可用List或BitSet来记录。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 给定一个 1-n 的整数数组，请找到其中缺少的数字 * * @author austin * @since 2019/7/30 14:31 */public class FindMissingNumber &#123; /** * 支持多个重复或是缺失多个情况 * * @param arr 数组 * @param count n */ public static void printMissingNumber(int[] arr, int count) &#123; int missingCount = count - arr.length; BitSet bitSet = new BitSet(count); for (int i : arr) &#123; bitSet.set(i - 1); &#125; int lastIndex = 0; for (int i = 0; i &lt; missingCount; i++) &#123; lastIndex = bitSet.nextClearBit(lastIndex); System.out.println(++lastIndex); &#125; &#125; /** * 只支持缺失1个情况 */ public static void printSingleMissingNumber(int[] arr, int count) &#123; int exceptedSum = count * (count + 1) / 2; int sum = 0; for (int i : arr) &#123; sum += i; &#125; System.out.println(exceptedSum - sum); &#125; public static void main(String[] args) &#123; // 缺失一个数字 printMissingNumber(new int[] &#123;1, 2, 3, 4, 6&#125;, 6); // 缺失3个数字 printMissingNumber(new int[] &#123;1, 2, 3, 4, 6, 9, 8&#125;, 10); // 缺失一个数字 printSingleMissingNumber(new int[] &#123;1, 2, 3, 4, 6&#125;, 6); &#125;&#125; 2. 在给定的成对整数数组中，请找出所有总和等于给定数字的组合。解决方法与代码：http://javarevisited.blogspot.com/2014/08/how-to-find-all-pairs-in-array-of-integers-whose-sum-equal-given-number-java.html 三种思路：（1）两层循环，时间复杂度O(n^2);（2）存储到HashTable里，在HashTable里找（sum - arr[i]）值， 时间复杂度： O(n), 空间复杂度： O(n);（3）先排序O(nlogn), 然后首尾想加，往中间靠； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import java.util.Arrays;import java.util.HashSet;import java.util.Set;/** * 在给定的成对整数数组中，请找出所有总和等于给定数字的组合 * * @author austin * @since 2019/7/30 15:35 */public class ComposeSum &#123; /** * 可输出全部组合 * 两层循环 */ public static void printPairs(int[] array, int sum) &#123; for (int i = 0; i &lt; array.length; i++) &#123; int first = array[i]; for (int j = i + 1; j &lt; array.length; j++) &#123; int second = array[j]; if ((first + second) == sum) &#123; System.out.printf("(%d, %d) %n", first, second); &#125; &#125; &#125; &#125; /** * map方式 */ public static void printPairsUsingSet(int[] numbers, int n) &#123; if (numbers.length &lt; 2) &#123; return; &#125; Set set = new HashSet(numbers.length); for (int value : numbers) &#123; int target = n - value; // if target number is not in set then add if (!set.contains(target)) &#123; set.add(value); &#125; else &#123; System.out.printf("(%d, %d) %n", value, target); &#125; &#125; &#125; /** * 先排序 O(n log(n)) */ public static void printPairsUsingTwoPointers(int[] numbers, int k) &#123; if (numbers.length &lt; 2) &#123; return; &#125; Arrays.sort(numbers); int left = 0; int right = numbers.length - 1; while (left &lt; right) &#123; int sum = numbers[left] + numbers[right]; if (sum == k) &#123; System.out.printf("(%d, %d) %n", numbers[left], numbers[right]); left = left + 1; right = right - 1; &#125; else if (sum &lt; k) &#123; left = left + 1; &#125; else if (sum &gt; k) &#123; right = right - 1; &#125; &#125; &#125; public static void main(String[] args)&#123; printPairs(new int[]&#123; 2, 4, 3, 5, 6, -2, 4, 7, 8, 9 &#125;, 12); System.out.println(); printPairsUsingSet(new int[]&#123; 2, 4, 3, 5, 6, -2, 4, 7, 8, 9 &#125;, 12); System.out.println(); printPairsUsingTwoPointers(new int[]&#123; 2, 4, 3, 5, 6, -2, 4, 7, 8, 9 &#125;, 12); &#125;&#125; 3. 数组中重复的数据给定一个整数数组 a，其中1 ≤ a[i] ≤ n （n为数组长度）, 其中有些元素出现两次而其他元素出现一次。 找到所有出现两次的元素。 你可以不用到任何额外空间并在O(n)时间复杂度内解决这个问题吗？ 示例： 输入: 1[4,3,2,7,8,2,3,1] 输出: 1[2,3] 解题思路：这个题目开头暗示了n的范围，所以可以加以利用，将元素转换成数组的索引并对应的将该处的元素乘以-1； 若数组索引对应元素的位置本身就是负数，则表示已经对应过一次；在结果列表里增加该索引的正数就行； 1234567891011121314151617class Solution &#123; public List&lt;Integer&gt; findDuplicates(int[] nums) &#123; List&lt;Integer&gt; dupliacates = new ArrayList&lt;Integer&gt;(); for(int i : nums)&#123; if(i &lt; 0)&#123; i = -i; &#125; if(nums[i-1] &lt; 0)&#123; // 已经重复过 dupliacates.add(i); continue; &#125; nums[i-1] = -1 * nums[i-1]; &#125; return dupliacates; &#125;&#125; 4. 快速排序快排采用的是分治的思想。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.Arrays;/** * 快速排序 * * @author austin * @since 2019/7/30 20:05 */public class QuickSort &#123; public static void sort(int[] nums) &#123; if (null == nums || nums.length == 0) &#123; return; &#125; quickSort(nums, 0, nums.length - 1); &#125; private static void quickSort(int[] nums, int low, int high) &#123; // 选取中位数 int pivot = nums[low + (high - low) / 2]; int left = low; int right = high; while (left &lt;= right) &#123; while (nums[left] &lt; pivot) &#123; left++; &#125; while (nums[right] &gt; pivot) &#123; right--; &#125; if (left &lt;= right) &#123; int temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; left++; right--; &#125; &#125; if (low &lt; right) &#123; quickSort(nums, low, right); &#125; if (high &gt; left) &#123; quickSort(nums, left, high); &#125; &#125; public static void main(String[] args)&#123; int[] unsorted = &#123;6, 5, 3, 1, 8, 7, 2, 4&#125;; System.out.println("Unsorted array :" + Arrays.toString(unsorted)); sort(unsorted); System.out.println("Sorted array :" + Arrays.toString(unsorted)); &#125;&#125; 链表链表是另一种常见的数据结构，和数组相似，链表也是线性的数据结构并且以线性方式存储元素。而与数组不同的是，链表不是将元素存储在连续的位置中，而是可以存储在任意位置，彼此之间通过节点相互连接。 链表也可以说就是一个节点列表，每个节点中包含存储的值和下一个节点的地址。也正是因为这种结构，在链表里添加和删除元素很容易，你只需要更改链接而不用创建新的数组。但是搜索会很困难，并且在单链表中找到一个元素就需要 O（n）个时间。 链表有多种形式，如：单链表，允许你在一个方向上进行遍历；双链表，可以在两个方向上进行遍历；循环链表，最后节点的指针指向第一个节点从而形成一个环形的链；因为链表是一种递归数据结构，所以在解决链表问题时，熟练掌握递归算法就显得更加重要了。 1. 判断单链表是否存在环及求环入口点 先判断是否有环设置两个指针(fast, slow)，初始值都指向头，slow每次前进一步，fast每次前进二步，如果链表存在环，则fast必定先进入环，而slow后进入环，两个指针必定相遇。(当然，fast先行头到尾部为NULL，则为无环链表) 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public boolean hasCycle(ListNode head) &#123; ListNode slow = head; ListNode quick = head; while(null != quick)&#123; if(quick.next != null &amp;&amp; null != quick.next.next)&#123; quick = quick.next.next; &#125;else&#123; return false; &#125; slow = slow.next; if(slow == quick)&#123; return true; &#125; &#125; return false; &#125;&#125; 此问题可扩展至： 求循环链表任一节点“对面的”（最远端）的节点 算法同上，当quick到达起始节点或起始节点next时，slow指示的就是最远端的节点。 经过第1步确认存在环后，寻找环入口点： 算法描述： 当quick若与slow相遇时，slow肯定没有走遍历完链表，而quick已经在环内循环了n圈(1&lt;=n)。假设slow走了s步，则fast走了2s步（fast步数还等于s 加上在环上多转的n圈），设环长为r，则： 122s = s + nrs = nr 设整个链表长L，入口环与相遇点距离为x，起点到环入口点的距离为a。 12345a + x = s = nra + x = (n–1)r + r = (n-1)r + L - aa = (n-1)r + (L – a – x) (L – a – x) 为相遇点到环入口点的距离，由此可知，从链表头到环入口点等于(n-1)循环内环+相遇点到环入口点 于是我们从链表头、与相遇点分别设一个指针，每次各走一步，两个指针必定相遇，且相遇第一点为环入口点。 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode hasCycle(ListNode head) &#123; ListNode slow = head; ListNode quick = head.next.next; // 一定有环， 寻找相遇点 while(slow != quick)&#123; slow = slow.next; quick = quick.next.next; &#125; // quick指针重新指向head节点 quick = head; while(slow != quick)&#123; slow = slow.next; quick = quick.next; &#125; return slow; &#125;&#125; 此问题可扩展至： 判断两个单链表是否相交，如果相交，给出相交的第一个点（两个链表都不存在环）。 根据问题描述，两个单链表自相交点起，将合并为一个单链表，这是理解算法的关键。 算法描述： 将其中一个链表首尾相连，检测另外一个链表是否存在环，如果存在，则两个链表相交，而检测出来的依赖环入口即为相交的第一个点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if(null == headA || null == headB)&#123; return null; &#125; // 先将一个链表构成环 ListNode tail = headA; while(tail.next != null)&#123; tail = tail.next; &#125; tail.next = headA; ListNode slow = headB; ListNode quick = headB; while(quick != null)&#123; if(null != quick.next &amp;&amp; null != quick.next.next)&#123; quick = quick.next.next; &#125;else&#123; tail.next = null; return null; &#125; slow = slow.next; if(slow == quick)&#123; break; &#125; &#125; // 能够执行到此处一定是有环 quick = headB; while(slow != quick)&#123; slow = slow.next; quick = quick.next; &#125; tail.next = null; return slow; &#125;&#125; 2. 单链表相交找到两个单链表相交的起始节点。 注意：如果两个链表没有交点，返回 null.在返回结果后，两个链表仍须保持原有的结构。可假定整个链表结构中没有循环。程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。 解题思路：双指针法 创建两个指针 pA 和 pB，分别初始化为链表 A 和 B 的头结点。然后让它们向后逐结点遍历。 当 pA 到达链表的尾部时，将它重定位到链表 B 的头结点 (你没看错，就是链表 B); 类似的，当 pB 到达链表的尾部时，将它重定位到链表 A 的头结点。 若在某一时刻 pA 和 pB 相遇，则 pA/pB 为相交结点。 想弄清楚为什么这样可行, 可以考虑以下两个链表: A={1,3,5,7,9,11} 和 B={2,4,9,11}，相交于结点 9。由于 B.length (=4) &lt; A.length (=6)，pB 比 pA 少经过 2 个结点，会先到达尾部。将 pB 重定向到 A 的头结点，pA 重定向到 B 的头结点后，pB 要比 pA 多走 2 个结点。因此，它们会同时到达交点。如果两个链表存在相交，它们末尾的结点必然相同。因此当 pA/pB 到达链表结尾时，记录下链表 A/B 对应的元素。若最后元素不相同，则两个链表不相交。 复杂度分析 时间复杂度 : O(m+n)O(m+n)。空间复杂度 : O(1)O(1)。 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if(headA == null || headB == null) &#123; return null; &#125; ListNode pA = headA, pB = headB; // 在这里第一轮体现在pA和pB第一次到达尾部会移向另一链表的表头, // 而第二轮体现在如果pA或pB相交就返回交点, 不相交最后就是null==null while(pA != pB)&#123; pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; &#125; return pA; &#125;&#125; 3. 反转单链表两种思路：（1）迭代假设存在链表 1 → 2 → 3 → Ø，我们想要把它改成 Ø ← 1 ← 2 ← 3。 在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！ 12345678910111213141516171819202122/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; ListNode prev = null; ListNode cur = head; ListNode next = null; while(cur != null)&#123; next = cur.next; cur.next = prev; prev = cur; cur = next; &#125; return prev; &#125;&#125; 复杂度分析 时间复杂度：O(n)，假设 n 是列表的长度，时间复杂度是 O(n)。空间复杂度：O(1)。 （2）递归核心思想： 1head.next.next = head 12345678910111213141516171819/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; if(head == null || head.next == null)&#123; return head; &#125; ListNode p = reverseList(head.next); head.next.next = head; head.next = null; return p; &#125;&#125; 复杂度分析 时间复杂度：O(n)，假设 n 是列表的长度，那么时间复杂度为 O(n)。空间复杂度：O(n)，由于使用递归，将会使用隐式栈空间。递归深度可能会达到 n 层。 4. 删除链表的倒数第N个节点双指针法：第一个指针从列表的开头向前移动 n+1n+1 步，而第二个指针将从列表的开头出发。现在，这两个指针被 n 个结点分开。我们通过同时移动两个指针向前来保持这个恒定的间隔，直到第一个指针到达最后一个结点。此时第二个指针将指向从最后一个结点数起的第 n 个结点。我们重新链接第二个指针所引用的结点的 next 指针指向该结点的下下个结点。 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode dummy = new ListNode(0); // 哑指针，防止极端情况 dummy.next = head; ListNode first = dummy; ListNode second = dummy; // Advances first pointer so that the gap between first and second is n nodes apart for (int i = 1; i &lt;= n + 1; i++) &#123; first = first.next; &#125; // Move first to the end, maintaining the gap while (first != null) &#123; first = first.next; second = second.next; &#125; second.next = second.next.next; return dummy.next; &#125;&#125; 二叉树二叉树是一种非常重要的数据结构，很多其它数据结构都是基于二叉树的基础演变而来的。 对于二叉树，有深度遍历和广度遍历，深度遍历有前序、中序以及后序三种遍历方法，广度遍历即我们平常所说的层次遍历。 因为树的定义本身就是递归定义，因此采用递归的方法去实现树的三种遍历不仅容易理解而且代码很简洁，而对于广度遍历来说，需要其他数据结构的支撑，比如堆了。所以，对于一段代码来说，可读性有时候要比代码本身的效率要重要的多。 1. 二叉树深度遍历 递归方式 1234567891011121314151617181920212223242526272829303132/*** 前序遍历*/ public void preOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; System.out.print(root.val+" "); preOrderTraverse1(root.left); preOrderTraverse1(root.right); &#125;&#125;/*** 中序遍历*/ public void inOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; inOrderTraverse1(root.left); System.out.print(root.val+" "); inOrderTraverse1(root.right); &#125;&#125;/*** 后序遍历*/ public void postOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; postOrderTraverse1(root.left); postOrderTraverse1(root.right); System.out.print(root.val+" "); &#125;&#125; 非递归方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/*** 前序遍历*/public void preOrderTraverse(TreeNode root) &#123; LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; System.out.println(cur.val); stack.push(cur); cur = cur.left; &#125; else &#123; cur = stack.pop(); cur = cur.right; &#125; &#125;&#125;/*** 中序遍历*/public void midOrderTraverse(TreeNode root) &#123; LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; else &#123; cur = stack.pop(); System.out.println(cur.val); cur = cur.right; &#125; &#125;&#125;/*** 后序遍历*/public void postOrderTraverse(TreeNode root) &#123; LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; stack.push(cur); cur = cur.right; &#125; else &#123; cur = stack.pop(); System.out.println(cur.val); cur = cur.left; &#125; &#125;&#125; 2. 二叉树层遍历层次遍历的代码比较简单，只需要一个队列即可，先在队列中加入根结点。之后对于任意一个结点来说，在其出队列的时候，访问之。同时如果左孩子和右孩子有不为空的，入队列。代码如下： 1234567891011121314151617181920public void levelTraverse(TreeNode root)&#123; if(root == null)&#123; return; &#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); TreeNode cur; while (!queue.isEmpty())&#123; cur = queue.poll(); System.out.print(cur.val); if (cur.left != null)&#123; queue.offer(cur.left); &#125; if (cur.right != null)&#123; queue.offer(cur.right); &#125; &#125;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机性能监控与故障处理工具]]></title>
    <url>%2F2019%2F08%2F12%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[概述本文参考的是周志明的 《深入理解Java虚拟机》 第四章 ，为了整理思路，简单记录一下，方便后期查阅。 JDK本身提供了很多方便的JVM性能调优监控工具，除了集成式的VisualVM和jConsole外，还有jps、jstack、jmap、jhat、jstat、hprof等小巧的工具，本文希望能起抛砖引玉之用，让大家能开始对JVM性能调优的常用工具有所了解。 JDK的命令行工具 命令名称 全称 用途 jstat JVM Statistics Monitoring Tool 用于收集Hotspot虚拟机各方面的运行数据 jps JVM Process Status Tool 显示指定系统内所有的HotSpot虚拟机进程 jinfo Configuration Info for Java 显示虚拟机配置信息 jmap JVM Memory Map 生成虚拟机的内存转储快照，生成heapdump文件 jhat JVM Heap Dump Browser 用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户在浏览器上查看分析结果 jstack JVM Stack Trace 显示虚拟机的线程快照 详情参考： https://www.ymq.io/2017/08/01/jvm-4/]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>工具</tag>
        <tag>Jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程面试集锦]]></title>
    <url>%2F2019%2F08%2F01%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[什么是线程？进程？协程？进程 具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位.（比如一个qq程序就是一个进程或者多个进程），系统进行资源分配的最小单位.。进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全 线程程序执行流的最小单元（操作系统可识别的最小执行和调度单位）。也可以理解线程是一个程序里面不同的执行路径。是为了提高cpu的利用率而设计的。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。线程是轻量级的进程，它们是共享在父进程拥有的资源下，每个线程在父进程的环境中顺序的独立的执行一个活动，每个CPU核心在同一时刻只能执行一个线程，尽管我们有时感觉自己的计算机同时开着多个任务，其实他们每个的执行都是走走停停的，CPU轮流给每个进程及线程分配时间。 协程（线程的线程）协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 什么是线程安全和线程不安全？ 通俗的说：加锁的就是是线程安全的，不加锁的就是是线程不安全的 线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问，直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。 一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。 Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。 线程不安全线程不安全：就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。线程安全问题都是由全局变量及静态变量引起的。 若每个线程中对全局变量、静态变量只有读操作，而无写操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步，否则的话就可能影响线程安全。 什么是自旋锁？自旋锁 是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; lock() 方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环，如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock方法释放了该锁。 自旋锁优缺点 缺点 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） 可重入的自旋锁和不可重入的自旋锁上面那段代码，仔细分析一下就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。 而且，即使第二次能够成功获取，那么当第一次释放锁的时候，第二次获取到的锁也会被释放，而这是不合理的。 为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数。 12345678910111213141516171819202122232425public class ReentrantSpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); private int count; public void lock() &#123; Thread current = Thread.currentThread(); if (current == cas.get()) &#123; // 如果当前线程已经获取到了锁，线程数增加一，然后返回 count++; return; &#125; // 如果没获取到锁，则通过CAS自旋 while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread cur = Thread.currentThread(); if (cur == cas.get()) &#123; if (count &gt; 0) &#123;// 如果大于0，表示当前线程多次获取了该锁，释放锁通过count减一来模拟 count--; &#125; else &#123;// 如果count==0，可以将锁释放，这样就能保证获取锁的次数与释放锁的次数是一致的了。 cas.compareAndSet(cur, null); &#125; &#125; &#125;&#125; 自旋锁与互斥锁 自旋锁与互斥锁都是为了实现保护资源共享的机制。 无论是自旋锁还是互斥锁，在任意时刻，都最多只能有一个保持者。 获取互斥锁的线程，如果锁已经被占用，则该线程将进入睡眠状态；获取自旋锁的线程则不会睡眠，而是一直循环等待锁释放。 总结 自旋锁：线程获取锁的时候，如果锁被其他线程持有，则当前线程将循环等待，直到获取到锁。 自旋锁等待期间，线程的状态不会改变，线程一直是用户态并且是活动的(active)。 自旋锁如果持有锁的时间太长，则会导致其它等待获取锁的线程耗尽CPU。 自旋锁本身无法保证公平性，同时也无法保证可重入性。 基于自旋锁，可以实现具备公平性和可重入性质的锁。 参考： https://segmentfault.com/a/1190000015795906#articleHeader0 什么是Java内存模型？ VM包括两个子系统和两个组件。 两个子系统： Class loader（类装载）根据给定的全限定名类名(如：java.lang.Object)来装载class文件到Runtime data area中的method area。程序中可以extends java.lang.ClassLoader类来实现自己的Class loader。 Execution engine（执行引擎）执行classes中的指令。任何JVM specification实现(JDK)的核心都是Execution engine，不同的JDK例如Sun的JDK和IBM的JDK好坏主要就取决于他们各自实现的Execution engine的好坏。 两个组件 Native interface(本地接口)与native libraries交互，是其它编程语言交互的接口。当调用native方法的时候，就进入了一个全新的并且不再受虚拟机限制的世界，所以也很容易出现JVM无法控制的native heap OutOfMemory。 Runtime data area（运行时数据区）这就是我们常说的JVM的内存。主要分为五个部分： 方法区 有时候也成为永久代，在该区内很少发生垃圾回收，但是并不代表不发生GC，在这里进行的GC主要是对方法区里的常量池和对类型的卸载 方法区主要用来存储已被虚拟机加载的类的信息、常量、静态变量和即时编译器编译后的代码等数据，方法区也称持久代（Permanent Generation）。 该区域是被线程共享的。 方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量并不一定是编译时确定，运行时生成的常量也会存在这个常量池中。 虚拟机栈 虚拟机栈也就是我们平常所称的栈内存, 它为java方法服务，每个方法在执行的时候都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接和方法出口等信息。 虚拟机栈是线程私有的，它的生命周期与线程相同。 局部变量表里存储的是基本数据类型、returnAddress类型（指向一条字节码指令的地址）和对象引用，这个对象引用有可能是指向对象起始地址的一个指针，也有可能是代表对象的句柄或者与对象相关联的位置。局部变量所需的内存空间在编译器间确定 操作数栈的作用主要用来存储运算结果以及运算的操作数，它不同于局部变量表通过索引来访问，而是压栈和出栈的方式 每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接. 动态链接就是将常量池中的符号引用在运行期转化为直接引用 可通过参数-Xss设置栈容量 本地方法栈本地方法栈和虚拟机栈类似，只不过本地方法栈为Native方法服务 堆 Java堆是所有线程所共享的一块内存，在虚拟机启动时创建 Java堆唯一的目的是存放对象实例，几乎所有的对象实例和数组都在这里创建，因此该区域经常发生垃圾回收操作 可通过参数 -Xms 和-Xmx设置 Java堆为了便于更好的回收和分配内存，可以细分为：新生代和老年代； 新生代：包括Eden区、From Survivor区、To Survivor区，系统默认大小Eden:Survivor=8:1。 老年代：在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 程序计数器内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理和线程恢复等功能都需要依赖这个计数器完成。该内存区域是唯一一个java虚拟机规范没有规定任何OOM情况的区域 什么是CAS？介绍CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A 时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。 CAS 不通过JVM，直接利用java本地方 JNI（Java Native Interface为JAVA本地调用），直接调用CPU 的cmpxchg（汇编指令）指令。利用CPU的CAS指令，同时借助JNI来完成Java的非阻塞算法,实现原子操作。其它原子操作都是利用类似的特性完成的。 整个java.util.concurrent都是建立在CAS之上的，因此对于synchronized阻塞算法，J.U.C在性能上有了很大的提升。 CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS优点确保对内存的读-改-写操作都是原子操作执行 CAS缺点CAS虽然很高效的解决原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大和只能保证一个共享变量的原子操作 总结 使用CAS在线程冲突严重时，会大幅降低程序性能；CAS只适合于线程冲突较少的情况使用。 synchronized在jdk1.6之后，已经改进优化。synchronized的底层实现主要依靠Lock-Free的队列，基本思路是自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 什么是乐观锁和悲观锁？悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中 synchronized和 ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中 java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 使用场景 乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 悲观锁就比较合适多写的场景。多写的情况，一般会经常产生冲突，如果使用乐观锁，就会导致上层应用会不断的进行retry，这样反倒是降低了性能。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子：假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 1. 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 1. 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 2. 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 3. 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点 1 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference类就提供了此种能力，其中的 compareAndSet方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了 AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用 AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 *和 *轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 什么是AQS？简介AbstractQueuedSynchronizer简称AQS，是一个用于构建锁和同步容器的框架。事实上concurrent包内许多类都是基于AQS构建，例如ReentrantLock，Semaphore，CountDownLatch，ReentrantReadWriteLock，FutureTask等。AQS解决了在实现同步容器时设计的大量细节问题。 AQS使用一个FIFO的队列表示排队等待锁的线程，队列头节点称作“哨兵节点”或者“哑节点”，它不与任何线程关联。其他的节点与等待线程关联，每个节点维护一个等待状态waitStatus。 CAS 原子操作在concurrent包的实现参考 https://blog.52itstyle.com/archives/948/ 由于java的CAS同时具有 volatile 读和volatile写的内存语义，因此Java线程之间的通信现在有了下面四种方式： A线程写volatile变量，随后B线程读这个volatile变量。 A线程写volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 Java的CAS会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile变量的读/写和CAS可以实现线程之间的通信。把这些特性整合在一起，就形成了整个concurrent包得以实现的基石。 如果我们仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为volatile； 然后，使用CAS的原子条件更新来实现线程之间的同步； 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（Java.util.concurrent.atomic包中的类），这些concurrent包中的基础类都是使用这种模式来实现的，而concurrent包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent包的实现示意图如下： AQS实现AQS没有锁之类的概念，它有个state变量，是个int类型，在不同场合有着不同含义。 AQS围绕state提供两种基本操作“获取”和“释放”，有条双向队列存放阻塞的等待线程，并提供一系列判断和处理方法，简单说几点： state是独占的，还是共享的； state被获取后，其他线程需要等待； state被释放后，唤醒等待线程； 线程等不及时，如何退出等待。 至于线程是否可以获得state，如何释放state，就不是AQS关心的了，要由子类具体实现。 例如ReentrantLocky用它表示线程重入锁的次数，Semaphore用它表示剩余的许可数量，FutureTask用它表示任务的状态。对state变量值的更新都采用CAS操作保证更新操作的原子性。 AbstractQueuedSynchronizer继承了AbstractOwnableSynchronizer，这个类只有一个变量：exclusiveOwnerThread，表示当前占用该锁的线程，并且提供了相应的get，set方法。 什么是原子操作？在Java Concurrent API中有哪些原子类(atomic classes)？原子操作是指一个不受其他操作影响的操作任务单元。原子操作是在多线程环境下避免数据不一致必须的手段。 Atomic包一共提供了13个类，包含四种类型的原子更新方式，分别是： 基本类型 AtomicBoolean AtomicInteger AtomicLong 数组类型 AtomicIntegerArray AtomicLongArray AtomicReferenceArray 引用类型 AtomicReference AtomicReferenceFieldUpdater AtomicMarkableReference 属性类型 AtomicIntegeFeildUpdater AtomicLongFieldUpdater AtomicStampedRefernce 什么是Executors框架？Executor框架同java.util.concurrent.Executor 接口在Java 5中被引入，是一个静态工具类。Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。 无限制的创建线程会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。 利用Executors框架可以非常方便的创建一个线程池，Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 什么是阻塞队列？如何使用阻塞队列来实现生产者-消费者模型？什么是阻塞队列？阻塞队列是一个在队列基础上又支持了两个附加操作的队列。 2个附加操作： 支持阻塞的插入方法队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法队列空时，获取元素的线程会等待队列变为非空。 几个关键方法在阻塞队列不可用的时候，上述2个附加操作提供了四种处理方法 方法/处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 阻塞队列的应用场景阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。简而言之，阻塞队列是生产者用来存放元素、消费者获取元素的容器。 java里的阻塞队列JDK 7 提供了7个阻塞队列，如下 1、ArrayBlockingQueue 数组结构组成的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序，但是默认情况下不保证线程公平的访问队列，即如果队列满了，那么被阻塞在外面的线程对队列访问的顺序是不能保证线程公平（即先阻塞，先插入）的。 2、LinkedBlockingQueue一个由链表结构组成的有界阻塞队列此队列按照先出先进的原则对元素进行排序 3、PriorityBlockingQueue支持优先级的无界阻塞队列 4、DelayQueue支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素 5、SynchronousQueue不存储元素的阻塞队列，每一个put必须等待一个take操作，否则不能继续添加元素。并且他支持公平访问队列。 6、LinkedTransferQueue由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，多了tryTransfer和transfer方法 transfer方法如果当前有消费者正在等待接收元素（take或者待时间限制的poll方法），transfer可以把生产者传入的元素立刻传给消费者。如果没有消费者等待接收元素，则将元素放在队列的tail节点，并等到该元素被消费者消费了才返回。 tryTransfer方法用来试探生产者传入的元素能否直接传给消费者。如果没有消费者在等待，则返回false。和上述方法的区别是该方法无论消费者是否接收，方法立即返回。而transfer方法是必须等到消费者消费了才返回。 7、LinkedBlockingDeque链表结构的双向阻塞队列，优势在于多线程入队时，减少一半的竞争。 什么是Callable和Future? Callable 和 Future 是比较有趣的一对组合。当我们需要获取线程的执行结果时，就需要用到它们。Callable用于产生结果，Future用于获取结果。 Callable接口使用泛型去定义它的返回类型。Executors类提供了一些有用的方法去在线程池中执行Callable内的任务。由于Callable任务是并行的，必须等待它返回的结果。java.util.concurrent.Future对象解决了这个问题。 在线程池提交Callable任务后返回了一个Future对象，使用它可以知道Callable任务的状态和得到Callable返回的执行结果。Future提供了get()方法，等待Callable结束并获取它的执行结果。 什么是FutureTask?FutureTask可用于异步获取执行结果或取消执行任务的场景。通过传入Runnable或者Callable的任务给FutureTask，直接调用其run方法或者放入线程池执行，之后可以在外部通过FutureTask的get方法异步获取执行结果。 FutureTask非常适合用于耗时的计算，主线程可以在完成自己的任务后，再去获取结果。 FutureTask还可以确保即使调用了多次run方法，它都只会执行一次Runnable或者Callable任务，或者通过cancel取消FutureTask的执行等。 高并发场景示例：FutureTask在高并发环境下确保任务只执行一次 在很多高并发的环境下，往往我们只需要某些任务只执行一次。这种使用情景FutureTask的特性恰能胜任。举一个例子，假设有一个带key的连接池，当key存在时，即直接返回key对应的对象；当key不存在时，则创建连接。对于这样的应用场景，通常采用的方法为使用一个Map对象来存储key和连接池对应的对应关系，典型的代码如下面所示： 12345678910111213141516171819202122private Map&lt;String, Connection&gt; connectionPool = new HashMap&lt;String, Connection&gt;();private ReentrantLock lock = new ReentrantLock();public Connection getConnection(String key) &#123; try &#123; lock.lock(); if (connectionPool.containsKey(key)) &#123; return connectionPool.get(key); &#125; else &#123; //创建 Connection Connection conn = createConnection(); connectionPool.put(key, conn); return conn; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; //创建Connection private Connection createConnection() &#123; return null;&#125; 在上面的例子中，我们通过加锁确保高并发环境下的线程安全，也确保了connection只创建一次，然而确牺牲了性能。改用ConcurrentHash的情况下，几乎可以避免加锁的操作，性能大大提高，但是在高并发的情况下有可能出现Connection被创建多次的现象。这时最需要解决的问题就是当key不存在时，创建Connection的动作能放在connectionPool之后执行，这正是FutureTask发挥作用的时机，基于ConcurrentHashMap和FutureTask的改造代码如下： 123456789101112131415161718192021222324252627282930private ConcurrentHashMap&lt;String, FutureTask&lt;Connection&gt;&gt; connectionPool = new ConcurrentHashMap&lt;String, FutureTask&lt;Connection&gt;&gt;();public Connection getConnection(String key) throws Exception &#123; FutureTask&lt;Connection&gt; connectionTask = connectionPool.get(key); if (connectionTask != null) &#123; return connectionTask.get(); &#125; else &#123; Callable&lt;Connection&gt; callable = new Callable&lt;Connection&gt;() &#123; @Override public Connection call() throws Exception &#123; // TODO Auto-generated method stub return createConnection(); &#125; &#125;; FutureTask&lt;Connection&gt; newTask = new FutureTask&lt;Connection&gt;(callable); connectionTask = connectionPool.putIfAbsent(key, newTask); if (connectionTask == null) &#123; connectionTask = newTask; connectionTask.run(); &#125; return connectionTask.get(); &#125;&#125;//创建Connectionprivate Connection createConnection() &#123; return null;&#125; 经过这样的改造，可以避免由于并发带来的多次创建连接及锁的出现。 什么是同步容器和并发容器的实现？一、同步容器主要代表有Vector和HashTable，以及Collections.synchronizedXxx等。 锁的粒度为当前对象整体。 迭代器是及时失败的，即在迭代的过程中发现被修改，就会抛出ConcurrentModificationException。 二、并发容器主要代表有ConcurrentHashMap、CopyOnWriteArrayList、ConcurrentSkipListMap、ConcurrentSkipListSet。 锁的粒度是分散的、细粒度的，即读和写是使用不同的锁。 迭代器具有弱一致性，即可以容忍并发修改，不会抛出ConcurrentModificationException。 ConcurrentHashMap采用分离锁技术，同步容器中，是一个容器一个锁，但在ConcurrentHashMap中，会将hash表的数组部分分成若干段，每段维护一个锁，以达到高效的并发访问； 三、阻塞队列主要代表有LinkedBlockingQueue、ArrayBlockingQueue、PriorityBlockingQueue(Comparable, Comparator)、SynchronousQueue。 提供了可阻塞的put和take方法，以及支持定时的offer和poll方法。 适用于生产者、消费者模式（线程池和工作队列-Executor），同时也是同步容器 四、双端队列主要代表有ArrayDeque和LinkedBlockingDeque。意义：正如阻塞队列适用于生产者消费者模式，双端队列同样适用与另一种模式，即工作密取。在生产者-消费者设计中，所有消费者共享一个工作队列，而在工作密取中，每个消费者都有各自的双端队列。 如果一个消费者完成了自己双端队列中的全部工作，那么他就可以从其他消费者的双端队列末尾秘密的获取工作。具有更好的可伸缩性，这是因为工作者线程不会在单个共享的任务队列上发生竞争。 在大多数时候，他们都只是访问自己的双端队列，从而极大的减少了竞争。当工作者线程需要访问另一个队列时，它会从队列的尾部而不是头部获取工作，因此进一步降低了队列上的竞争。 适用于：网页爬虫等任务中 五、比较及适用场景 如果不需要阻塞队列，优先选择ConcurrentLinkedQueue； 如果需要阻塞队列，队列大小固定优先选择ArrayBlockingQueue，队列大小不固定优先选择LinkedBlockingQueue； 如果需要对队列进行排序，选择PriorityBlockingQueue； 如果需要一个快速交换的队列，选择SynchronousQueue； 如果需要对队列中的元素进行延时操作，则选择DelayQueue。 什么是多线程？优缺点？多线程：是指从软件或者硬件上实现多个线程的并发技术。 多线程的好处：使用多线程可以把程序中占据时间长的任务放到后台去处理，如图片、视屏的下载发挥多核处理器的优势，并发执行让系统运行的更快、更流畅，用户体验更好 多线程的缺点： 大量的线程降低代码的可读性； 更多的线程需要更多的内存空间 当多个线程对同一个资源出现争夺时候要注意线程安全的问题。 什么是多线程的上下文切换？即使是单核CPU也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程时同时执行的，时间片一般是几十毫秒（ms） 上下文切换过程中，CPU会停止处理当前运行的程序，并保存当前程序运行的具体位置以便之后继续运行。 CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再次加载这个任务的状态。 从任务保存到再加载的过程就是一次上下文切换。 ThreadLocal的设计理念与作用？Java中的ThreadLocal类允许我们创建只能被同一个线程读写的变量。因此，如果一段代码含有一个ThreadLocal变量的引用，即使两个线程同时执行这段代码，它们也无法访问到对方的ThreadLocal变量 InheritableThreadLocal 1public static ThreadLocal&lt;Integer&gt; threadLocal = new InheritableThreadLocal&lt;Integer&gt;(); InheritableThreadLocal类是ThreadLocal类的子类。ThreadLocal中每个线程拥有它自己的值，与ThreadLocal不同的是，InheritableThreadLocal允许一个线程以及该线程创建的所有子线程都可以访问它保存的值。 ThreadPool（线程池）用法与优势？为什么要用线程池: 减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。 可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为消耗过多的内存，而把服务器累趴下(每个线程需要大约1MB内存，线程开的越多，消耗的内存也就越大，最后死机)。 减少在创建和销毁线程上所花的时间以及系统资源的开销,如不使用线程池，有可能造成系统创建大量线程而导致消耗完系统内存 Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。 new Thread 缺点 每次new Thread新建对象性能差。 线程缺乏统一管理，可能无限制新建线程，相互之间竞争，及可能占用过多系统资源导致死机或oom。 缺乏更多功能，如定时执行、定期执行、线程中断。 Executors提供四种线程池 newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 ThreadPoolExecutor的构造函数123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 参数： corePoolSize核心线程数大小，当线程数&lt;corepoolsize ，会创建线程执行 maximumPoolSize 最大线程数， 当线程数 &gt;= corePoolSize的时候，会把runnable放入workQueue中 keepAliveTime 保持存活时间，当线程数大于corePoolSize的空闲线程能保持的最大时间。 unit 时间单位 workQueue 保存任务的阻塞队列 threadFactory 创建线程的工厂 handler 拒绝策略 任务执行顺序： 当线程数小于corePoolSize时，创建线程执行任务。 当线程数大于等于corePoolSize并且workQueue没有满时，放入workQueue中 线程数大于等于corePoolSize并且当workQueue满时，新任务新建线程运行，线程总数要小于maximumPoolSize 当线程总数等于maximumPoolSize并且workQueue满了的时候执行handler的rejectedExecution。也就是拒绝策略。 ThreadPoolExecutor默认有四个拒绝策略： ThreadPoolExecutor.AbortPolicy() 直接抛出异常RejectedExecutionException ThreadPoolExecutor.CallerRunsPolicy() 直接调用run方法并且阻塞执行 ThreadPoolExecutor.DiscardPolicy() 直接丢弃后来的任务 ThreadPoolExecutor.DiscardOldestPolicy() 丢弃在队列中队首的任务 当然可以自己继承 RejectedExecutionHandler 来写拒绝策略. Synchronized和ReentrantLock的区别？java在编写多线程程序时，为了保证线程安全，需要对数据同步，经常用到两种同步方式就是Synchronized和重入锁ReentrantLock。 基础知识 可重入锁可重入锁是指同一个线程可以多次获取同一把锁。ReentrantLock和synchronized都是可重入锁。 可中断锁可中断锁是指线程尝试获取锁的过程中，是否可以响应中断。synchronized是不可中断锁，而ReentrantLock则提供了中断功能。 公平锁与非公平锁公平锁是指多个线程同时尝试获取同一把锁时，获取锁的顺序按照线程达到的顺序，而非公平锁则允许线程“插队”。synchronized是非公平锁，而ReentrantLock的默认实现是非公平锁，但是也可以设置为公平锁。 CAS操作(CompareAndSwap)CAS操作简单的说就是比较并交换。CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存位置的值与预期原值相匹配，那么处理器会自动将该位置值更新为新值。否则，处理器不做任何操作。无论哪种情况，它都会在 CAS 指令之前返回该位置的值。CAS 有效地说明了“我认为位置 V 应该包含值 A；如果包含该值，则将 B 放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可。” Synchronizedsynchronized是java内置的关键字，它提供了一种独占的加锁方式。synchronized的获取和释放锁由JVM实现，用户不需要显示的释放锁，非常方便。然而synchronized也有一定的局限性 例如： 当线程尝试获取锁的时候，如果获取不到锁会一直阻塞； 如果获取锁的线程进入休眠或者阻塞，除非当前线程异常，否则其他线程尝试获取锁必须一直等待。 ReentrantLockReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。 用法示例： 12345678910private Lock lock = new ReentrantLock();public void test() &#123; lock.lock(); try &#123; doSomeThing(); &#125; finally &#123; lock.unlock(); &#125;&#125; lock() 如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于休眠状态，直到获取锁 tryLock() 如果获取了锁立即返回true，如果别的线程正持有锁，立即返回false； tryLock(long timeout,TimeUnit unit) 如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false； lockInterruptibly 如果获取了锁定立即返回，如果没有获取锁定，当前线程处于休眠状态，直到或者锁定，或者当前线程被别的线程中断 ReentrantLock特性 等待可中断避免，出现死锁的情况（如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false） 公平锁与非公平锁多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好 ReenTrantLock实现的原理：简单来说，ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。 总结在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁) 后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。 Synchronized： 在资源竞争不是很激烈的情况下，偶尔会有同步的情形下，synchronized是很合适的。原因在于，编译程序通常会尽可能的进行优化synchronize，另外可读性非常好。 ReentrantLock: ReentrantLock用起来会复杂一些。在基本的加锁和解锁上，两者是一样的，所以无特殊情况下，推荐使用synchronized。ReentrantLock的优势在于它更灵活、更强大，增加了轮训、超时、中断等高级功能。 ReentrantLock默认使用非公平锁是基于性能考虑，公平锁为了保证线程规规矩矩地排队，需要增加阻塞和唤醒的时间开销。如果直接插队获取非公平锁，跳过了对队列的处理，速度会更快。 Semaphore有什么作用？Semaphore就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个int型整数n，表示某段代码最多只有n个线程可以访问，如果超出了n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。 由此可以看出如果Semaphore构造函数中传入的int型整数n=1，相当于变成了一个synchronized了。 1234567891011121314151617// 阻塞// 用来获取一个许可，若无许可能够获得，则会一直等待，直到获得许可public void acquire() throws InterruptedException;// 用来释放许可。注意，在释放许可之前，必须先获获得许可public void release(); // 非阻塞//尝试获取一个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire() &#123;&#125;; //尝试获取一个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false public boolean tryAcquire(long timeout , TimeUnit unit ) throws InterruptedException &#123;&#125;; //尝试获取permits个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire(int permits ) &#123;&#125;; //尝试获取permits个许可，若在指定的时间内获取成功，则立即返回true public boolean tryAcquire(int permits , long timeout , TimeUnit unit ) throws InterruptedException &#123;&#125;; //得到当前可用的许可数目 public int availablePermits(); 示例:假若一个工厂有5台机器，但是有8个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过Semaphore来实现： 12345678910111213141516171819202122232425262728293031323334public class Test&#123; public static void main(String[] args) &#123; int N = 8 ; //工人数 Semaphore semaphore = new Semaphore(5); //机器数目 for (int i = 0; i &lt; N; i++) &#123; new Worker(i, semaphore ).start (); &#125; static class Worker extends Thread &#123; private int num; private Semaphore semaphore; public Worker(int num, Semaphore semaphore)&#123; this.num = num; this.semaphore = semaphore; &#125; @Override public void run() &#123; try &#123; semaphore.acquire(); System.out.println ("工人"+ this.num + "占用一个机器在生产..."); Thread.sleep (2000 ); System.out.println ("工人"+ this.num + "释放出机器"); semaphore.release (); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; Java Concurrency API中的Lock接口(Lock interface)是什么？对比同步它有什么优势？Lock接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。 它的优势有： 可以使锁更公平 可以使线程在等待锁的时候响应中断 可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间 可以在不同的范围，以不同的顺序获取和释放锁 ReentrantReadWriteLock读写锁的使用？Lock比传统线程模型中的synchronized方式更加面向对象，与生活中的锁类似，锁本身也应该是一个对象。两个线程执行的代码片段要实现同步互斥的效果，它们必须用同一个Lock对象。 读写锁：分为读锁和写锁，多个读锁不互斥，读锁与写锁互斥，这是由jvm自己控制的，你只要上好相应的锁即可。 如果你的代码只读数据，可以很多人同时读，但不能同时写，那就上读锁；如果你的代码修改数据，只能有一个人在写，且不能同时读取，那就上写锁。总之，读的时候上读锁，写的时候上写锁！ ReentrantReadWriteLock会使用两把锁来解决问题，一个读锁，一个写锁 线程进入读锁的前提条件： 没有其他线程的写锁 没有写请求或者有写请求，但调用线程和持有锁的线程是同一个 线程进入写锁的前提条件： 没有其他线程的读锁 没有其他线程的写锁 注意点： 读锁的重入是允许多个申请读操作的线程的，而写锁同时只允许单个线程占有，该线程的写操作可以重入。 如果一个线程占有了写锁，在不释放写锁的情况下，它还能占有读锁，即写锁降级为读锁。 对于同时占有读锁和写锁的线程，如果完全释放了写锁，那么它就完全转换成了读锁，以后的写操作无法重入，在写锁未完全释放时写操作是可以重入的。 公平模式下无论读锁还是写锁的申请都必须按照AQS锁等待队列先进先出的顺序。非公平模式下读操作插队的条件是锁等待队列head节点后的下一个节点是SHARED型节点，写锁则无条件插队。 读锁不允许newConditon获取Condition接口，而写锁的newCondition接口实现方法同ReentrantLock。 CyclicBarrier和CountDownLatch的用法及区别？ CountDownLatch CyclicBarrier 减计数方式 加计数方式 计算为0时释放所有等待的线程 计数达到指定值时释放所有等待线程 计数为0时，无法重置 计数达到指定值时，计数置为0重新开始 调用countDown()方法计数减一，调用await()方法只进行阻塞，对计数没任何影响 调用await()方法计数加1，若加1后的值不等于构造方法的值，则线程阻塞 不可重复利用 可重复利用 LockSupport工具？1、LockSupport基本介绍与基本使用LockSupport是JDK中比较底层的类，用来创建锁和其他同步工具类的基本线程阻塞。java锁和同步器框架的核心 AQS: AbstractQueuedSynchronizer，就是通过调用 LockSupport .park()和 LockSupport .unpark()实现线程的阻塞和唤醒 的。 LockSupport 很类似于二元信号量(只有1个许可证可供使用)，如果这个许可还没有被占用，当前线程获取许可并继 续 执行；如果许可已经被占用，当前线 程阻塞，等待获取许可。 全部操作： park()/park(Object)等待通行准许。 parkNanos(long)/parkNanos(Object, long)在指定运行时间（即相对时间）内，等待通行准许。 parkUntil(long)/parkUntil(Object, long)在指定到期时间（即绝对时间）内，等待通行准许。 unpark(Thread)发放通行准许或提前发放。（注：不管提前发放多少次，只用于一次性使用。） getBlocker(Thread)进入等待通行准许时，所提供的对象。 主要用途：当前线程需要唤醒另一个线程，但是只确定它会进入阻塞，但不确定它是否已经进入阻塞，因此不管是否已经进入阻塞，还是准备进入阻塞，都将发放一个通行准许。 Condition接口及其实现原理？ 在java.util.concurrent包中，有两个很特殊的工具类，Condition和ReentrantLock，使用过的人都知道，ReentrantLock（重入锁）是jdk的concurrent包提供的一种独占锁的实现 我们知道在线程的同步时可以使一个线程阻塞而等待一个信号，同时放弃锁使其他线程可以能竞争到锁在synchronized中我们可以使用Object的wait()和notify方法实现这种等待和唤醒 但是在Lock中怎么实现这种wait和notify呢？答案是Condition，学习Condition主要是为了方便以后学习blockqueue和concurrenthashmap的源码，同时也进一步理解ReentrantLock。Condition是一个多线程间协调通信的工具类，使得某个，或者某些线程一起等待某个条件（Condition）,只有当该条件具备( signal 或者 signalAll方法被带调用)时 ，这些等待线程才会被唤醒，从而重新争夺锁。 Fork/Join框架的理解?Oracle的官方给出的定义是：Fork/Join框架是一个实现了ExecutorService接口的多线程处理器。它可以把一个大的任务划分为若干个小的任务并发执行，充分利用可用的资源，进而提高应用的执行效率。 我们再通过Fork和Join这两个单词来理解下Fork/Join框架，Fork就是把一个大任务切分为若干子任务并行的执行，Join就是合并这些子任务的执行结果，最后得到这个大任务的结果。 工作窃取算法工作窃取算法是指线程从其他任务队列中窃取任务执行（可能你会很诧异，这个算法有什么用。待会你就知道了）。 考虑下面这种场景：有一个很大的计算任务，为了减少线程的竞争，会将这些大任务切分为小任务并分在不同的队列等待执行，然后为每个任务队列创建一个线程执行队列的任务。那么问题来了，有的线程可能很快就执行完了，而其他线程还有任务没执行完，执行完的线程与其空闲下来不如帮助其他线程执行任务，这样也能加快执行进程。所以，执行完的空闲线程从其他队列的尾部窃取任务执行，而被窃取任务的线程则从队列的头部取任务执行（这里使用了双端队列，既不影响被窃取任务的执行过程又能加快执行进度）。 从以上的介绍中，能够发现工作窃取算法的优点是充分利用线程提高并行执行的进度。当然缺点是在某些情况下仍然存在竞争，比如双端队列只有一个任务需要执行的时候 使用Fork/Join框架两步： 分割任务：首先需要创建一个ForkJoin任务，执行该类的fork方法可以对任务不断切割，直到分割的子任务足够小 合并任务执行结果：子任务执行的结果同一放在一个队列中，通过启动一个线程从队列中取执行结果。 Fork/Join实现了ExecutorService，所以它的任务也需要放在线程池中执行。它的不同在于它使用了工作窃取算法，空闲的线程可以从满负荷的线程中窃取任务来帮忙执行。 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.ExecutionException;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;public class CountTask extends RecursiveTask&lt;Integer&gt;&#123; //阈值 private static final int THRESHOLD = 2; //起始值 private int start; //结束值 private int end; public CountTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; boolean compute = (end - start) &lt;= THRESHOLD; int res = 0; if (compute)&#123; for (int i = start; i &lt;= end; i++)&#123; res += i; &#125; &#125;else &#123; //如果长度大于阈值，则分割为小任务 int mid = (start + end) / 2; CountTask task1 = new CountTask(start,mid); CountTask task2 = new CountTask(mid + 1, end); //计算小任务的值 task1.fork(); task2.fork(); //得到两个小任务的值 int task1Res = task1.join(); int task2Res = task2.join(); res = task1Res + task2Res; &#125; return res; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ForkJoinPool pool = new ForkJoinPool(); CountTask task = new CountTask(1,5); ForkJoinTask&lt;Integer&gt; submit = pool.submit(task); System.out.println("Final result:" + submit.get()); &#125;&#125; 代码中使用了FokJoinTask，其与一般任务的区别在于它需要实现compute方法，在方法需要判断任务是否在阈值区间内，如果不是则需要把任务切分到足够小，直到能够进行计算。 每个被切分的子任务又会重新进入compute方法，再继续判断是否需要继续切分，如果不需要则直接得到子任务执行的结果，如果需要的话则继续切分，如此循环，直到调用join方法得到最终的结果。 wait()和sleep()的区别? sleep() 方法是线程类（Thread）的静态方法，让调用线程进入睡眠状态，让出执行机会给其他线程，等到休眠时间结束后，线程进入就绪状态和其他线程一起竞争cpu的执行时间。 因为sleep() 是static静态的方法，他不能改变对象的机锁，当一个synchronized块中调用了sleep() 方法，线程虽然进入休眠，但是对象的机锁没有被释放，其他线程依然无法访问这个对象。 wait()wait()是Object类的方法，当一个线程执行到wait方法时，它就进入到一个和该对象相关的等待池，同时释放对象的机锁，使得其他线程能够访问，可以通过notify，notifyAll方法来唤醒等待的线程 线程的五个状态（五种状态，创建、就绪、运行、阻塞和死亡）? start()方法和run()方法的区别？每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，方法run()称为线程体。通过调用Thread类的start()方法来启动一个线程。 start()启动一个线程，真正实现了多线程运行。这时无需等待run方法体代码执行完毕，可以直接继续执行下面的代码；这时此线程是处于就绪状态， 并没有运行。 然后通过此Thread类调用方法run()来完成其运行状态， 这里方法run()称为线程体，它包含了要执行的这个线程的内容， Run方法运行结束， 此线程终止。然后CPU再调度其它线程。 run()方法是在本线程里的，只是线程里的一个函数,而不是多线程的。如果直接调用run(),其实就相当于是调用了一个普通函数而已，直接待用run()方法必须等待run()方法执行完毕才能执行下面的代码，所以执行路径还是只有一条，根本就没有线程的特征，所以在多线程执行时要使用start()方法而不是run()方法。 Runnable接口和Callable接口的区别？ Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已； Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，非常有用。 volatile关键字的作用？volatile关键字的作用主要有两个： （1）多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据; （2）代码底层执行是Java代码–&gt;字节码–&gt;根据字节码执行对应的C/C++代码–&gt;C/C++代码被编译成汇编语言–&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率; 从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性. Java中如何获取到线程dump文件？死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： （1）获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java （2）打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈， 线程和进程有什么区别？ 进程是系统进行资源分配的基本单位，有独立的内存地址空间 线程是CPU独立运行和独立调度的基本单位，没有单独地址空间，有独立的栈，局部变量，寄存器， 程序计数器等。 创建进程的开销大，包括创建虚拟地址空间等需要大量系统资源 创建线程开销小，基本上只有一个内核对象和一个堆栈。 一个进程无法直接访问另一个进程的资源；同一进程内的多个线程共享进程的资源。 进程切换开销大，线程切换开销小；进程间通信开销大，线程间通信开销小。 线程属于进程，不能独立执行。每个进程至少要有一个线程，成为主线程 线程实现的方式有几种（四种）？ 继承Thread类，重写run方法 实现Runnable接口，重写run方法，实现Runnable接口的实现类的实例对象作为Thread构造函数的target 实现Callable接口通过FutureTask包装器来创建Thread线程 1FutureTask&lt;Object&gt; oneTask = new FutureTask&lt;Object&gt;(oneCallable); 通过线程池创建线程 12ExecutorService executorService = Executors.newFixedThreadPool(5);executorService.execute(new RunnableTask()); 高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ （1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 （2）并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 （3）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考（2）。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。 锁的等级：方法锁、对象锁、类锁?1. 通过在方法声明中加入 synchronized关键字来声明 synchronized 方法 synchronized 方法控制对类成员变量的访问：每个类实例对应一把锁，每个 synchronized 方法都必须获得调用该方法的类实例的锁方能执行，否则所属线程阻塞，方法一旦执行，就独占该锁，直到从该方法返回时才将锁释放，此后被阻塞的线程方能获得该锁，重新进入可执行状态。 这种机制确保了同一时刻对于每一个类实例，其所有声明为 synchronized 的成员函数中至多只有一个处于可执行状态，从而有效避免了类成员变量的访问冲突。 2. 对象锁（synchronized修饰方法或代码块） 当一个对象中有synchronized method或synchronized block的时候调用此对象的同步方法或进入其同步区域时，就必须先获得对象锁。如果此对象的对象锁已被其他调用者占用，则需要等待此锁被释放。（方法锁也是对象锁） java的所有对象都含有1个互斥锁，这个锁由JVM自动获取和释放。线程进入synchronized方法的时候获取该对象的锁，当然如果已经有线程获取了这个对象的锁，那么当前线程会等待；synchronized方法正常返回或者抛异常而终止，JVM会自动释放对象锁。这里也体现了用synchronized来加锁的1个好处，方法抛异常的时候，锁仍然可以由JVM来自动释放。 3. 类锁(synchronized 修饰静态的方法或代码块) 由于一个class不论被实例化多少次，其中的静态方法和静态变量在内存中都只有一份。所以，一旦一个静态的方法被申明为synchronized。此类所有的实例化对象在调用此方法，共用同一把锁，我们称之为类锁。 对象锁是用来控制实例方法之间的同步，类锁是用来控制静态方法（或静态变量互斥体）之间的同步 如果同步块内的线程抛出异常会发生什么？无论你的同步块是正常还是异常退出的，里面的线程都由JVM来自动释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 并发编程（concurrency）并行编程（parallellism）有什么区别？并发和并行是： 解释一：并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔发生。 解释二：并行是在不同实体上的多个事件，并发是在同一实体上的多个事件。 解释三：在一台处理器上“同时”处理多个任务，在多台处理器上同时处理多个任务。如hadoop分布式集群 所以并发编程的目标是充分的利用处理器的每一个核，以达到最高的处理性能。 如何在两个线程之间共享数据? 通过在线程之间共享对象, 然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的； Exchanger 用于进行线程间数据交换； 生产者消费者模型的作用是什么?这个问题很理论，但是很重要： （1）通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用 （2）解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 怎么唤醒一个阻塞的线程? 如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它； 如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 Java中用到的线程调度算法是什么抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。 单例模式的线程安全性?首先要说的是单例模式的线程安全意味着：某个类的实例在多线程环境下只会被创建一次出来。单例模式有很多种的写法，我总结一下： （1）饿汉式单例模式的写法：线程安全 （2）懒汉式单例模式的写法：非线程安全 （3）双检锁单例模式的写法：线程安全 同步方法和同步块，哪个是更好的选择?同步块是更好的选择，因为它不会锁住整个对象（当然也可以让它锁住整个对象）。 同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。 123456789101112131415161718public class SyncObj&#123; // 同步方法会锁住整个对象 public synchronized void showA()&#123; System.out.println("showA.."); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public void showB() &#123; // 同步块 synchronized (this) &#123; System.out.println("showB.."); &#125; &#125;&#125; 如何检测死锁？怎么预防死锁？死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁。通俗地讲就是两个或多个进程被无限期地阻塞、相互等待的一种状态 死锁产生的原因？ 1.因竞争资源发生死锁 现象：系统中供多个进程共享的资源的数目不足以满足全部进程的需要时，就会引起对诸资源的竞争而发生死锁现象 2.进程推进顺序不当发生死锁 死锁的四个必要条件： 互斥条件：进程对所分配到的资源不允许其他进程进行访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源 请求和保持条件：进程获得一定的资源之后，又对其他资源发出请求，但是该资源可能被其他进程占有，此事请求阻塞，但又对自己获得的资源保持不放 不可剥夺条件：是指进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用完后自己释放 环路等待条件：是指进程发生死锁后，若干进程之间形成一种头尾相接的循环等待资源关系这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。 检测死锁有两个容器，一个用于保存线程正在请求的锁，一个用于保存线程已经持有的锁。每次加锁之前都会做如下检测: 检测当前正在请求的锁是否已经被其它线程持有,如果有，则把那些线程找出来 遍历第一步中返回的线程，检查自己持有的锁是否正被其中任何一个线程请求，如果第二步返回真,表示出现了死锁 死锁的解除与预防：理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁。 所以，在系统设计、进程调度等方面注意如何不让这四个必要条件成立，如何确定资源的合理分配算法，避免进程永久占据系统资源。 此外，也要防止进程在处于等待状态的情况下占用资源。因此，对资源的分配要给予合理的规划。 转载：想进大厂？50个多线程面试题，你会多少？（一）想进大厂？50个多线程面试题，你会多少？（二）]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池ThreadPoolExecutor源码详解]]></title>
    <url>%2F2019%2F07%2F19%2FJava%E7%BA%BF%E7%A8%8B%E6%B1%A0ThreadPoolExecutor%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文从源码层面去解读下java线程池的实现思想和代码。 总览先看一张java线程池的继承关系图： 简单介绍下： Executor 位于最顶层，也是最简单的，只有一个 execute(Runnable runnable) 接口方法定义 ExecutorService 也是接口，在 Executor 接口的基础上添加了很多的接口方法，很多时候我们使用这个接口就够了 AbstractExecutorService，这是抽象类，这里实现了非常有用的一些方法供子类直接使用，例如: invokeAll()、 invokeAny() ThreadPoolExecutor 类，这个类才是真正的线程池实现，提供了非常丰富的功能。 从图中的方法可以看到，还涉及到一些其他类： 其中： Executors类这个是工具类，里面的方法都是静态方法，如以下我们最常用的用于生成 ThreadPoolExecutor 的实例的一些方法： 123456789101112public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; Future接口由于线程池支持获取线程执行的结果，所以，引入了 Future 接口，RunnableFuture 继承自此接口，然后我们最需要关心的就是它的实现类 FutureTask。 FutureTask类在线程池的使用过程中，我们是往线程池提交任务（task），我们提交的每个任务是实现了 Runnable 接口的，其实就是先将 Runnable 的任务包装成 FutureTask，然后再提交到线程池。它首先是一个任务（Task），然后具有 Future 接口的语义，即可以在将来（Future）得到执行的结果。 BlockingQueue如果线程数达到 corePoolSize，我们的每个任务会提交到等待队列中，等待线程池中的线程来取任务并执行。这里的 BlockingQueue 通常我们使用其实现类 LinkedBlockingQueue、ArrayBlockingQueue 和 SynchronousQueue，每个实现类都有不同的特征. Executor接口1234567/* * @since 1.5 * @author Doug Lea */public interface Executor &#123; void execute(Runnable command);&#125; 可以看到 Executor 接口非常简单，就一个 void execute(Runnable command) 方法，代表提交一个任务。为了理解 java 线程池的整个设计方案，我会按照 Doug Lea 的设计思路来多说一些相关的东西。 我们经常这样启动一个线程： 123new Thread(new Runnable()&#123; // do something&#125;).start(); 用了线程池 Executor 后就可以像下面这么使用： 123Executor executor = anExecutor;executor.execute(new RunnableTask1());executor.execute(new RunnableTask2()); 如果我们希望线程池同步执行每一个任务，我们可以这么实现这个接口： 12345class DirectExecutor implements Executor &#123; public void execute(Runnable r) &#123; r.run(); // 这里不是用的new Thread(r).start()，也就是说没有启动任何一个新的线程。 &#125;&#125; 如果我们希望每个任务提交进来后，直接启动一个新的线程来执行这个任务，我们可以这么实现： 12345class ThreadPerTaskExecutor implements Executor &#123; public void execute(Runnable r) &#123; new Thread(r).start(); // 每个任务都用一个新的线程来执行 &#125;&#125; 我们再来看下怎么组合两个 Executor 来使用，下面这个实现是将所有的任务都加到一个 queue 中，然后从 queue 中取任务，交给真正的执行器执行，这里采用 synchronized 进行并发控制： 12345678910111213141516171819202122232425262728293031323334353637class SerialExecutor implements Executor &#123; // 任务队列 final Queue&lt;Runnable&gt; tasks = new ArrayDeque&lt;Runnable&gt;(); // 这个才是真正的执行器 final Executor executor; // 当前正在执行的任务 Runnable active; // 初始化的时候，指定执行器 SerialExecutor(Executor executor) &#123; this.executor = executor; &#125; // 添加任务到线程池: 将任务添加到任务队列，scheduleNext 触发执行器去任务队列取任务 public synchronized void execute(final Runnable r) &#123; tasks.offer(new Runnable() &#123; public void run() &#123; try &#123; r.run(); &#125; finally &#123; scheduleNext(); &#125; &#125; &#125;); if (active == null) &#123; scheduleNext(); &#125; &#125; protected synchronized void scheduleNext() &#123; if ((active = tasks.poll()) != null) &#123; // 具体的执行转给真正的执行器 executor executor.execute(active); &#125; &#125;&#125; Executor 这个接口只有提交任务的功能，太简单了，我们想要更丰富的功能，比如我们想知道执行结果、我们想知道当前线程池有多少个线程活着、已经完成了多少任务等等，这些都是这个接口的不足的地方。接下来我们要介绍的是继承自 Executor 接口的 ExecutorService 接口，这个接口提供了比较丰富的功能，也是我们最常使用到的接口。 ExecutorService简单初略地来看一下这个接口中都有哪些方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152简单初略地来看一下这个接口中都有哪些方法：public interface ExecutorService extends Executor &#123; // 关闭线程池，已提交的任务继续执行，不接受继续提交新任务 void shutdown(); // 关闭线程池，尝试停止正在执行的所有任务，不接受继续提交新任务 // 它和前面的方法相比，加了一个单词“now”，区别在于它会去停止当前正在进行的任务 List&lt;Runnable&gt; shutdownNow(); // 线程池是否已关闭 boolean isShutdown(); // 如果调用了 shutdown() 或 shutdownNow() 方法后，所有任务结束了，那么返回true // 这个方法必须在调用shutdown或shutdownNow方法之后调用才会返回true boolean isTerminated(); // 等待所有任务完成，并设置超时时间 // 我们这么理解，实际应用中是，先调用 shutdown 或 shutdownNow， // 然后再调这个方法等待所有的线程真正地完成，返回值意味着有没有超时 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个 Callable 任务 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); // 提交一个 Runnable 任务，第二个参数将会放到 Future 中，作为返回值， // 因为 Runnable 的 run 方法本身并不返回任何东西 &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); // 提交一个 Runnable 任务 Future&lt;?&gt; submit(Runnable task); // 执行所有任务，等全部完成后返回 Future 类型的一个 list &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; // 也是执行所有任务，但是这里设置了超时时间 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; // 只要其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; // 同上一个方法，只要其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果， // 不过这个带超时，超过指定的时间，抛出 TimeoutException 异常 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 这些方法都很好理解，一个简单的线程池主要就是这些功能，能提交任务，能获取结果，能关闭线程池，这也是为什么我们经常用这个接口的原因。 FutureTask在继续往下层介绍 ExecutorService 的实现类之前，我们先来说说相关的类 FutureTask。 ![](/images/future_task_ inherit.png) FutureTask 通过 RunnableFuture 间接实现了 Runnable 接口，所以每个 Runnable 通常都先包装成 FutureTask，然后调用 executor.execute(Runnable command) 将其提交给线程池. Runnable 的 void run() 方法是没有返回值的，所以，通常，如果我们需要的话，会在 submit 中指定第二个参数作为返回值： 1&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); 其实到时候会通过这两个参数，将其包装成 Callable。 Callable 也是因为线程池的需要，所以才有了这个接口。它和 Runnable 的区别在于 run() 没有返回值，而 Callable 的 call() 方法有返回值，同时，如果运行出现异常，call() 方法会抛出异常。 123public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 123public interface Runnable &#123; public abstract void run();&#125; 下面，我们来看看 ExecutorService 的抽象实现 AbstractExecutorService 。 AbstractExecutorServiceAbstractExecutorService 抽象类派生自 ExecutorService 接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 invokeAny方法： invokeAll方法： newTaskFor方法： 用于将任务包装成 FutureTask 定义于最上层接口 Executor中的 void execute(Runnable command) 由于不需要获取结果，不会进行 FutureTask 的包装。 需要获取结果（FutureTask），用 submit 方法，不需要获取结果，可以用 execute 方法。 下面重点讲解下newTaskFor和invokeAny、invokeAll方法源码。 newTaskFor &amp;&amp; submit1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public abstract class AbstractExecutorService implements ExecutorService &#123; /** * RunnableFuture 是用于获取执行结果的，我们常用它的子类 FutureTask * 下面两个 newTaskFor 方法用于将我们的任务包装成 FutureTask 提交到线程池中执行 */ protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; return new FutureTask&lt;T&gt;(runnable, value); &#125; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable); &#125; /** * 提交任务 */ public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) &#123; throw new NullPointerException(); &#125; // 1. 将任务包装成 FutureTask RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); // 2. 交给子类执行器执行 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) &#123; throw new NullPointerException(); &#125; // 1. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); // 2. 交给子类执行器执行 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) &#123; throw new NullPointerException(); &#125; // 1. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task); // 2. 交给子类执行器执行 execute(ftask); return ftask; &#125;&#125; invokeAny123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122/** * 此方法目的：将 tasks 集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了 * 第二个参数 timed 代表是否设置超时机制，超时时间为第三个参数， * 如果 timed 为 true，同时超时了还没有一个线程返回结果，那么抛出 TimeoutException 异常 */private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; if (tasks == null) &#123; throw new NullPointerException(); &#125; int ntasks = tasks.size(); if (ntasks == 0) &#123; throw new IllegalArgumentException(); &#125; ArrayList&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(ntasks); // ExecutorCompletionService 不是一个真正的执行器，参数 this 才是真正的执行器 // 它对执行器进行了包装，每个任务结束后，将结果保存到内部的一个 completionQueue 队列中 // 这也是为什么这个类的名字里面有个 Completion 的原因。 ExecutorCompletionService&lt;T&gt; ecs = new ExecutorCompletionService&lt;T&gt;(this); // For efficiency, especially in executors with limited // parallelism, check to see if previously submitted tasks are // done before submitting more of them. This interleaving // plus the exception mechanics account for messiness of main // loop. try &#123; // 用于保存异常信息，此方法如果没有得到任何有效的结果，那么我们可以抛出最后得到的一个异常 ExecutionException ee = null; final long deadline = timed ? System.nanoTime() + nanos : 0L; Iterator&lt;? extends Callable&lt;T&gt;&gt; it = tasks.iterator(); // 首先先提交一个任务，后面的任务到下面的 for 循环一个个提交 futures.add(ecs.submit(it.next())); --ntasks; // 提交了一个任务，所以任务数量减 1 int active = 1; // 正在执行的任务数(提交的时候 +1，任务结束的时候 -1) for (; ; ) &#123; // ecs 上面说了，其内部有一个 completionQueue 用于保存执行完成的结果 // BlockingQueue的poll方法不阻塞，返回 null 代表队列为空 Future&lt;T&gt; f = ecs.poll(); // 非阻塞 // 为 null，说明刚刚提交的第一个线程还没有执行完成 // 在前面先提交一个任务，加上这里做一次检查，也是为了提高性能 if (f == null) &#123; if (ntasks &gt; 0) &#123; // 再提交一个任务 --ntasks; futures.add(ecs.submit(it.next())); ++active; &#125; else if (active == 0) &#123; // 没有任务了，同时active为0,说明 任务都执行完成了 break; &#125; else if (timed) &#123; f = ecs.poll(nanos, TimeUnit.NANOSECONDS); // 带等待时间的poll方法 if (f == null) &#123; throw new TimeoutException(); // 如果已经超时，抛出 TimeoutException 异常，这整个方法就结束了 &#125; nanos = deadline - System.nanoTime(); &#125; else &#123; f = ecs.take(); // 没有任务了，有一个在运行中，再获取一次结果，阻塞方法，直到任务结束 &#125; &#125; /* * 我感觉上面这一段并不是很好理解，这里简单说下： * 1. 首先，这在一个 for 循环中，我们设想每一个任务都没那么快结束， * 那么，每一次都会进到第一个分支，进行提交任务，直到将所有的任务都提交了 * 2. 任务都提交完成后，如果设置了超时，那么 for 循环其实进入了“一直检测是否超时” 这件事情上 * 3. 如果没有设置超时机制，那么不必要检测超时，那就会阻塞在 ecs.take() 方法上， 等待获取第一个执行结果 * ?. 这里我还没理解 active == 0 这个分支的到底是干嘛的？ */ if (f != null) &#123; // 有任务结束了 --active; try &#123; return f.get(); // 阻塞获取执行结果，如果有异常，都包装成 ExecutionException &#125; catch (ExecutionException eex) &#123; ee = eex; &#125; catch (RuntimeException rex) &#123; ee = new ExecutionException(rex); &#125; &#125; &#125; if (ee == null) &#123; ee = new ExecutionException(); &#125; throw ee; &#125; finally &#123; // 方法退出之前，取消其他的任务 for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; futures.get(i).cancel(true); &#125; &#125;&#125;/** * 将tasks集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了，不设置超时时间 */public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; try &#123; return doInvokeAny(tasks, false, 0); &#125; catch (TimeoutException cannotHappen) &#123; assert false; return null; &#125;&#125;/** * 将tasks集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了，需要指定超时时间 */public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; return doInvokeAny(tasks, true, unit.toNanos(timeout));&#125; invokeAll123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103 /** * 将tasks集合中的任务提交到线程池执行，全部线程执行完后才可以结束了 * 其实我们自己提交任务到线程池，也是想要线程池执行所有的任务 * 只不过，我们是每次 submit 一个任务，这里以一个集合作为参数提交 */public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; if (tasks == null) &#123; throw new NullPointerException(); &#125; ArrayList&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; for (Callable&lt;T&gt; t : tasks) &#123; // 包装成 FutureTask RunnableFuture&lt;T&gt; f = newTaskFor(t); futures.add(f); // 提交任务 execute(f); &#125; for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; Future&lt;T&gt; f = futures.get(i); if (!f.isDone()) &#123; try &#123; // 这是一个阻塞方法，直到获取到值，或抛出了异常 // 这里有个小细节，其实 get 方法签名上是会抛出 InterruptedException 的 // 可是这里没有进行处理，而是抛给外层去了。此异常发生于还没执行完的任务被取消了 f.get(); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; &#125; &#125; done = true; // 这个方法返回返回 List&lt;Future&gt;，而且是任务都结束了 return futures; &#125; finally &#123; if (!done) &#123; // 异常情况下才会进入 // 方法退出之前，取消其他的任务 for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; futures.get(i).cancel(true); &#125; &#125; &#125;&#125;/** * 带超时的 invokeAll */public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; if (tasks == null) &#123; throw new NullPointerException(); &#125; long nanos = unit.toNanos(timeout); ArrayList&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; for (Callable&lt;T&gt; t : tasks) &#123; futures.add(newTaskFor(t)); &#125; final long deadline = System.nanoTime() + nanos; // 直接计算出超时时刻 final int size = futures.size(); // 提交一个任务，检测一次是否超时 for (int i = 0; i &lt; size; i++) &#123; execute((Runnable) futures.get(i)); nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; return futures; &#125; &#125; for (int i = 0; i &lt; size; i++) &#123; Future&lt;T&gt; f = futures.get(i); if (!f.isDone()) &#123; if (nanos &lt;= 0L) &#123; return futures; &#125; try &#123; // 调用带超时的 get 方法，这里的参数 nanos 是剩余的时间， // 因为上面其实已经用掉了一些时间了 f.get(nanos, TimeUnit.NANOSECONDS); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; catch (TimeoutException toe) &#123; return futures; &#125; nanos = deadline - System.nanoTime(); // 更新剩余时间 &#125; &#125; done = true; return futures; &#125; finally &#123; if (!done) &#123; for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; futures.get(i).cancel(true); &#125; &#125; &#125;&#125; 到这里，我们发现，这个抽象类包装了一些基本的方法，可是像 submit、invokeAny、invokeAll 等方法，它们都没有真正开启线程来执行任务，它们都只是在方法内部调用了 execute 方法，所以最重要的 execute(Runnable runnable) 方法还没出现，需要等具体执行器来实现这个最重要的部分，这里我们要说的就是 ThreadPoolExecutor 类了。 ThreadPoolExecutorThreadPoolExecutor 是 JDK 中的线程池实现，这个类实现了一个线程池需要的各个方法，它实现了任务提交、线程管理、监控等等方法。 构造函数Executors 这个工具类来快速构造一个线程池，对于初学者而言，这种工具类是很有用的，开发者不需要关注太多的细节，只要知道自己需要一个线程池，仅仅提供必需的参数就可以了，其他参数都采用作者提供的默认值。其调用的就是构造函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 构造方法 * * @param corePoolSize 核心线程数 * @param maximumPoolSize 最大线程数，线程池允许创建的最大线程数 * @param keepAliveTime 空闲线程的保活时间，如果某线程的空闲时间超过这个值都没有任务给它做，那么可以被关闭了。 * 注意这个值并不会对所有线程起作用，如果线程池中的线程数少于等于核心线程数 corePoolSize， * 那么这些线程不会因为空闲太长时间而被关闭，当然，也可以通过调用 allowCoreThreadTimeOut(true) * 使核心线程数内的线程也可以被回收 * @param unit 时间单位 * @param workQueue 任务队列，BlockingQueue 接口的某个实现（常使用 ArrayBlockingQueue 和 LinkedBlockingQueue） * @param threadFactory 用于生成线程，一般我们可以用默认的就可以了。 * 通常，我们可以通过它将我们的线程的名字设置得比较可读一些，如 Message-Thread-1， Message-Thread-2 类似这样。 * @param handler 当线程池已经满了，但是又有新的任务提交的时候，该采取什么策略由这个来指定。有 * 几种方式可供选择，像抛出异常、直接拒绝然后返回等，也可以自己实现相应的接口实现自己的逻辑。 */ public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) &#123; throw new IllegalArgumentException(); &#125; if (workQueue == null || threadFactory == null || handler == null) &#123; throw new NullPointerException(); &#125; this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 关键变量除了构造函数以外，还需要重点关注下几个重要的属性和函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3; // 29// 线程容量(2^29-1=536 870 911)private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; // 000 1111111111111111111111111111/* * RUNNING 定义为 -1， * SHUTDOWN 定义为 0， * 其他的都比 0 大， * 所以等于 0 的时候不能提交任务，大于 0 的话，连正在执行的任务也需要中断 */// runState存储在高3位// 接收新任务，处理队列任务private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; // 111 00000000000000000000000000000 (-536870912)// 不接受新的任务提交，但是会继续处理等待队列中的任务private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; // 000 00000000000000000000000000000 (0)// 不接收新任务，也不处理队列任务，并且中断所有处理中的任务private static final int STOP = 1 &lt;&lt; COUNT_BITS; // 001 00000000000000000000000000000 ( 268435456)// 所有任务都被终结，有效线程为0。会触发terminated()方法private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; // 010 00000000000000000000000000000 (1073741824)// 当terminated()方法执行结束时状态private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; // 011 00000000000000000000000000000 (1610612736)// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; // 取高三位，状态&#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; // 取低29位，线程数&#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc;&#125;/* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */private static boolean runStateLessThan(int c, int s) &#123; return c &lt; s;&#125;private static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s;&#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125;/** * 线程数自增 1 * Attempts to CAS-increment the workerCount field of ctl. */private boolean compareAndIncrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect + 1);&#125;/** * 线程数自减 1 * Attempts to CAS-decrement the workerCount field of ctl. */private boolean compareAndDecrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect - 1);&#125;/** * 只有当某线程被突然终止时才会调用该方法，其他线程数自减是在执行新的task时 * &lt;p&gt; */private void decrementWorkerCount() &#123; do &#123; &#125; while (!compareAndDecrementWorkerCount(ctl.get()));&#125; 看了这几种状态的介绍，读者大体也可以猜到十之八九的状态转换了，各个状态的转换过程有以下几种： 123456789RUNNING -&gt; SHUTDOWN：当调用了 shutdown() 后，会发生这个状态转换，这也是最重要的(RUNNING or SHUTDOWN) -&gt; STOP：当调用 shutdownNow() 后，会发生这个状态转换，这下要清楚 shutDown() 和 shutDownNow() 的区别了SHUTDOWN -&gt; TIDYING：当任务队列和线程池都清空后，会由 SHUTDOWN 转换为 TIDYINGSTOP -&gt; TIDYING：当任务队列清空后，发生这个转换TIDYING -&gt; TERMINATED：这个前面说了，当 terminated() 方法结束后 上面的几个记住核心的就可以了，尤其第一个和第二个。 另外，我们还要看看一个内部类 Worker，因为 Doug Lea 把线程池中的线程包装成了一个个 Worker，翻译成工人，就是线程池中做任务的线程。所以到这里，我们知道任务是 Runnable（内部叫 task 或 command），线程是 Worker。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/*** 继承了AbstractQueuedSynchronizer以简化获取和释放围绕每个任务执行的锁。* 这可以防止中断旨在唤醒等待任务的工作线程，而不是中断正在运行的任务。*/private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; private static final long serialVersionUID = 6138294804551838833L; /** * 由ThreadFactory创建的真实执行任务的线程 */ final Thread thread; /** * 前面说了，这里的 Runnable 是任务。 * 为什么叫 firstTask？因为在创建线程的时候，如果同时指定了这个线程起来以后需要执行的第一个任务， * 那么第一个任务就是存放在这里的(线程可不止执行这一个任务) * 当然了，也可以为 null，这样线程起来了，自己到任务队列（BlockingQueue）中取任务（getTask 方法） */ Runnable firstTask; /** * 用于存放此线程完全的任务数，注意了，这里用了 volatile，保证可见性 */ volatile long completedTasks; /** * Worker 只有这一个构造方法，传入 firstTask，也可以传 null */ Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 调用 ThreadFactory 来创建一个新的线程 this.thread = getThreadFactory().newThread(this); &#125; /** * 这里调用了外部类的 runWorker 方法 */ public void run() &#123; runWorker(this); &#125; ...// 其他几个方法没什么好看的，就是用 AQS 操作，来获取这个线程的执行权，用了独占锁&#125; excute()有了上面的这些基础后，我们终于可以看看 ThreadPoolExecutor 的 execute 方法了，前面源码分析的时候也说了，各种方法都最终依赖于 execute 方法. 首先分析下execute主要工作流程： （1）任务submit后先通过newTaskFor()封装成可返回结果的FutureTask; （2）调用execute方法执行； （3）execute方法在当前线程数（WC）小于coreSize时，直接创建新线程处理； （4）如果创建新线程失败，尝试加入任务队列，若此时线程池已经处于非Running状态，则不做处理； （5）成功加入任务队列后需要再次确认线程池状态（有可能在加入队列操作的过程中，线程池被shutdown了），如果此时线程池非Running,则移除该任务，执行拒绝策略；如果状态正常，则判断WC==0，如果等于0说明线程池中没有线程了，则创建一个新线程添加到pool中； （6）如果加入队列失败或者当前状态非Running, 则尝试创建新线程来处理该任务，如果失败，则执行拒绝策略； 具体流程如下图： 本来想用一张图表示整个流程，结果发现图还没有看源代码清晰，干脆放弃了，直接看代码吧。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public void execute(Runnable command) &#123; if (command == null) &#123; throw new NullPointerException(); &#125; /* * 三步走： * * 1. 当前线程数小于corePoolSize，添加一个新的worker,并把commond作为其第一个任务。 * 调用addWorker()方法会自动检查runState和workerCount，避免因为状态问题报错 * * 2. 任务成功加入队列后，仍然需要再次确认是否增加新的工作线程（有可能在上次检测运行线程数之后某些线程挂了）， * 或者在进入这个方法时，线程池shut down了。 * So we recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. 如果无法添加到队列，则尝试创建新线程。如果失败，则表示线程池shutdown了或者需要执行拒绝策略了。 * * 由此可见：在线程数超过corePoolSize后，只有队列满了才会再次创建新线程 */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加任务成功，那么就结束了。提交任务嘛，线程池已经接受了这个任务，这个方法也就可以返回了 // 至于执行的结果，到时候会包装到 FutureTask 中。 // 返回 false 代表线程池不允许提交任务 if (addWorker(command, true)) &#123; return; &#125; c = ctl.get(); &#125; // 到这里说明，要么当前线程数大于等于核心线程数，要么刚刚 addWorker 失败了 // 如果线程池处于 RUNNING 状态，把这个任务添加到任务队列 workQueue 中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; /* 如果任务进入了 workQueue，我们是否需要开启新的线程 * 因为线程数在 [0, corePoolSize) 是无条件开启新的线程 * 如果线程数已经大于等于 corePoolSize，那么将任务添加到队列中，然后进到这里 */ int recheck = ctl.get(); if (!isRunning(recheck) &amp;&amp; remove(command)) &#123; // 如果线程池已不处于RUNNING状态，那么移除已经入队的这个任务 reject(command); // 执行拒绝策略 &#125; else if (workerCountOf(recheck) == 0) &#123; // 如果线程池还是 RUNNING 的，并且线程数为 0，那么开启新的线程 // 这块代码的真正意图是：担心任务提交到队列中了，但是线程都关闭了 addWorker(null, false); &#125; &#125; else if (!addWorker(command, false)) &#123; // 线程池非Running或者队列满了，尝试创建新线程 // 创建新线程失败，说明当前线程数已经达到 maximumPoolSize，执行拒绝策略 reject(command); &#125;&#125; addWorker()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 简单分析： * 还是状态控制的问题，当线程池处于 SHUTDOWN 的时候，不允许提交任务，但是已有的任务继续执行 * 当状态大于 SHUTDOWN 时，不允许提交任务，且中断正在执行的任务 * 多说一句： * 如果线程池处于 SHUTDOWN，但是firstTask为null，且 workQueue 非空，那么是允许创建 worker 的 * * @param firstTask 准备提交给这个线程执行的第一个任务，可以为null. * @param core true 代表使用核心线程数 corePoolSize 作为创建线程的界线，也就说创建这个线程的时候， * 如果线程池中的线程总数已经达到 corePoolSize，那么不能响应这次创建线程的请求 * 如果是 false，代表使用最大线程数 maximumPoolSize 作为界线 */private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (; ; ) &#123; int c = ctl.get(); int rs = runStateOf(c); // 获取当前状态 // 如果线程池已关闭，并满足以下条件之一，那么不创建新的 worker： // 1. 线程池状态大于SHUTDOWN，其实也就是 STOP, TIDYING, 或 TERMINATED // 2. firstTask不为空 // 3. 任务队列为空 if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) &#123; return false; &#125; for (; ; ) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) &#123; // 超容量了或者超过当前限制了，不允许创建 return false; &#125; // 如果成功，那么就是所有创建线程前的条件校验都满足了，准备创建线程执行任务了 // 这里失败的话，说明有其他线程也在尝试往线程池中创建线程 if (compareAndIncrementWorkerCount(c)) &#123; break retry; // 退出循环，准备创建线程执行任务 &#125; c = ctl.get(); // 由于有并发，重新再读取一下 ctl // 正常如果是 CAS 失败的话，进到下一个里层的for循环就可以了 // 可是如果是因为其他线程的操作，导致线程池的状态发生了变更，如有其他线程关闭了这个线程池 // 那么需要回到外层的for循环 if (runStateOf(c) != rs) &#123; continue retry; &#125; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; /* * 到这里，我们认为在当前这个时刻，可以开始创建线程来执行任务了， * 因为该校验的都校验了，至于以后会发生什么，那是以后的事，至少当前是满足条件的 */ boolean workerStarted = false; // worker 是否已经启动 boolean workerAdded = false; // 是否已将这个 worker 添加到 workers 这个 HashSet 中 Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; // 这个是整个类的全局锁，持有这个锁才能让下面的操作“顺理成章”， // 因为关闭一个线程池需要这个锁，至少我持有锁的期间，线程池不会被关闭 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 小于 SHUTTDOWN 那就是 RUNNING，这个自不必说，是最正常的情况 // 如果等于 SHUTDOWN，前面说了，不接受新的任务，但是会继续执行等待队列中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // 检测线程是否已经是start状态 &#123; // 新添加的worker里面的 thread 可不能是已经启动的 throw new IllegalThreadStateException(); &#125; workers.add(w); // 加到 workers 这个 HashSet 中 int s = workers.size(); // largestPoolSize 用于记录 workers 中的个数的历史最大值 // 因为 workers 是不断增加减少的，通过这个值可以知道线程池的大小曾经达到的最大值 if (s &gt; largestPoolSize) &#123; largestPoolSize = s; &#125; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 添加成功的话，启动这个线程 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (!workerStarted) &#123; // 如果线程没有启动，需要做一些清理工作，如前面 workCount 加了 1，将其减掉 addWorkerFailed(w); &#125; &#125; // 返回线程是否启动成功 return workerStarted;&#125; addWorkFailed()12345678910111213141516171819/** * 线程创建失败回滚 * workers 中删除掉相应的 worker * workCount 减 1 * 终止检查，防止这个线程的存在阻碍了线程池的terminate */private void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (w != null) &#123; workers.remove(w); &#125; decrementWorkerCount(); tryTerminate(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; runWorker()回过头来，继续往下走。我们知道，worker 中的线程 start 后，其 run 方法会调用 runWorker 方法： 1234// Worker 类的 run() 方法public void run() &#123; runWorker(this);&#125; 继续往下看 runWorker 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 实际执行task, 循环从队列中取任务执行 * * @param w the worker */final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; // 循环调用 getTask 获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // 如果线程池状态大于等于 STOP，那么意味着该线程也要中断 // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) &#123; wt.interrupt(); &#125; try &#123; // 这是一个钩子方法，留给需要的子类实现 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 到这里终于可以执行任务了 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; // 这里不允许抛出 Throwable，所以转换为 Erro thrown = x; throw new Error(x); &#125; finally &#123; // 也是一个钩子方法，将 task 和异常作为参数，留给需要的子类实现 afterExecute(task, thrown); &#125; &#125; finally &#123; // 置空 task，准备 getTask 获取下一个任务 task = null; // 累加该worker完成的任务数 w.completedTasks++; // 释放掉 worker 的独占锁 w.unlock(); &#125; &#125; completedAbruptly = false; // 执行到这儿说明是getTask()为空，而不是报异常了 &#125; finally &#123; // 如果到这里，需要执行线程关闭： // 1. 说明 getTask 返回 null，也就是说，这个 worker 的使命结束了，执行关闭 // 2. 任务执行过程中发生了异常 // 第一种情况，已经在代码处理了将 workCount 减 1，这个在 getTask 方法分析中会说 // 第二种情况，workCount 没有进行处理，所以需要在 processWorkerExit 中处理 processWorkerExit(w, completedAbruptly); &#125;&#125; processWorkerExit()12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 线程终止后: * 1. 如果是异常退出, 则需要减掉当前workercCount * 2. 更新线程池完成任务数 * 3. 从workers中移除终止的线程； * 4. 终止检测 * 5. 如果线程池当前处于RUNNING/SHUTDOWN状态： * a) 允许回收核心线程时，至少要保证有一个worker线程； * b) 不允许回收核心线程时，当前线程小于corePoolSize，则创建新的线程； * c）如果worker线程是由于异常退出，则直接创建一个新的worker线程 * * @param w the worker * @param completedAbruptly true woker执行异常 */private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted &#123; decrementWorkerCount(); &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; completedTaskCount += w.completedTasks; workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 终止检查，防止这个线程的存在阻碍了线程池的terminate tryTerminate(); int c = ctl.get(); if (runStateLessThan(c, STOP)) &#123; // RUNNING/SHUTDOWN if (!completedAbruptly) &#123; // 说明当前任务队列中没有任务 int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; !workQueue.isEmpty()) &#123; // 允许回收核心线程 min = 1; &#125; if (workerCountOf(c) &gt;= min) &#123; // 当前线程数大于1 或者 corePoolSize, 暂时不创建新的线程 return; // replacement not needed &#125; &#125; addWorker(null, false); // 添加新的备用线程 &#125;&#125; getTask()getTask() 是怎么获取任务的，这个方法写得真的很好，每一行都很简单，组合起来却所有的情况都想好了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 此方法有三种可能： * 1. 阻塞直到获取到任务返回。我们知道，默认 corePoolSize 之内的线程是不会被回收的，它们会一直等待任务 * 2. 超时退出。keepAliveTime 起作用的时候，也就是如果这么多时间内都没有任务，那么应该执行关闭 * 3. 如果发生了以下条件，此方法必须返回 null: * - 池中有大于 maximumPoolSize 个 workers 存在(通过调用 setMaximumPoolSize 进行设置) * - 线程池处于 SHUTDOWN，而且 workQueue 是空的，前面说了，这种不再接受新的任务 * - 线程池处于 STOP，不仅不接受新的线程，连 workQueue 中的线程也不再执行 */private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (; ; ) &#123; int c = ctl.get(); int rs = runStateOf(c); // 两种可能 // 1. rs == SHUTDOWN &amp;&amp; workQueue.isEmpty() // 2. rs &gt;= STOP if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); // 减少工作线程数, processWorkerExit()方法中会将该线程移除 return null; &#125; int wc = workerCountOf(c); // 允许核心线程数内的线程回收，或当前线程数超过了核心线程数，那么有可能发生超时关闭 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 当前线程数超过maximumPoolSize // 允许回收核心线程或者当前线程超过corePoolSize &amp;&amp; 超时 // wc &gt; 1 或者 队列为空 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) &#123; // 减掉线程数 return null; // 获取任务的worker取不到任务就会退出 &#125; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : // 等待一定时间，如果仍然获取不到说明线程数过多，任务不够 workQueue.take(); // 阻塞获取 if (r != null) &#123; return r; &#125; timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果此 worker 发生了中断，采取的方案是重试 // 解释下为什么会发生中断，这个读者要去看 setMaximumPoolSize 方法， // 如果开发者将 maximumPoolSize 调小了，导致其小于当前的 workers 数量， // 那么意味着超出的部分线程要被关闭。重新进入 for 循环，自然会有部分线程会返回 null timedOut = false; &#125; &#125;&#125; tryTerminate()1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 把线程池状态设置为TERMINATED，在以下条件之一： * 1. SHUTDOWN 并且 pool线程和队列都为空； * 2. STOP 并且 pool线程为空； * &lt;p&gt; * 如果线程不为0时想要优雅终止，则中断空闲的worker线程以保证shutdown信号得到传播。 * * This method must be called following any action that might make * termination possible -- reducing worker count or removing tasks * from the queue during shutdown. The method is non-private to * allow access from ScheduledThreadPoolExecutor. */final void tryTerminate() &#123; for (; ; ) &#123; int c = ctl.get(); // 状态为RUNNING、SHUTDOWN、STOP 或者 (SHUTDOWN &amp;&amp; 队列不为空) 不允许 if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; !workQueue.isEmpty())) &#123; return; &#125; if (workerCountOf(c) != 0) &#123; // 终止一个空闲线程 interruptIdleWorkers(ONLY_ONE); return; &#125; // 当前线程数为0 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; // 设置状态为TIDYING try &#123; terminated(); // 触发终止后方法 &#125; finally &#123; ctl.set(ctlOf(TERMINATED, 0)); // 最终设置为TERMINATED状态 termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125; 拒绝策略ThreadPoolExecutor 中的拒绝策略。 12345678910111213141516171819/** * 此处的 handler 我们需要在构造线程池的时候就传入这个参数，它是 RejectedExecutionHandler 的实例。 * RejectedExecutionHandler 在 ThreadPoolExecutor 中有四个已经定义好的实现类可供我们直接使用， * 当然，我们也可以实现自己的策略，不过一般也没有必要。简要介绍下四中默认的拒绝策略： * &lt;p&gt; * 1. CallerRunsPolicy： 只要线程池没有被关闭，那么由提交任务的线程自己来执行这个任务 * &lt;p&gt; * 2. AbortPolicy：不管怎样，直接抛出 RejectedExecutionException 异常， 这个是默认的策略， * 如果我们构造线程池的时候不传相应的 handler 的话，那就会指定使用这个 * &lt;p&gt; * 3. DiscardPolicy：不做任何处理，直接忽略掉这个任务 * &lt;p&gt; * 4. DiscardOldestPolicy： 这个相对霸道一点，如果线程池没有被关闭的话， 把队列队头的任务(也就是等待了最长时间的)直接扔掉， * 然后提交这个任务到等待队列中 */ final void reject(Runnable command) &#123; // 执行拒绝策略 handler.rejectedExecution(command, this); &#125; Executors Executors它仅仅是工具类，它的所有方法都是 static 的。 FixedThreadPoole生成一个固定大小的线程池： 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 最大线程数设置为与核心线程数相等，此时 keepAliveTime 设置为 0（因为这里它是没用的，即使不为 0 也不会执行 corePoolSize 内的线程），任务队列采用 LinkedBlockingQueue，无界队列。 过程分析：刚开始，每提交一个任务都创建一个 worker，当 worker 的数量达到 nThreads 后，不再创建新的线程，而是把任务提交到 LinkedBlockingQueue 中，而且之后线程数始终为 nThreads。 SingleThreadExecutor生成只有一个线程的固定线程池，这个更简单，和上面的一样，只要设置线程数为 1 就可以了： 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; newCachedThreadPool生成一个需要的时候就创建新的线程，同时可以复用之前创建的线程（如果这个线程当前没有任务）的线程池： 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 过程分析：鉴于 corePoolSize 是 0，那么提交任务的时候，直接将任务提交到队列中，由于采用了 SynchronousQueue，所以如果是第一个任务提交的时候，offer 方法肯定会返回 false，因为此时没有任何 worker 对这个任务进行接收，那么将进入到最后一个分支来创建第一个 worker。之后再提交任务的话，取决于是否有空闲下来的线程对任务进行接收，如果有，会进入到第二个 if 语句块中，否则就是和第一个任务一样，进到最后的 else if 分支。 这种线程池对于任务可以比较快速地完成的情况有比较好的性能。如果线程空闲了 60 秒都没有任务，那么将关闭此线程并从线程池中移除。所以如果线程池空闲了很长时间也不会有问题，因为随着所有的线程都会被关闭，整个线程池不会占用任何的系统资源。 SynchronousQueue 是一个比较特殊的 BlockingQueue，其本身不储存任何元素，它有一个虚拟队列（或虚拟栈），不管读操作还是写操作，如果当前队列中存储的是与当前操作相同模式的线程，那么当前操作也进入队列中等待；如果是相反模式，则配对成功，从当前队列中取队头节点。具体的信息，可以看我的另一篇关于 BlockingQueue 的文章。 代码带注释完整代码： https://github.com/austin-brant/thread-pool-source-code]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Boot + Kafka实现生产+批量消费]]></title>
    <url>%2F2019%2F07%2F16%2FSpring-Boot-Kafka%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7-%E6%89%B9%E9%87%8F%E6%B6%88%E8%B4%B9%2F</url>
    <content type="text"><![CDATA[本文是Springboot + Kafka实现消息写入和批量消费，属于一个学习demo，下面直接上代码。 POM依赖1234567891011121314151617181920212223242526272829303132333435&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#============== kafka ===================# 指定kafka 代理地址，可以多个spring.kafka.bootstrap-servers=10.101.38.213:8092#指定template默认topic idspring.kafka.template.default-topic=topic-test#=============== provider =======================## 重试次数spring.kafka.producer.retries=3# 批量发送消息数量Bytesspring.kafka.producer.batch-size=16384# 32M批处理缓冲区spring.kafka.producer.buffer-memory=33554432spring.kafka.producer.properties.linger-ms=1# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer#=============== consumer =======================# 指定默认消费者group idspring.kafka.consumer.group-id=etl# 最早未被消费的offset, 若设置为earliest，那么会从头开始读partitionspring.kafka.consumer.auto-offset-reset=earliest# 批量一次最大拉取数据量spring.kafka.consumer.max-poll-records=5# 如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）spring.kafka.consumer.fetch-max-wait=10000# 自动提交spring.kafka.consumer.enable-auto-commit=falsespring.kafka.consumer.auto-commit-interval=10000# 连接超时时间, 自定义spring.kafka.consumer.session-timeout=15000# 指定消息key和消息体的编解码方式spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer#=============== listener =======================# 指定listener 容器中的线程数，用于提高并发量spring.kafka.listener.concurrency=1# 轮询消费者时使用的超时（以毫秒为单位）spring.kafka.listener.poll-timeout=50000# 是否开启批量消费，true表示批量消费spring.kafka.listener.batch-listener=truetopic.name=springDemo Kafka配置 如果不需要批量消费，只需KafkaTemplate进行produce， 则不需要该显式配置类，spring-boot的自动配置会根据配置文件帮我们创建好KafkaTemplate对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136package com.austin.brant.kafka.demo.config;import java.util.HashMap;import java.util.Map;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.kafka.annotation.EnableKafka;import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;import org.springframework.kafka.config.KafkaListenerContainerFactory;import org.springframework.kafka.core.DefaultKafkaConsumerFactory;import org.springframework.kafka.core.DefaultKafkaProducerFactory;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.kafka.core.ProducerFactory;import org.springframework.kafka.listener.ContainerProperties;/** * @author austin-brant * @since 2019/7/15 21:45 */@Configuration@EnableKafkapublic class KafkaConfig &#123; @Value("$&#123;spring.kafka.bootstrap-servers&#125;") private String bootstrapServers; @Value("$&#123;spring.kafka.producer.retries&#125;") private String producerRetries; // 生产者重试次数 @Value("$&#123;spring.kafka.producer.batch-size&#125;") private String producerBatchSize; @Value("$&#123;spring.kafka.producer.properties.linger-ms&#125;") private String producerLingerMs; @Value("$&#123;spring.kafka.producer.buffer-memory&#125;") private String producerBufferMemory; @Value("$&#123;spring.kafka.consumer.enable-auto-commit&#125;") private Boolean autoCommit; @Value("$&#123;spring.kafka.consumer.auto-commit-interval&#125;") private Integer autoCommitInterval; @Value("$&#123;spring.kafka.consumer.group-id&#125;") private String groupId; @Value("$&#123;spring.kafka.consumer.max-poll-records&#125;") private Integer maxPollRecords; @Value("$&#123;spring.kafka.consumer.fetch-max-wait&#125;") private Integer maxPollIntervals; @Value("$&#123;spring.kafka.consumer.auto-offset-reset&#125;") private String autoOffsetReset; @Value("$&#123;spring.kafka.listener.concurrency&#125;") private Integer concurrency; @Value("$&#123;spring.kafka.listener.poll-timeout&#125;") private Long pollTimeout; @Value("$&#123;spring.kafka.consumer.session-timeout&#125;") private String sessionTimeout; @Value("$&#123;spring.kafka.listener.batch-listener&#125;") private Boolean batchListener; /** * ProducerFactory */ @Bean public ProducerFactory&lt;String, String&gt; producerFactory() &#123; Map&lt;String, Object&gt; configs = new HashMap&lt;String, Object&gt;(); //参数 configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); configs.put(ProducerConfig.RETRIES_CONFIG, producerRetries); configs.put(ProducerConfig.BATCH_SIZE_CONFIG, producerBatchSize); configs.put(ProducerConfig.LINGER_MS_CONFIG, producerLingerMs); configs.put(ProducerConfig.BUFFER_MEMORY_CONFIG, producerBufferMemory); configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); return new DefaultKafkaProducerFactory&lt;String, String&gt;(configs); &#125; /** * KafkaTemplate */ @Bean public KafkaTemplate&lt;String, String&gt; kafkaTemplate() &#123; return new KafkaTemplate&lt;String, String&gt;(producerFactory(), true); &#125; /** * 添加KafkaListenerContainerFactory，用于批量消费消息 */ @Bean public KafkaListenerContainerFactory&lt;?&gt; batchFactory() &#123; ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(new DefaultKafkaConsumerFactory&lt;Object, Object&gt;(consumerConfigs())); factory.setBatchListener(batchListener); // 开启批量监听 factory.setConcurrency(concurrency); // 并发消费线程 factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE); factory.getContainerProperties().setPollTimeout(pollTimeout); return factory; &#125; @Bean public Map&lt;String, Object&gt; consumerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords); // 批量消费的数量 props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervals); //每一批读取间隔时间 props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset); // 最早未被消费的offset props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, autoCommit); // 是否自动提交 props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, autoCommitInterval); // 自动提交间隔 props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, sessionTimeout); // props.put(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG, 180000); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); return props; &#125;&#125; 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.austin.brant.kafka.demo.provider;import java.util.Date;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.kafka.support.SendResult;import org.springframework.stereotype.Component;import org.springframework.util.concurrent.ListenableFuture;import org.springframework.util.concurrent.ListenableFutureCallback;import com.austin.brant.kafka.demo.model.Message;import lombok.extern.slf4j.Slf4j;/** * 生产者 * * @author austin-brant * @since 2019/7/15 19:39 */@Component@Slf4jpublic class KafkaProducer &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; public void send(String topic, String message) &#123; ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; future = kafkaTemplate.send(topic, Message.builder() .id(System.currentTimeMillis()) .msg(message) .sendTime(new Date()).build().toString()); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, String&gt;&gt;() &#123; @Override public void onFailure(Throwable throwable) &#123; log.error("send message [&#123;&#125;] to topic [&#123;&#125;] failed, ", message, topic); &#125; @Override public void onSuccess(SendResult&lt;String, String&gt; stringStringSendResult) &#123; log.info("send message [&#123;&#125;] to topic [&#123;&#125;] success, ", message, topic); &#125; &#125;); log.info("send message end"); &#125; public void batchSend(String topic, List&lt;String&gt; message) &#123; message.forEach(it -&gt; kafkaTemplate.send(topic, it)); &#125;&#125; 消费者12345678910111213141516171819202122232425262728293031323334353637383940414243package com.austin.brant.kafka.demo.consumer;import java.util.List;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.kafka.support.Acknowledgment;import org.springframework.stereotype.Component;import lombok.extern.slf4j.Slf4j;/** * 消费者 * * @author austin-brant * @since 2019/7/15 19:58 */@Slf4j@Componentpublic class KafkaConsumer &#123; // @KafkaListener(topics = "$&#123;topic.name&#125;") // public void listen(ConsumerRecord&lt;String, String&gt; record) &#123; // consumer(record); // &#125; @KafkaListener(topics = &#123;"$&#123;topic.name&#125;"&#125;, containerFactory = "batchFactory", id = "consumer") public void listen(List&lt;ConsumerRecord&lt;String, String&gt;&gt; records, Acknowledgment ack) &#123; log.info("batch listen size &#123;&#125;.", records.size()); try &#123; records.forEach(it -&gt; consumer(it)); &#125; finally &#123; ack.acknowledge(); //手动提交偏移量 &#125; &#125; /** * 单条消费 */ public void consumer(ConsumerRecord&lt;String, String&gt; record) &#123; log.info("主题:&#123;&#125;, 内容: &#123;&#125;", record.topic(), record.value()); &#125;&#125; 完整代码：https://github.com/austin-brant/kafka-spring-boot-demo]]></content>
      <categories>
        <category>中间件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>入门</tag>
        <tag>Spring-Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常用设计模式]]></title>
    <url>%2F2019%2F07%2F15%2FJava%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Java常用设计模式设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结，其中最出名的当属 Gang of Four (GoF) 的分类了，他们将设计模式分类为 23 种经典的模式，根据用途我们又可以分为三大类，分别为: 创建型模式 结构型模式 行为型模式 六大原则有6大重要的设计原则在开篇和大家分享下，这些原则将贯通全文： 单一职责原则每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来。 开发-封闭原则对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。它是面向对象设计的核心所在。 依赖倒转原则抽象不应该依赖细节，细节应该依赖于抽象，说白了就是要针对接口编程，不要面向实现编程。a)高层模块不应依赖低层模块。两个都应该依赖抽象；b)抽象不应该依赖细节。细节应该依赖抽象。 里氏代换原则白话翻译：一个软件实体如果使用的是一个父类的话，那么一定适用于其子类，而且它察觉不出父类对象和子类对象的区别。也就是说：在软件里把父类都用子类替换，程序行为不会变化。 总结成一句话就是：子类型必须能够替换它们的父类型。 接口隔离原则类间的依赖关系应该建立在最小的接口上。通俗来讲：建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。也就是说，我们要为各个类建立专用的接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 迪米特法则也叫最少知识原则。如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法的话，可以通过第三者转发这个调用。其根本思想就是强调类之间的松耦合。 创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式和名字一样简单，非常简单，直接上代码吧： 123456789101112131415public class FoodFactory &#123; public static Food makeFood(String name) &#123; if (name.equals("noodle")) &#123; Food noodle = new LanZhouNoodle(); noodle.addSpicy("more"); return noodle; &#125; else if (name.equals("chicken")) &#123; Food chicken = new HuangMenChicken(); chicken.addCondiment("potato"); return chicken; &#125; else &#123; return null; &#125; &#125;&#125; 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 1234567891011121314151617181920212223242526272829public interface FoodFactory &#123; Food makeFood(String name);&#125;public class ChineseFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new ChineseFoodA(); &#125; else if (name.equals("B")) &#123; return new ChineseFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;public class AmericanFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new AmericanFoodA(); &#125; else if (name.equals("B")) &#123; return new AmericanFoodB(); &#125; else &#123; return null; &#125; &#125;&#125; 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 12345678public class APP &#123; public static void main(String[] args) &#123; // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood("A"); &#125;&#125; 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样读者看着比较清晰： 抽象工厂模式当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： 12345678910// 得到 Intel 的 CPUCPUFactory cpuFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard); 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果 Intel 家产的 CPU 和 AMD 产的主板不能兼容使用，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 12345678910111213public static void main(String[] args) &#123; // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);&#125; 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式单例模式用得最多，错得最多。 饿汉模式最简单： 1234567891011121314151617public class Singleton &#123; // 首先，将 new Singleton() 堵死 private Singleton() &#123;&#125;; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 private static Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125; // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) &#123; return new Date(); &#125;&#125; 很多人都能说出饿汉模式的缺点，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 1234567891011121314151617181920public class Singleton &#123; // 首先，也是先堵死 new Singleton() 这条路 private Singleton() &#123;&#125; // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() &#123; if (instance == null) &#123; // 加锁 synchronized (Singleton.class) &#123; // 这一次判断也是必须的，不然会有并发问题 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 双重检查，指的是两次检查 instance 是否为 null。volatile 在这里是需要的，希望能引起读者的关注。很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 1234567891011public class Singleton3 &#123; private Singleton3() &#123;&#125; // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder &#123; private static Singleton3 instance = new Singleton3(); &#125; public static Singleton3 getInstance() &#123; return Holder.instance; &#125;&#125; 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： 12Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build(); 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class User &#123; // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) &#123; this.name = name; this.password = password; this.nickName = nickName; this.age = age; &#125; // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() &#123; return new UserBuilder(); &#125; public static class UserBuilder &#123; // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() &#123; &#125; // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) &#123; this.name = name; return this; &#125; public UserBuilder password(String password) &#123; this.password = password; return this; &#125; public UserBuilder nickName(String nickName) &#123; this.nickName = nickName; return this; &#125; public UserBuilder age(int age) &#123; this.age = age; return this; &#125; // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() &#123; if (name == null || password == null) &#123; throw new RuntimeException("用户名和密码必填"); &#125; if (age &lt;= 0 || age &gt;= 150) &#123; throw new RuntimeException("年龄不合法"); &#125; // 还可以做赋予”默认值“的功能 if (nickName == null) &#123; nickName = name; &#125; return new User(name, password, nickName, age); &#125; &#125;&#125; 代码核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 123456789public class APP &#123; public static void main(String[] args) &#123; User d = User.builder() .name("foo") .password("pAss12345") .age(25) .build(); &#125;&#125; 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 1234567@Builderclass User &#123; private String name; private String password; private String nickName; private int age;&#125; 怎么样，省下来的时间是不是又可以干点别的了。当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： 1User user = new User().setName("").setPassword("").setAge(20); 原型模式这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 1protected native Object clone() throws CloneNotSupportedException; java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 总结 创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式。 单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源； 建造者模式专门对付属性很多的那种类，为了让代码更优美； 原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken("1kg"); f.setSpicy("1g"); f.setSalt("3g"); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle("500g"); f.setSalt("5g"); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println("我们马上要开始制作鸡肉了"); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println("鸡肉制作完成啦，加点胡椒粉"); // 增强 food.addCondiment("pepper"); return food; &#125; public Food makeNoodle() &#123; System.out.println("准备制作拉面~"); Food food = foodService.makeNoodle(); System.out.println("制作完成啦") return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。 因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 12345678910111213141516171819public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println("咕咕叫"); &#125; public void fly() &#123; System.out.println("鸡也会飞哦"); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 1Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合；类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。总体来说，对象适配用得比较多。 适配器模式和代理模式的异同比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样: 代理模式做的是增强原方法的活； 适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 1234567891011121314151617181920public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 12345678910111213141516171819202122232425262728// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 12345678910111213141516171819public class BlackTea extends Beverage &#123; public String getDescription() &#123; return "红茶"; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return "绿茶"; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829303132333435public class Lemon extends Condiment &#123; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 private Beverage bevarage; public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + ", 加柠檬"; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + ", 加芒果"; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + " 价格：￥" + beverage.cost()); //"绿茶, 加柠檬, 加芒果 价格：￥16"&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如： BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能 LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了 DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 123LineNumberInputStream is = new LineNumberInputStream( new BufferedInputStream( new FileInputStream(""))); InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Circle::draw()"); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Rectangle::draw()"); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return ("Employee :[ Name : " + name + ", dept : " + dept + ", salary :" + salary+" ]"); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren()这些方法。这说的其实就是组合模式。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。 Flyweight 是轻量级的意思，享元分开来说就是 共享元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。 代理模式是做方法增强的 适配器模式是把鸡包装成鸭这种用来适配接口的 桥梁模式做到了很好的解耦 装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景 门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可 组合模式用于描述具有层次结构的数据 享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能 行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。首先，先定义一个策略接口： 123public interface Strategy &#123; public void draw(int radius, int x, int y);&#125; 然后我们定义具体的几个策略： 1234567891011121314151617181920public class RedPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125; 使用策略的类： 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeDraw(int radius, int x, int y)&#123; return strategy.draw(radius, x, y); &#125;&#125; 客户端演示： 1234public static void main(String[] args) &#123; Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);&#125; 放到一张图上，让大家看得清晰些： 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 1234567891011121314151617181920212223242526public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; // 通知观察者们 public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 定义观察者接口： 1234public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125; 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 123456789101112131415161718192021222324252627282930public class BinaryObserver extends Observer &#123; // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) &#123; this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); &#125; // 该方法由主题类在数据变更的时候进行调用 @Override public void update() &#123; String result = Integer.toBinaryString(subject.getState()); System.out.println("订阅的数据发生变化，新的数据处理为二进制值为：" + result); &#125;&#125;public class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println("订阅的数据发生变化，新的数据处理为十六进制值为：" + result); &#125;&#125; 客户端使用也非常简单： 12345678910public static void main(String[] args) &#123; // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);&#125; output: 12订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 1234567891011121314public abstract class RuleHandler &#123; // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) &#123; this.successor = successor; &#125; public RuleHandler getSuccessor() &#123; return successor; &#125;&#125; 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 1234567891011121314public class NewUserRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; if (context.isNewUser()) &#123; // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("该活动仅限新用户参与"); &#125; &#125;&#125; 校验用户所在地区是否可以参与： 123456789101112public class LocationRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("非常抱歉，您所在的地区无法参与本次活动"); &#125; &#125;&#125; 校验奖品是否已领完： 123456789101112public class LimitRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(userInfo); &#125; &#125; else &#123; throw new RuntimeException("您来得太晚了，奖品被领完了"); &#125; &#125;&#125; 客户端： 123456789public static void main(String[] args) &#123; RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);&#125; 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。通常会有一个抽象类： 123456789101112131415public abstract class AbstractTemplate &#123; // 这就是模板方法 public void templateMethod()&#123; init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 &#125; protected void init() &#123; System.out.println("init 抽象层已经实现，子类也可以选择覆写"); &#125; // 留给子类实现 protected abstract void apply(); protected void end() &#123; &#125;&#125; 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。 也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 12345678public class ConcreteTemplate extends AbstractTemplate &#123; public void apply() &#123; System.out.println("子类实现抽象方法 apply"); &#125; public void end() &#123; System.out.println("我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了"); &#125;&#125; 客户端调用演示： 12345public static void main(String[] args) &#123; AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();&#125; 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。定义状态接口： 123public interface State &#123; public void doAction(Context context);&#125; 定义减库存的状态： 12345678910111213public class DeductState implements State &#123; public void doAction(Context context) &#123; System.out.println("商品卖出，准备减库存"); context.setState(this); //... 执行减库存的具体操作 &#125; public String toString()&#123; return "Deduct State"; &#125;&#125; 定义补库存状态： 1234567891011public class RevertState implements State &#123; public void doAction(Context context) &#123; System.out.println("给此商品补库存"); context.setState(this); //... 执行加库存的具体操作 &#125; public String toString() &#123; return "Revert State"; &#125;&#125; 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 1234567891011121314public class Context &#123; private State state; private String name; public Context(String name) &#123; this.name = name; &#125; public void setState(State state) &#123; this.state = state; &#125; public void getState() &#123; return this.state; &#125;&#125; 我们来看下客户端调用，大家就一清二楚了： 123456789101112131415public static void main(String[] args) &#123; // 我们需要操作的是 iPhone X Context context = new Context("iPhone X"); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();&#125; 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，这里不进行介绍了。 转载&amp;参考 【转载】 https://juejin.im/post/5bc96afff265da0aa94a4493【参考】 https://www.tutorialspoint.com/design_pattern/index.htm]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper概念介绍]]></title>
    <url>%2F2019%2F07%2F12%2FZookeeper%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Zookeeper简介概念Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 名字由来Zookeeper名字的由来是比较有趣的，下面的片段摘抄自《从PAXOS到ZOOKEEPER分布式一致性原理与实践》一书： Zookeeper最早起源于雅虎的研究院的一个研究小组。在立项初期，考虑到很多项目都是用动物的名字来命名的(例如著名的Pig项目)，雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家Raghu Ramakrishnan开玩笑说：再这样下去，我们这儿就变成动物园了。此话一出，大家纷纷表示就叫动物园管理员吧——因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好用来进行分布式环境的协调——于是，Zookeeper的名字由此诞生了。 Curator无疑是Zookeeper客户端中的瑞士军刀，它译作”馆长”或者’’管理者’’，不知道是不是开发小组有意而为之，笔者猜测有可能这样命名的原因是说明Curator就是Zookeeper的馆长(脑洞有点大：Curator就是动物园的园长)。 应用场景 ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 ZooKeeper的一些概念会话（Session）Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。 通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。 Znode在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。 一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 版本Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat中记录了这个 ZNode 的三个数据版本，分别是： version（当前ZNode数据内容的版本号） cversion（当前ZNode子节点的版本号 aversion（当前ZNode的ACL变更版本号） 特别说明： ZK 中版本就是修改次数：即使修改前后，内容不变，但版本仍会+1： version=0 表示节点创建之后，修改的次数为 0。 cversion 子节点列表：ZNode，其中 cversion 只会感知子节点列表变更信息，新增子节点、删除子节点，而不会感知子节点数据内容的变更。 目标：解决 ZNode 的并发更新问题，实现 CAS（Compare And Switch）乐观锁。 WatcherWatcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 ACLZookeeper采用ACL（Access-Control-Lists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。 CREATE: 能创建子节点 READ：能获取节点数据和列出其子节点 WRITE: 能设置节点数据 DELETE: 能删除子节点 ADMIN: 能设置ACL权限 其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制. 重要概念总结 ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提交数据节点监听服务。 Zookeeper特性 节点类型： 临时节点：客户端和服务端之间的Session过期之后节点会自动消失。 持久节点：创建节点之后，节点就会一直存在，除非手动删除。 临时顺序节点：拥有临时节点的特性，同时会根据创建的顺序给节点添加一个编号（编号作为节点名字的一部分）。 持久顺序节点：拥有持久节点的特性，同时会根据创建的顺序给节点添加一个编号（编号作为节点名字的一部分）。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 Watcher机制：节点数据变更注册时，在该节点的Watcher都会被通知。子节点列表变化注册该节点的Watcher也会被通知。 多个客户端同时创建一个节点，保证只有一个客户端可以创建成功。 对于有N台服务器组成的集群，保证有小于等于（N/2）-1 台服务器不能提供服务时，集群的数据仍然保持完整。 ZooKeeper 设计目标简单的数据模型ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。 可构建集群为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。 ZooKeeper 官方提供的架构图： 上图中每一个Server代表一个安装Zookeeper服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。 顺序访问对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id） ZooKeeper 集群角色介绍最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。 除了 Leader 外，Follower 和 Observer 都只能提供读服务。 Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Boot + Zookeeper(Curator)实现分布式锁]]></title>
    <url>%2F2019%2F07%2F12%2FSpring-Boot-Zookeeper-Curator-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[Curator简介Apache Curator是Netflix公司开源的一个Zookeeper客户端，目前已经是Apache的顶级项目，与Zookeeper提供的原生客户端相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量，通过封装的一套高级API，里面提供了更多丰富的操作，例如session超时重连、主从选举、分布式计数器、分布式锁等等适用于各种复杂场景的zookeeper操作。本文介绍如果通过Zookeeper实现一个分布式锁。 代码结构 核心依赖123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 配置文件1234567891011server.port=8012#重试次数curator.retryCount=5#重试间隔时间curator.elapsedTimeMs=5000# zookeeper 地址curator.connectString=10.101.38.213:8181# session超时时间curator.sessionTimeoutMs=60000# 连接超时时间curator.connectionTimeoutMs=5000 初始化ZK-Client1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.austin.brant.zk.demo.config;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.CuratorFrameworkFactory;import org.apache.curator.retry.RetryNTimes;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * curator配置 * * @author austin-brant * @since 2019/7/12 17:03 */@Configurationpublic class CuratorConfiguration &#123; @Value("$&#123;curator.retryCount&#125;") private int retryCount; @Value("$&#123;curator.elapsedTimeMs&#125;") private int elapsedTimeMs; @Value("$&#123;curator.connectString&#125;") private String connectString; @Value("$&#123;curator.sessionTimeoutMs&#125;") private int sessionTimeoutMs; @Value("$&#123;curator.connectionTimeoutMs&#125;") private int connectionTimeoutMs; @Bean(name = "curatorFramework", initMethod = "start") public CuratorFramework curatorFramework() &#123; return CuratorFrameworkFactory.newClient( connectString, sessionTimeoutMs, connectionTimeoutMs, new RetryNTimes(retryCount, elapsedTimeMs) ); &#125;&#125; 分布式锁实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132package com.austin.brant.zk.demo.utils;import java.util.concurrent.CountDownLatch;import javax.annotation.Resource;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.recipes.cache.PathChildrenCache;import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.ZooDefs;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Service;import lombok.extern.slf4j.Slf4j;/** * 基于zk的分布锁实现 * * @author austin-brant * @since 2019/7/12 17:17 */@Slf4j@Servicepublic class DistributedLockByZk implements InitializingBean &#123; private final static String ROOT_PATH_LOCK = "rootlock"; private CountDownLatch countDownLatch = new CountDownLatch(1); @Resource(name = "curatorFramework") private CuratorFramework curatorFramework; /** * 获取分布式锁 */ public void acquireDistributedLock(String path) &#123; String keyPath = "/" + ROOT_PATH_LOCK + "/" + path; while (true) &#123; try &#123; curatorFramework .create() .creatingParentsIfNeeded() .withMode(CreateMode.EPHEMERAL) // 临时节点 .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(keyPath); log.info("success to acquire lock for path:&#123;&#125;", keyPath); break; &#125; catch (Exception e) &#123; log.info("failed to acquire lock for path:&#123;&#125;", keyPath); log.info("while try again ......."); if (countDownLatch.getCount() &lt;= 0) &#123; countDownLatch = new CountDownLatch(1); &#125; try &#123; // 阻塞等待锁释放，重新获取 countDownLatch.wait(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; &#125; /** * 释放分布式锁 */ public boolean releaseDistributedLock(String path) &#123; String keyPath = "/" + ROOT_PATH_LOCK + "/" + path; try &#123; if (curatorFramework.checkExists().forPath(keyPath) != null) &#123; curatorFramework.delete().forPath(keyPath); &#125; &#125; catch (Exception e) &#123; log.error("failed to release lock"); return false; &#125; return true; &#125; /** * 创建 watcher 事件 */ private void addWatcher(String path) throws Exception &#123; String keyPath; if (path.equals(ROOT_PATH_LOCK)) &#123; keyPath = "/" + path; &#125; else &#123; keyPath = "/" + ROOT_PATH_LOCK + "/" + path; &#125; final PathChildrenCache cache = new PathChildrenCache(curatorFramework, keyPath, false); cache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT); cache.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_REMOVED)) &#123; String oldPath = event.getData().getPath(); log.info("success to release lock for path:&#123;&#125;", oldPath); if (oldPath.contains(path)) &#123; //释放计数器，让当前的请求获取锁 countDownLatch.countDown(); &#125; &#125; &#125; &#125;); &#125; /** * 初始化创建永久父节点 */ @Override public void afterPropertiesSet() &#123; curatorFramework = curatorFramework.usingNamespace("lock-namespace"); String path = "/" + ROOT_PATH_LOCK; try &#123; if (curatorFramework.checkExists().forPath(path) == null) &#123; curatorFramework .create() .creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(path); &#125; addWatcher(ROOT_PATH_LOCK); log.info("root path 的 watcher 事件创建成功"); &#125; catch (Exception e) &#123; log.error("connect zookeeper fail，please check the log &gt;&gt; &#123;&#125;", e.getMessage(), e); &#125; &#125;&#125; 完整代码https://github.com/austin-brant/zookeeper-spring-boot Curator使用参考Curator包含了几个包： curator-framework：对zookeeper的底层api的一些封装 curator-client：提供一些客户端的操作，例如重试策略等 curator-recipes：封装了一些高级特性，如：Cache事件监听、选举、分布式锁、分布式计数器、分布式Barrier等 Maven依赖(使用curator的版本：2.12.0，对应Zookeeper的版本为：3.4.x，如果跨版本会有兼容性问题，很有可能导致节点操作失败)： Curator使用详解参考：Zookeeper客户端Curator使用详解]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Springboot</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github Page + Hexo + Next 搭建个人博客]]></title>
    <url>%2F2019%2F07%2F11%2FGithub-Page-Hexo-Next-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[什么是 Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 准备环境准备 node 和 git 环境， 安装NodeJs首先，安装 NodeJS，因为 Hexo 是基于 Node.js 驱动的一款博客框架，相比起前面提到过的 Jekyll 框架更快更简洁。 安装git然后，安装 git，一个分布式版本控制系统，用于项目的版本控制管理，作者是 Linux 之父。如果 Git 还不熟悉可以参考廖雪峰大神的 Git 教程。 在命令行中输入相应命令验证是否成功，如果成功会有相应的版本号。 123git versionnode -vnpm -v 安装 Hexo安装如果以上环境准备好了就可以使用 npm 开始安装 Hexo 了。也可查看 Hexo 的详细文档。在命令行输入执行以下命令： 1npm install -g hexo-cli 预览安装 Hexo 完成后，再执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 123hexo init myBlogcd myBlognpm install 新建完成后，指定文件夹的目录如下： 12345678.├── _config.yml # 网站的配置信息，您可以在此配置大部分的参数。 ├── package.json├── scaffolds # 模版文件夹├── source # 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹| ├── _drafts # 草稿文件| └── _posts # 文章Markdowm文件 └── themes # 主题文件夹 好了，如果上面的命令都没报错的话，就恭喜了，运行 1hexo s 命令，其中 s 是 server 的缩写，在浏览器中输入 http://localhost:4000 回车就可以预览效果了。 配置GitHub Page首先如果你还没有 Github 账号的先 注册 一个。然后创建git hub同名仓库。 PS: Github 仅能使用一个同名仓库的代码托管一个静态站点 然后打开仓库创建一个 index.html 文件，并随意先写点内容，比如 Hello World. 这个时候打开 http://你的用户名.github.io 就可以看到你的站点啦，是不是很简单！index.html 内容只是暂时的预览效果，后面把 Hexo 的文件部署上去就可以在 http://你的用户名.github.io 看到你自己的博客啦！ 部署到 Github此时，本地和Github的工作做得差不了，是时候把它们两个连接起来了。你也可以查看官网的部署教程。先不着急，部署之前还需要修改配置和安装部署插件。第一：打开项目根目录下的 _config.yml 配置文件配置参数。拉到文件末尾，填上如下配置（也可同时部署到多个仓库，后面再说）： 第二：要安装一个部署插件 hexo-deployer-git。 1npm install hexo-deployer-git --save 最后执行以下命令就可以部署上传啦，以下 g 是 generate 缩写，d 是 deploy 缩写： 1hexo g -d 稍等一会，在浏览器访问网址： https://你的用户名.github.io 就会看到你的博客啦！！ Hexo写作头部规则相关设置文章中的头部会需要根据规则编写标题、更新时间，标签分类等类容。对读者是不可见的，语法如下： 1234567---title: Git添加账号date: 2017-06-08 19:49:26tags: [git]categories: gitcomments: true--- 以下是预先定义的参数，您可在模板中使用这些参数值并加以利用。 参数 描述 默认值 说明 layout 布局 title 标题 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 只有文章支持分类和标签.在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games 常用Hexo指令官方文档： https://hexo.io/zh-cn/docs/commands init1hexo init [folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 new1hexo new [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 1hexo new &quot;post title with whitespace&quot; generate1$ hexo generate 生成静态文件。 123选项 描述-d, --deploy 文件生成后立即部署网站-w, --watch 监视文件变动 该命令可以简写为 1$ hexo g deploy1$ hexo deploy 部署网站。 12参数 描述-g, --generate 部署之前预先生成静态文件 该命令可以简写为： 1$ hexo d clean1$ hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。 在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 server1$ hexo server 启动服务器。默认情况下，访问网址为： http://localhost:4000/。 1234选项 描述-p, --port 重设端口-s, --static 只使用静态文件-l, --log 启动日记记录，使用覆盖记录格式 该命令可以简写： 1$ hexo s 多终端编辑如果我想要在公司写博客怎么办，或者说如果我换电脑了怎么办，因为在github中的我们github.io项目是只有编译后的文件的，没有源文件的，也就是说，如果我们的电脑坏了，打不开了，我们的博客就不能进行更新了，所以需要把源文件也上传到github（或者Coding）上，然后对我们的源文件进行版本管理，这样我们就可以在另一台电脑上pull我们的源码，然后编译完之后push上去。 配置提交git将博客编辑路径hexo配置及soure文件等提交到git hub仓库；为了筛选出配置文件、主题目录、博文等重要信息，作为需要GItHub管理的文件, public内文件是根据source文件夹内容自动生成，不需要备份，不然每次改动内容太多, 即使是私有仓库，除去在线服务商员工可以看到的风险外，还有云服务商被攻击造成泄漏等可能，所以不建议将配置文件传上去。.gitignore文件： 12345678.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/_config.yml 注意这里有个坑！！！如果你用的是第三方的主题theme，是使用git clone下来的话，要把主题文件夹下面把.git文件夹删除掉，不然主题无法push到远程仓库，导致你发布的博客是一片空白。 另一台电脑操作 首先需要搭建环境（Node，Git） 安装Hexo 1npm install -g hexo-cli 拉上一步提交的代码到本地进入文件路径执行 1hexo g 然后根据提示进行操作即可。 发布报错错误如下 12345678Connection to github.com closed by remote host.fatal: The remote end hung up unexpectedlyerror: failed to push some refs to &apos;git@github.com:xxxxx/xxxxx.github.io.git&apos;FATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: Spawn failed at ChildProcess.&lt;anonymous&gt; (/home/pi/blog/node_modules/_hexo-util@0.6.3@hexo-util/lib/spawn.js:52:19) at ChildProcess.emit (events.js:182:13) at Process.ChildProcess._handle.onexit (internal/child_process.js:240:12) 这个错误是因为本地的博客版本与远程的版本不一致，解决方法: 删除博客目录下的.deploy_git文件夹，然后克隆远程(也就是将要发布的地址)的仓库到博客目录里面，然后改名字为.deploy_git 另外一个不那么绕的办法是把远端仓库删除，删除本地的.deploy_git，再次发布，不过这样做会导致之前的提交记录丢失。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Spring-Boot]过滤器vs拦截器]]></title>
    <url>%2F2019%2F07%2F10%2FSpring-Boot-%E8%BF%87%E6%BB%A4%E5%99%A8vs%E6%8B%A6%E6%88%AA%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言在spring-boot中，经常会用到滤器和拦截器，但是什么场景适合用过滤器，什么场景适合用拦截器，而且有什么异同点？下面来详细分析一下。 过滤器过滤器Filter，是在Servlet规范中定义的，是Servlet容器支持的，该接口定义在javax.servlet包下，主要是在客户端请求(HttpServletRequest)进行预处理，以及对服务器响应(HttpServletResponse)进行后处理。接口代码如下: 1234567891011package javax.servlet;import java.io.IOException;public interface Filter &#123; void init(FilterConfig var1) throws ServletException; void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException; void destroy();&#125; 对上面三个接口方法进行分析: init(FilterConfig)初始化接口，在用户自定义的Filter初始化时被调用，它与Servlet的 init方法的作用是一样的。 doFilter(ServletRequest,ServletResponse,FilterChain)在每个用户的请求进来时这个方法都会被调用，并在Servlet的service方法之前调用(如果我们是开发Servlet项目)，而FilterChain就代表当前的整个请求链，通过调用 FilterChain.doFilter可以将请求继续传递下去，如果想拦截这个请求，可以不调用FilterChain.doFilter，那么这个请求就直接返回了，所以Filter是一种责任链设计模式，在spring security就大量使用了过滤器，有一条过滤器链。 destroy当Filter对象被销毁时，这个方法被调用，注意，当Web容器调用这个方法之后，容器会再调用一次doFilter方法。 自定义Filter过滤器在springboot自定义Filter类如下: 123456789101112131415161718192021@Componentpublic class MyFilter implements Filter &#123; private Logger logger = LoggerFactory.getLogger(MyFilter.class); @Override public void init(FilterConfig filterConfig) throws ServletException &#123; logger.info("filter init"); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; logger.info("doFilter"); //对request,response进行预处理 //TODO 进行业务逻辑 filterChain.doFilter(servletRequest, servletResponse); &#125; @Override public void destroy() &#123; logger.info("filter destroy"); &#125;&#125; FilterRegistrationBean方式在springboot中提供了FilterRegistrationBean方式，此类提供setOrder方法，可以为多个filter设置排序值。代码如下: 1234567891011121314151617181920212223242526272829303132333435@Configurationpublic class FilterConfig &#123; /** * 配置一个Filter注册器 * * @return */ @Bean public FilterRegistrationBean filterRegistrationBean1() &#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(filter1()); registrationBean.setName("filter1"); //设置顺序 registrationBean.setOrder(10); return registrationBean; &#125; @Bean public FilterRegistrationBean filterRegistrationBean2() &#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(filter2()); registrationBean.setName("filter2"); //设置顺序 registrationBean.setOrder(3); return registrationBean; &#125; @Bean public Filter filter1() &#123; return new MyFilter(); &#125; @Bean public Filter filter2() &#123; return new MyFilter2(); &#125;&#125; 拦截器拦截器是Spring提出的概念，它的作用于过滤器类似，可以拦截用户请求并进行相应的处理，它可以进行更加精细的控制。在SpringMVC中，DispatcherServlet捕获每个请求，在到达对应的Controller之前，请求可以被拦截器处理，在拦截器中进行前置处理后，请求最终才到达Controller。 拦截器的接口是org.springframework.web.servlet.HandlerInterceptor接口，接口代码如下: 1234567891011public interface HandlerInterceptor &#123; default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return true; &#125; default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123; &#125; default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123; &#125;&#125; 复制代码接口方法解读: preHandle方法对客户端发过来的请求进行前置处理，如果方法返回true,继续执行后续操作，如果返回false，执行中断请求处理，请求不会发送到Controller postHandler方法在请求进行处理后执行，也就是在Controller方法调用之后处理，当然前提是之前的 preHandle方法返回 true。具体来说，postHandler方法会在DispatcherServlet进行视图返回渲染前被调用，也就是说我们可以在这个方法中对 Controller 处理之后的ModelAndView对象进行操作 afterCompletion方法该方法在整个请求结束之后执行，当然前提依然是 preHandle方法的返回值为 true才行。该方法一般用于资源清理工作. 自定义拦截器123456789101112131415161718public class MyInterceptor implements HandlerInterceptor &#123; private Logger logger = LoggerFactory.getLogger(MyInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; logger.info("preHandle...."); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; logger.info("postHandle..."); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; logger.info("afterCompletion..."); &#125;&#125; 注册拦截器同时配置拦截器规则12345678910111213@Configurationpublic class WebMvcConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(handlerInterceptor()) //配置拦截规则 .addPathPatterns("/**"); &#125; @Bean public HandlerInterceptor handlerInterceptor() &#123; return new MyInterceptor(); &#125;&#125; 多个拦截器协同工作在springMVC中我们可以实现多个拦截器，并依次将他们注册进去，如下： 123456public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(handlerInterceptor()) .addPathPatterns("/**"); registry.addInterceptor(handlerInterceptor2()) .addPathPatterns("/**");&#125; 拦截器的顺序也跟他们注册时的顺序有关，至少 preHandle方法是这样，下图表示了两个拦截器协同工作时的执行顺序： 后台打印日志显示了执行顺序： 1234567io-9999-exec-2] c.p.filter.interceptor.MyInterceptor : preHandle....2018-09-13 12:13:31.292 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor2 : preHandle2....2018-09-13 12:13:31.388 INFO 9736 --- [nio-9999-exec-2] c.p.filter.controller.HelloController : username:pjmike,password:1234562018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor2 : postHandle2...2018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor : postHandle...2018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor2 : afterCompletion2...2018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor : afterCompletion... 拦截器与过滤器之间的区别从上面对拦截器与过滤器的描述来看，它俩是非常相似的，都能对客户端发来的请求进行处理，它们的区别如下： 作用域不同 过滤器依赖于servlet容器，只能在 servlet容器，web环境下使用 拦截器依赖于spring容器，可以在spring容器中调用，不管此时Spring处于什么环境 细粒度的不同 过滤器的控制比较粗，只能在请求进来时进行处理，对请求和响应进行包装 拦截器提供更精细的控制，可以在controller对请求处理之前或之后被调用，也可以在渲染视图呈现给用户之后调用 中断链执行的难易程度不同 拦截器可以 preHandle方法内返回 false 进行中断 过滤器就比较复杂，需要处理请求和响应对象来引发中断，需要额外的动作，比如将用户重定向到错误页面 小结简单总结一下，拦截器相比过滤器有更细粒度的控制，依赖于Spring容器，可以在请求之前或之后启动，过滤器主要依赖于servlet，过滤器能做的，拦截器基本上都能做。 [转载] springboot系列文章之过滤器 vs 拦截器]]></content>
      <tags>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx Https 配置]]></title>
    <url>%2F2019%2F07%2F10%2FNginx-Https-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Https配置简介要保证Web浏览器到服务器的安全连接，HTTPS几乎是唯一选择。HTTPS其实就是HTTP over SSL，也就是让HTTP连接建立在SSL安全连接之上。 SSL使用证书来创建安全连接。有两种验证模式： 1 仅客户端验证服务器的证书，客户端自己不提供证书； 2 客户端和服务器都互相验证对方的证书。 显然第二种方式安全性更高，一般用网上银行会这么搞，但是，Dayu服务只需要采用第一种方式就可以。 SSL证书服务器自己的证书必须经过某“权威”证书的签名，而这个“权威”证书又可能经过更权威的证书签名，这么一级一级追溯上去，最顶层那个最权威的证书就称为根证书。根证书直接内置在浏览器中，这样，浏览器就可以利用自己自带的根证书去验证某个服务器的证书是否有效。 SSL证书级别分为三种类型，域名型SSL证书（DV SSL）、企业型SSL证书（OVSSL）、增强型SSL证书（EVSSL） 1. 域名型 SSL 证书（DV SSL - Domain Validation SSL）即证书颁布机构只对域名的所有者进行在线检查，通常是验证域名下某个指定文件的内容，或者验证与域名相关的某条 TXT 记录； 比如访问 [http|https]://www.mimvp.com/.../test.txt，文件内容： 2016082xxxxxmimvpcom2016 或添加一条 TXT 记录：www.mimvp.com –&gt; TXT –&gt; 20170xxxxxmimvpcom2066 2. 企业型 SSL 证书（OV SSL - Organization Validation SSL）是要购买者提交组织机构资料和单位授权信等在官方注册的凭证， 证书颁发机构在签发 SSL 证书前，不仅仅要检验域名所有权， 还必须对这些资料的真实合法性进行多方查验，只有通过验证的才能颁发 SSL 证书。 3. 增强型 SSL 证书（EV SSL - Extended Validation SSL）与其他 SSL 证书一样，都是基于 SSL/TLS 安全协议， 但是验证流程更加具体详细，验证步骤更多， 这样一来证书所绑定的网站就更加的可靠、可信。 它跟普通 SSL 证书的区别也是明显的，安全浏览器的地址栏变绿， 如果是不受信的 SSL 证书则拒绝显示，如果是钓鱼网站，地址栏则会变成红色，以警示用户。 证书获取在此就不详细介绍了 自签名证书生成下面简单介绍如何创建一个自签名的SSL证书。执行脚本create_cert.sh即可。 12345678910111213141516171819202122232425#!/bin/bash## 生成秘钥key## 会有两次要求输入密码,输入同一个即可,## 然后你就获得了一个server.key文件.openssl genrsa -des3 -out server.key 2048## 以后使用此文件(通过openssl提供的命令或API)可能经常回要求输入密码## 如果想去除输入密码的步骤可以使用以下命令:openssl rsa -in server.key -out server.key## 创建服务器证书的申请文件server.csr## 其中Country Name填CN,Common Name填主机名也可以不填,如果不填浏览器会认为不安全.## (例如你以后的url为https://abcd/xxxx....这里就可以填abcd),其他的都可以不填.openssl req -new -key server.key -out server.csr## 创建CA证书## 此时,你可以得到一个ca.crt的证书,这个证书用来给自己的证书签名.openssl req -new -x509 -key server.key -out ca.crt -days 3650## 创建自当前日期起有效期为期十年的服务器证书server.crtopenssl x509 -req -days 3650 -in server.csr -CA ca.crt -CAkey server.key -CAcreateserial -out server.crt 其中第三条指令在第二步第二条时会出来一个填写资料的界面（我已经填好参考，有些地方可以空着） 1234567Country Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:BEIJINGLocality Name (eg, city) []:haidianOrganization Name (eg, company) [Internet Widgits Pty Ltd]:BaiduOrganizational Unit Name (eg, section) []:Common Name (e.g. server FQDN or YOUR name) []:localhostEmail Address []: 这里有点要注意， Common Name (e.g. server FQDN or YOUR name) []: 这一项，是最后可以访问的域名，我这里为了方便测试，写成 localhost ，如果是为了给网站生成证书，需要写成 xxxx.com 。 脚本执行完成后，ls你的文件夹,可以看到一共生成了5个文件:ca.crt ca.srl server.crt server.csr server.key其中,server.crt和server.key就是你的nginx需要的证书文件. 配置nginx(1) 查看nginx是否安装了ssl模块 nginx -V 又红框中标记内容，则表示已经安装了ssl模块，否则需要手动安装或是更换nginx版本。 ###（2）配置nginx 将之前生成的密钥及证书文件server.crt和server.key拷贝到nginx配置文件nginx.conf同级目录，然后修改nginx.conf. 1234567891011121314151617181920212223242526272829303132333435363738394041424344# HTTPS serverserver &#123; listen 443 ssl; server_name localhost; # 域名 ssl_certificate server.crt; ssl_certificate_key server.key; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root /home/bigdata/dayu/www/; index index.html; proxy_set_header Host $http_host; rewrite (.*) /index.html break; &#125; location /public/ &#123; root /home/bigdata/dayu/www; &#125; location /metamap/ &#123; proxy_pass http://127.0.0.1:8083/; proxy_redirect /metamap/ http://127.0.0.1:8083/; proxy_set_header Host $http_host; &#125; location /bflow/ &#123; proxy_pass http://127.0.0.1:8084/; proxy_redirect /workflow/ http://127.0.0.1:8084/; proxy_set_header Host $http_host; &#125; location /authenticate/ &#123; proxy_pass http://cp01-mxnet-test.epc.baidu.com:8010/; proxy_redirect /authenticate/ http://127.0.0.1:8082/; proxy_set_header Host $http_host; &#125; &#125;]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Https</tag>
      </tags>
  </entry>
</search>
