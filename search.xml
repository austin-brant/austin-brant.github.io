<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Shell常用脚本整理]]></title>
    <url>%2F2020%2F03%2F20%2FShell%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[读取文件每一行123456789101112131415161718#!/bin/bash# 方法1while read linedo echo $linedone &lt; filename(待读取的文件)# 方法2cat filename(待读取的文件) | while read linedo echo $linedone# 方法3for line in `cat filename(待读取的文件)`do echo $linedone 变量自增写循环时，常常要用到变量的自增，现在总结一下整型变量自增的方法： 123456789i=`expr $i + 1`let i+=1;((i++));i=$[$i+1];i=$(( $i + 1 )) 字符串去空格（1）去行首空格 sed &apos;s/^[ \t]*//g&apos;（2）去行尾空格 sed &apos;s/[ \t]*$//g&apos;（3）去所有空格 sed &apos;s/[[:space:]]//g&apos;shell参数扩展 如果parameter为null或者未设置，整个参数替换表达式值为word 1$&#123;parameter:-word&#125; 如果parameter为null或者未设置，整个参数替换表达式值为word，并且parameter参数值设置为word 12345 $&#123;parameter:=word&#125; ``` + 如果parameter为null或者未设置，则打印出错误信息。否则，整个参数替换表达式值为$parameter ${parameter:?word} 12+ 如果parameter不为null或者未设置，则整个参数替换表达式值为word ${parameter:+word} 12 + 获得字符串的长度 ${#parameter} 12 + 从尾开始扫描word，将匹配word正则表达式的字符过滤掉, %为最短匹配，%%为最长匹配 ${parameter%word} ${parameter%%word} 12+ 从头开始扫描word，将匹配word正则表达的字符过滤掉, #为最短匹配，##为最长匹配 ${parameter#word} ${parameter##word} 12+ 截取字符串,截取parameter的值的子字符串 ${parameter:offset}从offset到结束 ${parameter:offset:length}从offset开始截取length个 12+ 字符串替换，将parameter对应值的pattern字符串替换成为string字符串, /表示只替换一次，//表示全部替换 ${parameter/pattern/string} ${parameter//pattern/string} 123## Supervise进程监控supervise 监控进程，如果挂了，重启进程 -p, 状态路径，supervise运行时会在该路径下保存一些文件-f, 启动新的进程命令-F, 配置文件路径 the config file path, if not given, supervise will use the default value “supervise.conf”-r, while the service exits, supervise will excute the program RESTART_SH before restart the service, and the exiting times be passed to RESTART_SH as the only argument. -t, after TIME_LIMIT seconds, the counter for the exiting times will be reset. -v, print the version of supervise and exit. -h, print the help message and exit. 12示例： supervise -p ../supervise/status/hdfs-dumper -f “sh ./control.sh start” -F ../supervise/conf/supervise.conf 12## 远程操作 #!/bin/bash LIST_FILE=hdfsdumper.num.list size=cat ./$LIST_FILE | wc -l line_index=1while [ $line_index -le $size ]do machine=cat ./$LIST_FILE | sed -n -e &quot;${line_index}p&quot; | awk -F&#39; &#39; &#39;{print $1}&#39; number=cat ./$LIST_FILE | sed -n -e &quot;${line_index}p&quot; | awk -F&#39; &#39; &#39;{print $2}&#39; nohup expect -c “ spawn ssh root@$machine; expect { &quot;Are you sure you want to continue connecting (yes/no)? &quot; {send &quot;yes\r&quot;;exp_continue} &quot;password: &quot; {send &quot;password\n &quot;; exp_continue} &quot;]# &quot; {send &quot;su work; exit\n&quot;; exp_continue} &quot;bash-4.1$&quot; {send &quot;cd /home/work/xxx &amp;&amp; sh control.sh start; exit\n&quot;; exp_continue} &quot;]$ &quot; {send &quot;cd /home/work/xxx &amp;&amp; sh control.sh start; exit\n&quot;; exp_continue} }” &amp; line_index=expr $line_index + 1done 12## 远程同步文件 #!/bin/bash LIST_FILE=hdfsdumper.num.list size=cat ./$LIST_FILE | wc -l file_name=./dumper/jdk.tar.gz line_index=1while [ $line_index -le $size ]do machine=cat ./$LIST_FILE | sed -n -e &quot;${line_index}p&quot; | awk -F&#39; &#39; &#39;{print $1}&#39; number=cat ./$LIST_FILE | sed -n -e &quot;${line_index}p&quot; | awk -F&#39; &#39; &#39;{print $2}&#39; expect -c “ spawn rsync -avz $file_name root@$machine:/home/work/opencrawler expect { &quot;Are you sure you want to continue connecting (yes/no)? &quot; {send &quot;yes\r&quot;;exp_continue} &quot;*password: &quot; {send &quot;password\n &quot;; interact}; }”; line_index=expr $line_index + 1done 12## switch case cmd=$1case $cmd instart) echo “in start command.” if [ -f “running_tag.txt” ];then echo “running_tag file is exist, already running!” exit 1 fi nohup java -Xms1000m -Xmx1000m -Xmn500m -XX:+UseParallelGC ${MAIN_CLASSNAME} &gt;&gt;log/crontabservice_terminal.log 2&gt;&amp;1 &amp; ret=$? if [ $ret -eq 0 ];then echo “start [OK].” else echo “start [FAILED].” fi ;;stop) echo “in stop command.” ret=ps aux | grep &quot;${MAIN_CLASSNAME}&quot; | grep -v &quot;grep&quot; | awk &#39;{print $2}&#39; if [ ! $ret ];then echo “MODULE:${MAIN_CLASSNAME} not started.” exit 1 fi echo “pid:$ret” kill -9 $ret if [ $? -eq 0 ];then rm -f running_tag.txt fi rm -f running_tag.txt echo “Stop [OK].” ;;*) help exit 1 ;;esac]]></content>
      <categories>
        <category>工具</category>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>脚本</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python常用脚本整理]]></title>
    <url>%2F2020%2F03%2F20%2FPython%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Log日志1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/usr/bin/env python# -*- coding: utf-8 -*-"""标准日志配置"""import osimport loggingimport logging.handlersdef init_log(log_path, level=logging.INFO, when="D", backup=7, format="%(levelname)s: %(asctime)s: %(filename)s:%(lineno)d %(message)s", datefmt="%m-%d %H:%M:%S"): """ init_log - initialize log module Args: log_path - Log file path prefix. Log data will go to two files: log_path.log and log_path.log.wf Any non-exist parent directories will be created automatically level - msg above the level will be displayed DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL the default value is logging.INFO when - how to split the log file by time interval 'S' : Seconds 'M' : Minutes 'H' : Hours 'D' : Days 'W' : Week day default value: 'D' format - format of the log default format: %(levelname)s: %(asctime)s: %(filename)s:%(lineno)d * %(thread)d %(message)s INFO: 12-09 18:02:42: log.py:40 * 139814749787872 HELLO WORLD backup - how many backup file to keep default value: 7 Raises: OSError: fail to create log directories IOError: fail to open log file """ formatter = logging.Formatter(format, datefmt) logger = logging.getLogger() logger.setLevel(level) dir = os.path.dirname(log_path) if not os.path.isdir(dir): os.makedirs(dir) handler = logging.handlers.TimedRotatingFileHandler(log_path + ".log", when=when, backupCount=backup) handler.setLevel(level) handler.setFormatter(formatter) logger.addHandler(handler) handler = logging.handlers.TimedRotatingFileHandler(log_path + ".log.wf", when=when, backupCount=backup) handler.setLevel(logging.WARNING) handler.setFormatter(formatter) logger.addHandler(handler) 日志初始化样例： 123456789import logimport os"""初始化日志配置"""cur_time = os.popen("date +%Y%m%d").read().strip()log_path = "./log/sample" + cur_timelog.init_log(log_path) Http请求12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!/usr/bin/env python# -*- coding:utf-8 -*-import urllib2import sysreload(sys)sys.setdefaultencoding('utf8')import __init__import loggingTIME_OUT = 60def getHeader(request, headers): """ 组装header """ for k in headers: request.add_header(k, headers[k.strip()]) return requestdef get(url, headers=&#123;&#125;): """ headers : &#123; 'User-Agent' : user_agent &#125; 字典 """ try: request = urllib2.Request(url) request = getHeader(request, headers) response = urllib2.urlopen(request, timeout=TIME_OUT) result = response.read().encode('utf-8') return result except Exception, e: logging.error(str(e)) return None def post(url, headers, body): """ headers : &#123; 'User-Agent' : user_agent &#125; 字典 body : String 通过header中的Content-Type来表示解析类型 """ try: request = urllib2.Request(url) request = getHeader(request, headers) request.get_method = lambda: 'POST' if body and body != "&#123;&#125;": request.add_data(body) response = urllib2.urlopen(request, timeout=TIME_OUT) result = response.read().encode('utf-8') return result except Exception as e: logging.error(str(e)) return Nonedef delete(url, headers): """ headers : &#123; 'User-Agent' : user_agent &#125; 字典 """ try: request = urllib2.Request(url) request = getHeader(request, headers) request.get_method = lambda: 'DELETE' response = urllib2.urlopen(request, timeout=TIME_OUT) result = response.read().encode('utf-8') return result except Exception, e: logging.error(str(e)) return None Mysql连接123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/env python# -*- coding:utf-8 -*-import MySQLdbdef exe_sql(sql, type=0): """ 返回结果为元组格式 """ try: conn = MySQLdb.connect(host, user, passwd, db, port) cur = conn.cursor() cur.execute(sql) results = cur.fetchall() conn.commit() cur.close() conn.close() return results except MySQLdb.Error, e: print "Mysql Error %d: %s" % (e.args[0], e.args[1]) def exe_sql_return_dict(sql, type=0): """ 返回结果为字典格式 """ try: conn = MySQLdb.connect(host, user, passwd, db, port) cur = conn.cursor(MySQLdb.cursors.DictCursor) cur.execute(sql) results = cur.fetchall() conn.commit() cur.close() conn.close() return results except MySQLdb.Error, e: print "Mysql Error %d: %s" % (e.args[0], e.args[1]) Main函数123456789101112#!/usr/bin/env python# -*- coding: utf-8 -*-import sys import jsonreload(sys)sys.setdefaultencoding('utf8')if __name__ == '__main__': for line in sys.stdin: print line.strip() Write文件12345678910111213141516171819#!/usr/bin/env python# -*- coding:utf-8 -*-import jsondef write_to_file(filepath, content, mode="a+"): """ 写入文件 """ with open(filepath, mode) as fw: # fw.write(content + "\n") fw.write(content) def write_pretty(filepath, content): """ 写带缩进json内容到文件 """ with open(filepath, 'at') as fw: res = json.dump(content, fw, sort_keys=True, indent=4, separators=(',', ': ')) Read配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/usr/bin/env python# -*- coding:utf-8 -*-"""读取配置文件"""import ConfigParserimport stringimport loggingimport logclass Parser(object): """ 读取配置文件 """ def __init__(self, configfile): self.cf = ConfigParser.ConfigParser() self.cf.read(configfile) def get_all_sections(self): """ 获取所有的section """ secs = self.cf.sections() return secs def list_options(self, section): """ 获取某个section中所有的key """ opts = self.cf.options(section) return opts def get_key(self, section, key): """ 根据section和key获取value """ try: value = self.cf.get(section, key) if value: return value return None except Exception as e: logging.error(key + " in " + section + " does't exist!") return None if __name__ == '__main__': conf = Parser("host.conf") for section in conf.get_all_sections(): print section keys = conf.list_options(section) for key in keys: print key + ' : ' + conf.get_key(section, key) host.conf内容如下： 123456789[CHINA]provice=Beijingcity=Beijingdistrict=Haidian[USA]provice=State of New Yorkcity=NewYorkdistrict=B13 脚本输出内容如下： 12345678CHINAprovice : Beijingcity : Beijingdistrict : HaidianUSAprovice : State of New Yorkcity : NewYorkdistrict : B13 Http服务SimpleHttpServer一键启动一个http服务，可以快速满足文件下载等需求。 1python -m SimpleHTTPServer [端口号] Flaskflask是python提供的一个便捷http服务框架，简单高效。 12345678910111213141516171819202122232425#!/usr/bin/env python# -*- coding:utf-8 -*-import flaskfrom flask import Flaskfrom flask import make_response,render_template,Responseimport timeimport hashlibapp = Flask(__name__)@app.route('/')def index(): stamp = str(int(time.time())) print stamp token = hashlib.md5(("bigdata-meta" + stamp + "z2spmoowoeao2bzvqs796a8t08tszso0").encode('utf8')).hexdigest() result = "timestamp: %s &lt;br/&gt;&lt;br/&gt; token: %s" % (stamp, token) return result @app.route('/callback')def read(): print "callback" return 0; if __name__ == "__main__": app.run(host='0.0.0.0', debug=True, port=8343) 单元测试12345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env python# -*- coding: utf-8 -*-"""对util模块进行测试Authors: austinDate: 16/7/12 17:24."""import sys import osimport unittestimport utilclass UtilTestCase(unittest.TestCase): """ 测试类 """ def setUp(self): """ 初始化 """ self.cs_list = ['3182242396,2726385067,72','4030018123,3852097821,72'] self.expect_url_list = ['http://t10.baidu.com/it/u=3182242396,2726385067&amp;fm=72', 'http://t10.baidu.com/it/u=4030018123,3852097821&amp;fm=72'] def test_get_cs_url(self): """ 测试生成url函数正确性 """ ret = map(util.get_cs_url, self.cs_list) self.assertEqual(ret, self.expect_url_list)if __name__ == '__main__': unittest.main()]]></content>
      <categories>
        <category>工具</category>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常见面试题总结]]></title>
    <url>%2F2020%2F03%2F06%2FJava%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[JAVA基础Java的collection接口继承关系 jdk1.7 用的是哪种垃圾回收机制 1.8用的是啥jdk1.6 用的是 UseParallelGC， ParallelGCThreads=4jdk1.8 12345678910$ java -XX:+PrintCommandLineFlags -version-XX:InitialHeapSize=134217728 -XX:MaxHeapSize=2147483648 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGCjava version "1.8.0_181"Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) UseParallelGC 即 Parallel Scavenge + Serial Old, 再查看详细信息 12345678910111213java -XX:+PrintGCDetails -versionjava version "1.8.0_181"Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)Heap PSYoungGen total 38400K, used 1331K [0x0000000795580000, 0x0000000798000000, 0x00000007c0000000) eden space 33280K, 4% used [0x0000000795580000,0x00000007956cce48,0x0000000797600000) from space 5120K, 0% used [0x0000000797b00000,0x0000000797b00000,0x0000000798000000) to space 5120K, 0% used [0x0000000797600000,0x0000000797600000,0x0000000797b00000) ParOldGen total 87552K, used 0K [0x0000000740000000, 0x0000000745580000, 0x0000000795580000) object space 87552K, 0% used [0x0000000740000000,0x0000000740000000,0x0000000745580000) Metaspace used 2233K, capacity 4480K, committed 4480K, reserved 1056768K class space used 243K, capacity 384K, committed 384K, reserved 1048576K List和Array的区别，添加和删除元素的时间复杂度怎样1) 因为Array是基于索引(index)的数据结构，它使用索引在数组中搜索和读取数据是很快的。Array获取数据的时间复杂度是O(1),但是要删除数据却是开销很大的，因为这需要重排数组中的所有数据。 2) 相对于ArrayList，LinkedList插入是更快的。因为LinkedList不像ArrayList一样，不需要改变数组的大小，也不需要在数组装满的时候要将所有的数据重新装入一个新的数组，这是ArrayList最坏的一种情况，时间复杂度是O(n)，而LinkedList中插入或删除的时间复杂度仅为O(1)。ArrayList在插入数据时还需要更新索引（除了插入数组的尾部）。 3) 类似于插入数据，删除数据时，LinkedList也优于ArrayList。 4) LinkedList需要更多的内存，因为ArrayList的每个索引的位置是实际的数据，而LinkedList中的每个节点中存储的是实际的数据和前后节点的位置。 5） Array和List都属于顺序表。Array、ArrayList是一段连续的存储结构 红黑树添加元素和获取元素的时间复杂度插入一个元素到红黑树的时间为 O(log(N))*，其中 *N 为当前红黑树的元素个数，因此，采用插入方式构建元素个数为N的红黑树的时间复杂度为: log(1) + log(2) + log(N-1) = log((N-1)!) = Nlog(N) 那么采用迭代器遍历一棵红黑树的时间复杂度是多少呢？ 是 O(N) 。 也就是说非递归遍历一棵红黑树的时间复杂度和遍历数组的时间复杂度是一样的 原文链接：https://blog.csdn.net/gongyiling3468/article/details/47804223 CMS垃圾收集器 线程池原理，增长策略，拒绝策略哪几种，四种线程池分别有什么优缺点，有什么坑，线程池使用该怎么选择 线程池原理：复用Thead线程，减少创建和回收的CPU、内存资源消耗，过多任务加入等待队列或拒绝 增长策略：当前线程数 &lt; 核心线程 : 直接开启新线程执行任务当前线程数 &gt; 核心线程 : 加入等待队列队列已满&amp; 当前线程数 &lt; 最大线程 : 开启新线程队列已满&amp; 当前线程数 &gt; 最大线程 : 执行拒绝策略 拒绝策略 CallerRunsPolicy： 只要线程池没有被关闭，那么由提交任务的线程自己来执行这个任务 AbortPolicy：不管怎样，直接抛出 RejectedExecutionException 异常， 这个是默认的策略， 如果我们构造线程池的时候不传相应的 handler 的话，那就会指定使用这个 DiscardPolicy：不做任何处理，直接忽略掉这个任务 DiscardOldestPolicy： 这个相对霸道一点，如果线程池没有被关闭的话， 把队列队头的任务(也就是等待了最长时间的)直接扔掉，然后提交这个任务到等待队列中 线程池对比 FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 CachedThreadPool的特点就是在线程池空闲时，即线程池中没有可运行任务时，它会释放工作线程，从而释放工作线程所占用的资源。但是，但当出现新任务时，又要创建一新的工作线程，又要一定的系统开销。并且，在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪 锁粗化、锁消除 锁粗化(程序员控制)通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽可能短，但是大某些情况下，一个程序对同一个锁不间断、高频地请求、同步与释放，会消耗掉一定的系统资源，因为锁的轻求、同步与释放本身会带来性能损耗，这样高频的锁请求就反而不利于系统性能的优化了，虽然单次同步操作的时间可能很短。 锁粗化就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。 一种极端的情况如下： 123456789public void doSomethingMethod()&#123; synchronized(lock)&#123; //do some thing &#125; //这是还有一些代码，做其它不需要同步的工作，但能很快执行完毕 synchronized(lock)&#123; //do other thing &#125;&#125; 上面的代码是有两块需要同步操作的，但在这两块需要同步操作的代码之间，需要做一些其它的工作，而这些工作只会花费很少的时间，那么我们就可以把这些工作代码放入锁内，将两个同步代码块合并成一个，以降低多次锁请求、同步、释放带来的系统性能消耗，合并后的代码如下: 12345678public void doSomethingMethod()&#123; //进行锁粗化：整合成一次锁请求、同步、释放 synchronized(lock)&#123; //do some thing //做其它不需要同步但能很快执行完的工作 //do other thing &#125;&#125; 注意：这样做是有前提的，就是中间不需要同步的代码能够很快速地完成，如果不需要同步的代码需要花很长时间，就会导致同步块的执行需要花费很长的时间，这样做也就不合理了。 锁消除锁消除是发生在编译器级别的一种锁优化方式。有时候我们写的代码完全不需要加锁，却执行了加锁操作。虚拟机即时编译器在运行时，对一些代码上要求同步，但是被检测到不可能存在共享数据竞争的锁进行削除。锁削除的主要判定依据来源于逃逸分析的数据支持，如果判断到一段代码中，在堆上的所有数据都不会逃逸出去被其他线程访问到，那就可以把它们当作栈上数据对待，认为它们是线程私有的，同步加锁自然就无须进行。 锁消除前提是java必须运行在server模式（server模式会比client模式作更多的优化），同时必须开启逃逸分析: 1-server -XX:+DoEscapeAnalysis -XX:+EliminateLocks 其中+DoEscapeAnalysis表示开启逃逸分析，+EliminateLocks表示锁消除。 异步thrift原理服务方起草接口标准，负责实现，RPC框架生成服务端和客户端代理，服务端代理自启动，客户端代理绑定调用方，调用方按照接口标准，调用客户端代理，等价于RPC远程调用服务方实现 定义一个Thrift Service 1234//1. IDL编写的接口service AddService &#123; int add(1:int n1, 2:int n2)&#125; Thrift Service方法会提供两种类型的实现： Iface AsyncIface 1234567//2. thrift.exe生成接口的客户端代理 - 异步实现class AddService&#123; static class AsyncClient &#123; void add(int n1, int n2, AsyncMethodCallback callback) &#123; ... &#125;&#125; 123456//3. 接口的服务端实现class AddServiceImpl implements AddService.Iface &#123; int add(int n1, int n2) return n1 + n2; &#125;&#125; 1234//4. 服务端监听TNonblockingServerSocket socket = new TNonblockingServerSocket(9090);TServer server = new TNonblockingServer(socket, AddServiceImpl);server.start(); 外部的调用过程就是，先获得一个CallBack，然后调用start方法。 123456789101112//5. 客户端请求TNonblockingTransport socket = new TNonblockingSocket("localhost", 9090);AddService.AsyncClient client = new AddService.AsyncClient(socket);client.add(1,2,new AsynCallback(this)&#123; void onComplete() &#123; ... &#125; void onError(Exception exception) &#123; ... &#125; &#125;&#125;); 异步使用原则如果使用了AsynIface实现Service，需要注意几点： 不能直接在方法内处理req，req需要和handler（callback）封装交给另外的线程进行处理（暂且把这些线程叫做worker线程） worker线程只做计算逻辑，也就是根据req的要求进行操作，在操作req结束以后获得的resp或者error，不能直接调用handler（callback）的方法（因为callback中的方法是一个网络IO的操作，有可能会block当前线程，如果网络IO操作是一个异步操作的话就不会block当前线程） 讲几个jvm优化的案例堆外内存泄漏怎么排查 异常堆栈 top信息 一定时间过后，java 进程内存增长到接近 90%，服务器报警。此时 old 区内存在 50%左右，由于未达到 CMS GC 的阈值，因此不会触发 CMS GC，而导致服务器内存溢出崩溃。 堆外内存计算方式 广义堆外内存为：进程内存 - (Young 区占用 + Old 区占用)，可通过直接内存大小参数: -神器:MaxDirectMemorySize 设置, JVM申请直接内存时，会判断是否超过可申请的直接内存阈值，如果超过则会调用 System.gc() 触发GC，如果 GC 后内存还是不足，则抛出 OutOfMemoryError 异常 狭义堆外内存为：java.nio.DirectByteBuffer 创建的时候分配的内存 查看堆内存命令 jstat -gc 1000 : 每秒输出堆内存实际大小信息jstat -gcutil 1000 : 每秒输出堆内存百分比信息 jvisual VM可视化监控内存实时情况， 通过dump文件可以找到对象的引用根节点。 jmeter压测； 为了分析堆外内存到底是谁占用了，不得不安装google-perftools工具进行分析，它的原理是在java应用程序运行时，当调用 malloc 时换用它的libtcmalloc.so，这样就能做一些统计了 原文链接：https://blog.csdn.net/u012099869/article/details/82757999 Tomcat响应web请求的过程 Service -&gt; Connector(Socket) -&gt; Container (HttpServletRequest) -&gt; Engine -&gt; Host -&gt; Context -&gt; WrapServlet Pipeline-Valve-责任链模式 StandardEnginePipeValve StandardHostPipeValve StandardContextPipeValve StandardWraperPipeValve CopyOnWriteArrayList、CopyOnWriteArraySetCopyOnWriteArrayList原理： 在写的时候不对原集合进行修改，而是重新复制一份，修改完之后，再移动指针 CopyOnWriteArrayList add()方法： 1234567891011121314151617181920/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; final ReentrantLock lock = this.lock;//重入锁 lock.lock();//加锁啦 try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1);//拷贝新数组 newElements[len] = e; setArray(newElements);//将引用指向新数组 1 return true; &#125; finally &#123; lock.unlock();//解锁啦 &#125;&#125; add()在添加集合的时候加上了锁，保证了同步，避免了多线程写的时候会Copy出N个副本出来. 有这么一种情况，当一个线程刚好调用完add()方法，也就是刚好执行到上面1处的代码，也就是刚好将引用指向新数组，而此时有线程正在遍历呢？会不会报错呢？答案是不会的，因为你正在遍历的集合是旧的。 优缺点 缺点： 1、耗内存（集合复制） 2、实时性不高 优点： 1、数据一致性完整，为什么？因为加锁了，并发数据不会乱 2、解决了像ArrayList、Vector这种集合多线程遍历迭代问题，记住，Vector虽然线程安全，只不过是加了synchronized关键字，迭代问题完全没有解决！ 使用场景 读多写少（白名单，黑名单，商品类目的访问和更新场景），为什么？因为写的时候会复制新集合 集合不大，为什么？因为写的时候会复制新集合 实时性要求不高，为什么，因为有可能会读取到旧的集合数据 项目中使用延迟队列的场景，延迟队列是如何实现的 DelayQueue基本原理DelayQueue是一个没有边界BlockingQueue实现，加入其中的元素必需实现Delayed接口。当生产者线程调用put之类的方法加入元素时，会触发Delayed接口中的compareTo方法进行排序，也就是说队列中元素的顺序是按到期时间排序的，而非它们进入队列的顺序。排在队列头部的元素是最早到期的，越往后到期时间赿晚。 1234567891011121314151617181920212223242526272829303132/** * 消息体定义 实现Delayed接口就是实现两个方法即compareTo 和 getDelay * 最重要的就是getDelay方法，这个方法用来判断是否到期 */ @Datapublic class Message implements Delayed &#123; private int id; private String body; // 消息内容 // 延迟时长，这个是必须的属性因为要按照这个判断延时时长。 private long excuteTime; public Message(int id, String body, long delayTime) &#123; this.id = id; this.body = body; this.excuteTime = TimeUnit.NANOSECONDS.convert(delayTime, TimeUnit.MILLISECONDS) + System.nanoTime(); &#125; // 自定义实现比较方法返回 1 0 -1三个参数 @Override public int compareTo(Delayed delayed) &#123; Message msg = (Message) delayed; return Integer.valueOf(this.id) &gt; Integer.valueOf(msg.id) ? 1 : (Integer.valueOf(this.id) &lt; Integer.valueOf(msg.id) ? -1 : 0); &#125; // 延迟任务是否到时就是按照这个方法判断如果返回的是负数则说明到期否则还没到期 @Override public long getDelay(TimeUnit unit) &#123; return unit.convert(this.excuteTime - System.nanoTime(), TimeUnit.NANOSECONDS); &#125; &#125; 消费者线程查看队列头部的元素，注意是查看不是取出。然后调用元素的getDelay方法，如果此方法返回的值小０或者等于０，则消费者线程会从队列中取出此元素，并进行处理。如果getDelay方法返回的值大于0，则消费者线程wait返回的时间值后，再从队列头部取出元素，此时元素应该已经到期。 DelayQueue是Leader-Followr模式的变种，消费者线程处于等待状态时，总是等待最先到期的元素，而不是长时间的等待。消费者线程尽量把时间花在处理任务上，最小化空等的时间，以提高线程的利用效率。 消费者线程的数量要够，处理任务的速度要快。否则，队列中的到期元素无法被及时取出并处理，造成任务延期、队列元素堆积等情况。 应用场景 关闭空闲连接。服务器中，有很多客户端的连接，空闲一段时间之后需要关闭之。 清理过期数据业务上。比如缓存中的对象，超过了空闲时间，需要从缓存中移出。 任务超时处理。在网络协议滑动窗口请求应答式交互时，处理超时未响应的请求。 下单之后如果三十分钟之内没有付款就自动取消订单。 订餐通知:下单成功后60s之后给用户发送短信通知。 当订单一直处于未支付状态时，如何及时的关闭订单，并退还库存 如何定期检查处于退款状态的订单是否已经退款成功 新创建店铺，N天内没有上传商品，系统如何知道该信息，并发送激活短信 定时任务调度：使用DelayQueue保存当天将会执行的任务和执行时间，一旦从DelayQueue中获取到任务就开始执行。 参考链接：https://juejin.im/post/5b5e52ecf265da0f716c3203https://blog.csdn.net/dkfajsldfsdfsd/article/details/88966814 内存溢出的排除、定位，虚拟机参数；查看堆占用情况： jmap 查看heap内存使用情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ jmap -heap 30124Attaching to process ID 30124, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.152-b16using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 4215275520 (4020.0MB) NewSize = 88080384 (84.0MB) MaxNewSize = 1405091840 (1340.0MB) OldSize = 176160768 (168.0MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 490733568 (468.0MB) used = 197747880 (188.58707427978516MB) free = 292985688 (279.41292572021484MB) 40.296383393116486% usedFrom Space: capacity = 12058624 (11.5MB) used = 8982888 (8.566749572753906MB) free = 3075736 (2.9332504272460938MB) 74.49347454568614% usedTo Space: capacity = 30932992 (29.5MB) used = 0 (0.0MB) free = 30932992 (29.5MB) 0.0% usedPS Old Generation capacity = 275251200 (262.5MB) used = 53956432 (51.45686340332031MB) free = 221294768 (211.0431365966797MB) 19.60261462983631% used 可以查看到MetaspaceSize, CompressedClassSpaceSize, MaxMetaSize jmap和jdk版本有关系，有些jdk版本会查看不到内存信息，可以使用jstat来查看统计信息 jstat 收集统计信息 123456789# 占用大小$ jstat -gc 30124 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 29696.0 6656.0 0.0 6144.1 462848.0 95727.6 268800.0 52699.8 63320.0 61816.0 7552.0 7224.3 15 0.327 3 0.626 0.953# 占用比例$ jstat -gcutil 30124 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 92.31 28.60 19.61 97.62 95.66 15 0.327 3 0.626 0.953 123456789-Xms 堆初始内存-Xmx 堆最大内存-XX:+UseG1GC/CMS 垃圾回收器-XX:+DisableEx神器icitGC 禁止显示GC-XX:MaxDirectM神器orySize 设置最大堆外内存，默认是:进程内存 - (Young 区占用 + Old 区占用)-Xss：每个线程的堆栈大小，默认1M-Xmn: 年轻代大小（eden区 + 2 * survivor）-XX:NewRatio=4 年轻代与老年代1:4-XX:survivorRa神器o=8 Eden区与survivor大小比值 具体步骤： jps找到进程id jstat -gc -gcutil确认堆占用情况及是否频繁full gc jmap -histo:live pid | head 找到最多的几个instance jmap dump:live,format=b,file=head.hprof pid dump出存活的实例堆栈信息 jvisualvm载入dump文件，从类信息栏找到占用内存最大的类，点击查询实例详情，进一步找到引用对象 ZooKeeperzk选主的详细过程在Zookeeper集群中，主要分为三者角色，而每一个节点同时只能扮演一种角色，这三种角色分别是： Leader接受所有Follower的提案请求并统一协调发起提案的投票，负责与所有的Follower进行内部的数据交换(同步); Follower直接为客户端服务并参与提案的投票，同时与Leader进行数据交换(同步); Observer直接为客户端服务但并不参与提案的投票，同时也与Leader进行数据交换(同步); Observer的作用是为了拓展系统，提高读取速度。 Server工作过程中四种状态 LOOKING：竞选状态，当前Server不知道leader是谁，正在搜寻。 LEADING：领导者状态，表明当前服务器角色是leader。 FOLLOWING：随从状态，表明当前服务器角色是follower，同步leader状态，参与投票。 OBSERVING，观察状态，表明当前服务器角色是observer，同步leader状态，不参与投票。 选主机制Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。 Zab协议有两种模式，它们分别是： 恢复模式（选主） 广播模式（同步 当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。 状态同步保证了leader和Server具有相同的系统状态。 leader选举是保证分布式数据一致性的关键 当zk集群中的一台服务器出现以下两种情况之一时，就会开始leader选举。 （1）服务器初始化启动。 （2）服务器运行期间无法和leader保持连接。 而当一台机器进入leader选举流程时，当前集群也可能处于以下两种状态。 （1）集群中本来就已经存在一个leader。 （2）集群中确实不存在leader。 首先第一种情况，通常是集群中某一台机器启动比较晚，在它启动之前，集群已经正常工作，即已经存在一台leader服务器。当该机器试图去选举leader时，会被告知当前服务器的leader信息，它仅仅需要和leader机器建立连接，并进行状态同步即可。 下面重点看第二种情况，即集群中leader不存在的情况下如何进行leader选举。 数据模型投票信息中包含两个最基本的信息。 sid: 即server id，用来标识该机器在集群中的机器序号； zxid: 即zookeeper事务id。ZooKeeper状态的每一次改变, 都对应着一个递增的Transaction id, 该id称为zxid. 由于zxid的递增性质, 如果zxid1小于zxid2, 那么zxid1肯定先于zxid2发生. 创建任意节点, 或者更新任意节点的数据, 或者删除任意节点, 都会导致Zookeeper状态发生改变, 从而导致zxid的值增加. electionEpoch：逻辑时钟，用来判断多个投票是否在同一轮选举周期中，该值在服务端是一个自增序列，每次进入新一轮的投票后，都会对该值进行加1操作; state：当前服务器的状态; 以（sid，zxid）的形式来标识一次投票信息。例如，如果当前服务器要推举sid为1，zxid为8的服务器成为leader，那么投票信息可以表示为（1，8） 规则集群中的每台机器发出自己的投票后，也会接受来自集群中其他机器的投票。每台机器都会根据一定的规则，来处理收到的其他机器的投票，以此来决定是否需要变更自己的投票。规则如下： （1）初始阶段，都会给自己投票。 （2）当接收到来自其他服务器的投票时，都需要将别人的投票和自己的投票进行pk，规则如下： 优先检查zxid, zxid比较大的服务器优先作为leader; 如果zxid相同的话，就比较sid，sid比较大的服务器作为leader; 总结： 首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。如果发现该外部选票的选举轮次小于当前服务器的，那么忽略该外部投票，同时立即发送自己的内部投票。 第二轮根据第一轮比较结果再次向集群中所有机器发出上一次投票信息即可。 一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING 在3.4.0后的Zookeeper的版本只保留了TCP版本的FastLeaderElection选举算法 electionEpoch：逻辑时钟，用来判断多个投票是否在同一轮选举周期中，该值在服务端是一个自增序列，每次进入新一轮的投票后，都会对该值进行加1操作。 链接：https://www.jianshu.com/p/75e48405d678 Redisredispool线程池工作原理，JDK怎么实现 复用socket连接，存储到LinkedBlockingDeque中 imeBetweenEvictionRunsMillis毫秒秒检查一次连接池中空闲的连接,把空闲时间超过minEvictableIdleTimeMillis毫秒的连接断开,直到连接池中的连接数到minIdle为止 redis 事务 MULTI严格意义来讲,redis的事务和我们理解的传统数据库(如mysql)的事务是不一样的。 Redis中的事务（transaction）是一组命令的集合。 事务的原理是先将属于一个事务的命令发送给Redis，然后再让Redis依次执行这些命令。 如果在发送EXEC命令前客户端断线了，则Redis会清空事务队列，事务中的所有命令都不会执行。而一旦客户端发送了EXEC命令，所有的命令就都会被执行，即使此后客户端断线也没关系，因为Redis中已经记录了所有要执行的命令。 除此之外，Redis的事务还能保证一个事务内的命令依次执行而不被其他命令插入。试想客户端A需要执行几条命令，同时客户端B发送了一条命令，如果不使用事务，则客户端B的命令可能会插入到客户端A的几条命令中执行。如果不希望发生这种情况，也可以使用事务。 和传统的mysql事务不同的事，即使我们的加钱操作失败,我们也无法在这一组命令中让整个状态回滚到操作之前。 语法错误语法错误指命令不存在或者命令参数的个数不对，比如跟在MULTI命令后执行了3个命令：一个是正确的命令，成功地加入事务队列；其余两个命令都有语法错误。而只要有一个命令有语法错误，执行EXEC命令后Redis就会直接返回错误，连语法正确的命令也不会执行。 运行错误运行错误指在命令执行时出现的错误，比如使用散列类型的命令操作集合类型的键，这种错误在实际执行之前Redis是无法发现的，所以在事务里这样的命令是会被Redis接受并执行的。如果事务里的一条命令出现了运行错误，事务里其他的命令依然会继续执行，Redis的事务没有关系数据库事务提供的回滚（rollback）功能。为此开发者必须在事务执行出错后自己收拾剩下的摊子。 redis watch机制 WATCH命令可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行。监控一直持续到EXEC命令（事务中的命令是在EXEC之后才执行的，所以在MULTI命令后可以修改WATCH监控的键值） 利用watch实现incr具体做法如下: 123456WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 注意点由于WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以在一般的情况下我们需要在EXEC执行失败后重新执行整个函数。 执行EXEC命令后会取消对所有键的监控，如果不想执行事务中的命令也可以使用UNWATCH命令来取消监控。 Redis为什么扩容时会产生超时skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(logN)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 链接：https://juejin.im/post/57fa935b0e3dd90057c50fbc redis跳跃表 各种搜索结构提高效率的方式都是通过空间换时间得到的； 跳表最终形成的结构和搜索树很相似； 跳表通过随机的方式来决定新插入节点来决定索引的层数； 跳表搜索的时间复杂度是 O(logn)，插入/删除也是； 为了满足自身的功能需要， Redis 基于 William Pugh 论文中描述的跳跃表进行了以下修改： 允许重复的 score 值：多个不同的 member 的 score 值可以相同。 进行对比操作时，不仅要检查 score 值，还要检查 member ：当 score 值可以重复时，单靠 score 值无法判断一个元素的身份，所以需要连 member 域都一并检查才行。 每个节点都带有一个高度为 1 层的后退指针，用于从表尾方向向表头方向迭代：当执行 ZREVRANGE 或 ZREVRANGEBYSCORE 这类以逆序处理有序集的命令时，就会用到这个属性。 实例 12345678910redis&gt; ZADD s 6 x 10 y 15 z(integer) 3redis&gt; ZRANGE s 0 -1 WITHSCORES1) &quot;x&quot;2) &quot;6&quot;3) &quot;y&quot;4) &quot;10&quot;5) &quot;z&quot;6) &quot;15&quot; 在底层实现中， Redis 为 x 、 y 和 z 三个 member 分别创建了三个字符串， 值分别为 double 类型的 6 、 10 和 15 ， 然后用跳跃表将这些指针有序地保存起来， 形成这样一个跳跃表： 跳跃表是一种随机化数据结构，查找、添加、删除操作都可以在对数期望时间下完成。 跳跃表目前在 Redis 的唯一作用，就是作为有序集类型的底层数据结构之一，另一个构成有序集的结构是字典。 为了满足自身的需求，Redis 基于 William Pugh 论文中描述的跳跃表进行了修改，包括： score 值可重复， 经典skiplist中是不允许的； 对比一个元素需要同时检查它的 score 和 memeber； 每个节点带有高度为 1 层的后退指针，双向链表，用于从表尾方向向表头方向迭代； 在skiplist中可以很方便地计算出每个元素的排名(rank)； 实际上，Redis中sorted set的实现是这样的： 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个dict + 一个skiplist来实现的。简单来讲，dict用来查询数据到分数的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 网络 tcp确认机制在TCP确认机制中，无法有效处理非连续TCP片段。确认号表明所有低于该编号的sequence number已经被发送该编号的设备接收。如果我们收到的字节数落在两个非连续的范围内，则无法只通过一个编号来确认。这可能导致潜在严重的性能问题，特别是高速或可靠性较差的网络。 仅重传超时片段每个片段发送后，发送端都有一个计时器，在一定时间内没有收到包含该片段的ack信息，就认为该片段接收超时。这是一种更加保守的方式，仅重传超时的片段，希望其他片段都能够成功接收。如果该片段之后的其他片段实际上接收到了，这一方式是最佳的，如果没接收到，就无法正常执行。后者的情况每一个片段需要单独计时并重传。假设上述最坏情况下，所有20个500字节片段都丢失了。我们需要等片段1超时并重传。这一片段也许会得到确认，但之后我们需要等待片段2超时并重传。这一过程会重复多次。 重传所有片段这是一种更激进或者说更悲观的方式。无论何时一个片段超时了，不仅重传该片段，还有所有其他尚未确认的片段。这一方式确保了任何时间都有一个等待确认的停顿时间，在所有未确认片段丢失的情况下，会刷新全部未确认片段，以使对端设备多一次接收机会。在所有20个片段都丢失的情况下，相对于第一种方式节省了大量时间。这种方式的问题在于可能这些重传是不必要的。如果第一个片段丢失而其他19个实际上接收到了，也得重传那9500字节数据。 由于TCP不知道其他片段是否接收到，所以它也无法确认哪种方法更好，但只能选择一种方式。解决方式是对TCP滑动窗口算法进行扩展，添加允许设备分别确认非连续片段的功能。这一功能称为选择确认（selective acknowledgment, SACK）。 选择确认：通过SACK，连接的两方设备必须同时支持这一功能，通过连接时使用的SYN片段来协商是否允许SACK。这一过程完成之后，任一设备都可以在常规TCP片段中使用SACK选项。这一选项包含一个关于 已接收但未确认片段数据sequence number范围的列表，由于它们是非连续的。 各设备对重传队列进行修改，如果该片段已被选择确认过，则该片段中的SACK比特位置为1。该设备使用图2中激进方式的改进版本，一个片段重传之后，之后所有片段也会重传，除非SACK比特位为1。 例如，在4个片段的情况下，如果客户端接收到片段4而没有接收到片段3，当它发回确认号为201（片段1和片段2）的确认信息，其中包含一个SACK选项指明：“已接收到字节361至500，但尚未确认”。如果片段4在片段1和2之后到达，上述信息也可以通过第二个确认片段来完成。服务器确认片段4的字节范围，并为片段4打开SACK位。当片段3重传时，服务器看到片段4的SACK位为1，就不会对其重传。 在片段3重传之后，片段4的SACK位被清除。这是为了防止客户端出于某种原因改变片段4已接收的想法。客户端应当发送确认号为501或更高的确认信息，正式确认片段3和4接收到。如果这一情况没有发生，服务器必须接收到片段4的另一条选择确认信息才能将它的SACK位打开，否则，在片段3重传时或计时器超时的情况下会对其自动重传。 https://wizardforcel.gitbooks.io/network-basic/9.html 为什么tcp是可靠协议，怎么做到不丢包的？ 窗口滑动协议重传机制 将TCP与UDP这样的简单传输协议区分开来的是它传输数据的质量。TCP对于发送数据进行跟踪，这种数据管理需要协议有以下两大关键功能： 可靠性：保证数据确实到达目的地。如果未到达，能够发现并重传； 数据流控：管理数据的发送速率，以使接收设备不致于过载； 要完成这些任务，整个协议操作是围绕滑动窗口确认机制来进行的。 TCP面向流的滑动窗口确认机制:每一条消息都有一个识别编号，每一条消息都能够被独立地确认，因此同一时刻可以发送多条信息。设备B定期发送给A一条发送限制参数，制约设备A一次能发送的消息最大数量。设备B可以对该参数进行调整，以控制设备A的数据流。 为了提高速度，TCP并没有按照字节单个发送而是将数据流划分为片段。片段内所有字节都是一起发送和接收的，因此也是一起确认的。确认机制没有采用message ID字段，而是使用的片段内最后一个字节的sequence number。因此一次可以处理不同的字节数，这一数量即为片段内的sequence number。 发送窗口整个过程关键的操作在于接收方允许发送方一次能容纳的未确认的字节数，有时也称为窗口。该窗口决定了发送方允许传送的字节数。 可用窗口考虑到正在传输的数据量，发送方仍被允许发送的数据量。 https://wizardforcel.gitbooks.io/network-basic/7.html 七层负载均衡和四层负载均衡的区别 二层负载均衡会通过一个虚拟 MAC 地址接收请求，然后再分配到真实的 MAC 地址； 三层负载均衡会通过一个虚拟 IP 地址接收请求，然后再分配到真实的 IP 地址； 四层通过虚拟 IP + 端口接收请求，然后再分配到真实的服务器； 七层通过虚拟的 URL 或主机名接收请求，然后再分配到真实的服务器。 所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。 对于一般的应用来说，有了Nginx就够了。Nginx可以用于七层负载均衡。但是对于一些大的网站，一般会采用DNS+四层负载+七层负载的方式进行多层次负载均衡。 对比：负载均衡器通常称为四层交换机或七层交换机。 四层交换机主要分析 IP 层及 TCP/UDP 层，实现四层流量负载均衡。 七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如 HTTP 协议 URI 或 Cookie 信息。 负载均衡分为 L4 Switch（四层交换），即在 OSI第4层工作，就是 TCP层。此种 Load Balancer 不理解应用协议（如 HTTP/FTP/MySQL 等等）。例子：LVS，F5。 L7 Switch（七层交换），OSI 的最高层，应用层。此时，该 Load Balancer 能理解应用协议。例子： HAProxy，MySQL Proxy。 注意：上面的很多 Load Balancer 既可以做四层交换，也可以做七层交换。 常用负载均衡工具Nginx/LVS/HAProxy是目前使用最广泛的三种负载均衡软件。 LVSLVS（Linux Virtual Server），也就是Linux虚拟服务器, 是一个由章文嵩博士发起的自由软件项目。使用LVS技术要达到的目标是：通过LVS提供的负载均衡技术和Linux操作系统实现一个高性能、高可用的服务器群集，它具有良好可靠性、可扩展性和可操作性。从而以低廉的成本实现最优的服务性能。 LVS主要用来做四层负载均衡。 NginxNginx（发音同engine x）是一个网页服务器，它能反向代理HTTP, HTTPS, SMTP, POP3, IMAP的协议链接，以及一个负载均衡器和一个HTTP缓存。 Nginx主要用来做七层负载均衡。 HAProxyHAProxy是一个使用C语言编写的自由及开放源代码软件，其提供高可用性、负载均衡，以及基于TCP和HTTP的应用程序代理。 Haproxy主要用来做七层负载均衡。 负载均衡算法可以分为两类：静态负载均衡算法和动态负载均衡算法。 静态负载均衡算法包括：轮询，比率，优先权 动态负载均衡算法包括: 最少连接数, 最快响应速度，观察方法，预测法，动态性能分配， 动态服务器补充， 服务质量， 服务类型， 规则模式。 https://cloud.tencent.com/developer/article/1082047 微信网页扫码登陆的通信过程是什么样的；OAuth2协议 123456789101112131415161718192021222324252627 +----------+ | Resource | | Owner | | | +----------+ ^ | (B) +----|-----+ Client Identifier +---------------+ | -+----(A)-- &amp; Redirection URI ----&gt;| | | User- | | Authorization | | Agent -+----(B)-- User authenticates ---&gt;| Server | | | | | | -+----(C)-- Authorization Code ---&lt;| | +-|----|---+ +---------------+ | | ^ v (A) (C) | | | | | | ^ v | | +---------+ | | | |&gt;---(D)-- Authorization Code ---------&apos; | | Client | &amp; Redirection URI | | (Web后端)| | | |&lt;---(E)----- Access Token -------------------&apos; +---------+ (w/ Optional Refresh Token)Note: The lines illustrating steps (A), (B), and (C) are broken into two parts as they pass through the user-agent. 算法求一个int数二进制形式中1出现的个数位运算(1) count = n &amp; 1; n&gt;&gt;1; 循环32次(2) n &amp; (n-1), 每次可把最低位1变成 0； 一个二叉树某一个路径和等于某值回溯算法 字符串倒转的时间复杂度和空间复杂度双指针法一次遍历，使用 O(1) 的额外空间 12345678public void reverseString(char[] s) &#123; char temp; for(int i = 0, j = s.length - 1; i &lt;= j; i++, j--)&#123; temp = s[j]; s[j] = s[i]; s[i] = temp; &#125;&#125; 时间复杂度：O(N)*，执行了 *N/2 次的交换。空间复杂度：O(1)，只使用了常数级空间 两个链表分别表示两个数，求和 双队列法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; Stack&lt;ListNode&gt; stack1 = new Stack&lt;ListNode&gt;(); Stack&lt;ListNode&gt; stack2 = new Stack&lt;ListNode&gt;(); while(l1 != null)&#123; stack1.add(l1); l1 = l1.next; &#125; while(l2 != null)&#123; stack2.add(l2); l2 = l2.next; &#125; ListNode h0 = new ListNode(0); ListNode cur1 , cur2; int re = 0; while(!stack1.isEmpty() || !stack2.isEmpty())&#123; cur1 = null; cur2 = null; if(!stack1.isEmpty())&#123; cur1 = stack1.pop(); &#125; if(!stack2.isEmpty())&#123; cur2 = stack2.pop(); &#125; if(cur1 == null)&#123; cur1 = cur2; &#125; else if(cur2 != null)&#123; cur1.val += cur2.val; &#125; cur1.val += re; int temp = cur1.val; cur1.val = temp % 10; re = temp / 10; cur1.next = h0.next; h0.next = cur1; &#125; if(re != 0)&#123; ListNode node = new ListNode(re); node.next = h0.next; h0.next = node; &#125; return h0.next; &#125;&#125; 递归法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; int flow=0; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; if(l1==null) return l2; if(l2==null) return l1; ListNode res1=l1,res2=l2; int len1=0,len2=0; while(l1!=null)&#123; len1++; l1=l1.next; &#125; while(l2!=null)&#123; len2++; l2=l2.next; &#125; ListNode res=len1&gt;len2?add(res1,res2,len1,len2):add(res2,res1,len2,len1); if(flow==1) &#123; res1=new ListNode(1); res1.next=res; return res1; &#125; return res; &#125; public ListNode add(ListNode l1, ListNode l2,int len1,int len2) &#123; int temp; if((len1==1)&amp;&amp;(len2==1))&#123; temp=l1.val; l1.val=(l1.val+l2.val)%10; flow=(temp+l2.val)/10; &#125; else if(len1&gt;len2) &#123; temp=l1.val; l1.next=add(l1.next, l2, len1-1, len2); l1.val=(temp+flow)%10; flow=(temp+flow)/10; &#125; else &#123; l1.next=add(l1.next, l2.next, len1-1, len2-1); temp=l1.val; l1.val=(temp+flow+l2.val)%10; flow=(temp+flow+l2.val)/10; &#125; return l1; &#125;&#125; 两个list分别表示两个数，求和paxo算法是啥，zab算法怎么对ip进行限流，比如某个ip 1小时最多访问1万次，写出代码；从A、B两个数组中各取一个数，求两数差值最小N个数组中取两个数差值最小， N个数组中取两个数差值最小 电梯算法每层有一个value表示可上或可下value层，求A层到B层的最短按键数 实现一个栈， 有push pop 获取最小值 三个方法 要求 快！直播间在线人数统计有一个日志文件，一个人进入直播间会有一个uid和进入时间，退出直播间会有一个uid和退出时间，一次直播的过程中 同一时刻最多的在线人数怎么求？ 一次遍历，第一次出现 +1， 第二次出现 -1； 接上一题，如果日志文件不是按照时间排序的，那在o(n)时间内求同一时刻最多的在线人数]]></content>
      <categories>
        <category>Java</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot基本原理]]></title>
    <url>%2F2020%2F01%2F10%2FSpringBoot%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[启动流程启动类代码 123456@SpringBootApplicationpublic class SpringBootDemoApplication extends SpringBootServletInitializer &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootDemoApplication.class, args); &#125;&#125; 对照上面的典型代码，这个两个元素分别是： @SpringBootApplicationSpringApplication 以及 run() 方法 SpringApplication 这个类应该算是 SpringBoot 框架 的“创新”产物了，原始的 Spring中并没有这个类，SpringApplication 里面封装了一套 Spring 应用的启动流程，然而这对用户完全透明，因此我们上手 SpringBoot 时感觉简洁、轻量。 一般来说默认的 SpringApplication 执行流程已经可以满足大部分需求，但是 若用户想干预这个过程，则可以通过 SpringApplication 在流程某些地方开启的 扩展点 来完成对流程的扩展，典型的扩展方案那就是使用 set 方法。我们来举一个栗子，把我们天天司空见惯的 SpringBoot 应用的启动类来拆解一下写出来： 12345678910@SpringBootApplicationpublic class SpringBootDemoApplication &#123; public static void main( String[] args ) &#123; // 这是传统SpringBoot应用的启动，一行代码搞定，内部默认做了很多事 // SpringApplication.run( SpringBootDemoApplication args ); SpringApplication app = new SpringApplication(SpringBootDemoApplication ); app.setXXX( ... ); // 用户自定的扩展在此 ！！！ app.run( args ); &#125;&#125; 这样一拆解后我们发现，我们也需要先构造 SpringApplication 类对象，然后调用该对象的 run() 方法。那么接下来就讲讲 SpringApplication 的构造过程 以及其 run() 方法的流程，搞清楚了这个，那么也就搞清楚了SpringBoot应用是如何运行起来的！ SpringApplication 实例的初始化首先看下SpringApplication的构造方法： 12345678910111213public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, "PrimarySources must not be null"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); // (1) 推断应用的类型 this.webApplicationType = WebApplicationType.deduceFromClasspath(); // (2) 查找并加载 classpath下 META-INF/spring.factories文件中所有可用的 ApplicationContextInitializer setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class)); // (3) 查找并加载 classpath下 META-INF/spring.factories文件中的所有可用的 ApplicationListener setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); // (4) 推断并设置 main方法的定义类 this.mainApplicationClass = deduceMainApplicationClass();&#125; 详细过程如下： （1）推断应用的类型：根据你classpath 下是否能找到对应的class文件， 推断应用类型， 优先级依次是：REACTIVE 、NONE、SERVLET(默认)。 123456789101112static WebApplicationType deduceFromClasspath() &#123; if (ClassUtils.isPresent(WEBFLUX_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(WEBMVC_INDICATOR_CLASS, null) &amp;&amp; !ClassUtils.isPresent(JERSEY_INDICATOR_CLASS, null)) &#123; return WebApplicationType.REACTIVE; &#125; for (String className : SERVLET_INDICATOR_CLASSES) &#123; if (!ClassUtils.isPresent(className, null)) &#123; return WebApplicationType.NONE; &#125; &#125; return WebApplicationType.SERVLET;&#125; （2）使用 SpringFactoriesLoader查找并加载 classpath下 META-INF/spring.factories文件中所有可用的 ApplicationContextInitializer 1234# Initializersorg.springframework.context.ApplicationContextInitializer=\org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener (3) 使用 SpringFactoriesLoader查找并加载 classpath下 META-INF/spring.factories文件中的所有可用的 ApplicationListener 123# Application Listenersorg.springframework.context.ApplicationListener=\org.springframework.boot.autoconfigure.BackgroundPreinitializer (4) 推断并设置main方法的定义类 1234567891011121314private Class&lt;?&gt; deduceMainApplicationClass() &#123; try &#123; StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) &#123; if ("main".equals(stackTraceElement.getMethodName())) &#123; return Class.forName(stackTraceElement.getClassName()); &#125; &#125; &#125; catch (ClassNotFoundException ex) &#123; // Swallow and continue &#125; return null;&#125; SpringApplication 的run()方法先看代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); // 通过 SpringFactoriesLoader 加载META-INF/spring.factories文件，获取并创建 SpringApplicationRunListener对象 SpringApplicationRunListeners listeners = getRunListeners(args); // 然后由 SpringApplicationRunListener 来发出 starting 消息 listeners.starting(); try &#123; // 创建参数，并配置当前 SpringBoot 应用将要使用的 Environment ApplicationArguments applicationArguments = new DefaultApplicationArguments(args); // 完成之后，依然由 SpringApplicationRunListener 来发出 environmentPrepared 消息 ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); // 打印欢迎页信息 Banner printedBanner = printBanner(environment); context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances(SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); // 初始化 ApplicationContext，并设置 Environment，加载相关配置等 prepareContext(context, environment, listeners, applicationArguments, printedBanner); // 刷新context，最重要的一步，完成IOC容器注入整个过程 refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass).logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context;&#125; 流程图如下： 自动装配原理自动装配过程分析自动装配原理得从 @SpringbootApplication 入手分析 @SpringbootApplication包含了@SpringBootConfiguration，@EnableAutoConfiguration，@ComponentScan 12345678910@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication &#123; @ComponentScan如果没有指定扫描包，因此它默认扫描的是与该类同级的类或者同级包下的所有类; @SpringBootConfiguration通过源码得知它是一个@Configuration; @EnableAutoConfiguration 一旦加上此注解，那么将会开启自动装配功能，简单点讲，Spring会试图在你的classpath下找到所有配置的Bean然后进行装配。当然装配Bean时，会根据若干个@Conditional定制规则来进行初始化; 1234567891011121314151617181920212223@SuppressWarnings("deprecation")@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(EnableAutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration &#123; String ENABLED_OVERRIDE_PROPERTY = "spring.boot.enableautoconfiguration"; /** * Exclude specific auto-configuration classes such that they will never be applied. * @return the classes to exclude */ Class&lt;?&gt;[] exclude() default &#123;&#125;; /** * Exclude specific auto-configuration class names such that they will never be */ String[] excludeName() default &#123;&#125;;&#125; 根据文档注释的说明它指点我们去看EnableAutoConfigurationImportSelector。但是该类在SpringBoot1.5.X版本已经过时了，因此我们看一下它的父类AutoConfigurationImportSelector; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered &#123; private static final String[] NO_IMPORTS = &#123;&#125;; private static final Log logger = LogFactory .getLog(AutoConfigurationImportSelector.class); private ConfigurableListableBeanFactory beanFactory; private Environment environment; private ClassLoader beanClassLoader; private ResourceLoader resourceLoader; @Override public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; try &#123; // 读取mata-info/spring-autoconfigure-metadata.properties元数据与元数据的相关属性 AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = getAttributes(annotationMetadata); // 去 mata-info/spring.factories 文件中查询 EnableAutoConfiguration值 List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); // 去除重复的配置类，若我们自己写的starter可能存在重复的 configurations = removeDuplicates(configurations); configurations = sort(configurations, autoConfigurationMetadata); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); // 根据maven导入的启动器过滤出需要导入的配置类 configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return configurations.toArray(new String[configurations.size()]); &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125; &#125; protected boolean isEnabled(AnnotationMetadata metadata) &#123; return true; &#125;&#125; 首先该类实现了DeferredImportSelector接口，这个接口继承了ImportSelector, 该接口主要是为了导入 @Configuration 的配置项，而 DeferredImportSelector 是延期导入，当所有的@Configuration都处理过后才会执行; 回过头来我们看一下 AutoConfigurationImportSelector 的selectImport方法, 该方法刚开始会先判断是否进行自动装配，而后会从 META-INF/spring-autoconfigure-metadata.properties 读取元数据与元数据的相关属性，紧接着会调用getCandidateConfigurations方法： 12345678910111213141516protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames( getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, "No auto configuration classes found in META-INF/spring.factories. If you " + "are using a custom packaging, make sure that file is correct."); return configurations; &#125;/** * Return the class used by &#123;@link SpringFactoriesLoader&#125; to load configuration candidates.*/protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() &#123; return EnableAutoConfiguration.class;&#125; 在这里又遇到SpringFactoryiesLoader, 它会读取META-INF/spring.factories下的EnableAutoConfiguration的配置，紧接着在进行排除与过滤，进而得到需要装配的类。最后让所有配置在META-INF/spring.factories下的AutoConfigurationImportListener执行AutoConfigurationImportEvent事件，代码如下： 1234567891011121314151617private void fireAutoConfigurationImportEvents(List&lt;String&gt; configurations, Set&lt;String&gt; exclusions) &#123; List&lt;AutoConfigurationImportListener&gt; listeners = getAutoConfigurationImportListeners(); if (!listeners.isEmpty()) &#123; AutoConfigurationImportEvent event = new AutoConfigurationImportEvent(this, configurations, exclusions); for (AutoConfigurationImportListener listener : listeners) &#123; invokeAwareMethods(listener); listener.onAutoConfigurationImportEvent(event); &#125; &#125;&#125;protected List&lt;AutoConfigurationImportListener&gt; getAutoConfigurationImportListeners() &#123; return SpringFactoriesLoader.loadFactories(AutoConfigurationImportListener.class, this.beanClassLoader);&#125; 总结1）自动装配还是利用了 SpringFactoriesLoader 来加载META-INF/spring.factoires文件里所有配置的EnableAutoConfgruation，它会经过exclude和filter等操作，最终确定要装配的类 2) 处理@Configuration的核心还是ConfigurationClassPostProcessor，这个类实现了BeanFactoryPostProcessor, 因此当AbstractApplicationContext执行refresh()方法里的invokeBeanFactoryPostProcessors(beanFactory)方法时会执行自动装配 自定义starterTomcat启动流程 EmbeddedWebServerFactoryCustomizerAutoConfiguration 内嵌web容器工厂自定义定制器装配类 org.springframework.context.support.AbstractApplicationContext#refresh 如何扫描自定义组件Conditional注解常见的注解解释： @ConditionalOnBean匹配给定的class类型或者Bean的名字是否在SpringBeanFactory中存在 @ConditionalOnClass匹配给定的class类型是否在类路径(classpath)中存在 @ConditionalOnExpression匹配给定springEL表达式的值返回true时 @ConditionalOnJava匹配JDK的版本，其中range属性是枚举类型有两个值可以选择 EQUAL_OR_NEWER 不小于 OLDER_THAN 小于 value属性用于设置jdk版本 ConditionalOnMissingBeanspring上下文中不存在指定bean时 ConditionalOnWebApplication在web环境下创建 参考SpringBoot学习之自动装配深入理解SpringBoot之自动装配]]></content>
      <categories>
        <category>Spring</category>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>原理</tag>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty快速入门]]></title>
    <url>%2F2020%2F01%2F08%2FNetty%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Java NIO 核心概念 Linux五种I/O模型比较 Socket通信模型 Java NIO Buffer 一个Buffer本质上是内存中的一块， 可以将数据写入这块内存， 从这块内存获取数据 java.nio 定义了以下几个Buffer的实现: Java NIO Buffer三大核心概念：position、limit、capacity 最好理解的当然是 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。 一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值 从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了。 写操作模式下，limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity。 写结束后，切换到读模式，此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满了 DirectByteBuffer &amp; HeapByteBuffer DirectByteBuffer HeapByteBuffer 描述 底层存储在非JVM堆上，通过native代码操作 -神器:MaxDirectMemorySize= 标准java类，维护一份byte[]在JVM堆上 创建开销 大 小 存储位置 Native Heap Java Heap 数据拷贝 无需临时缓冲区做拷贝 拷贝到临时DirectByteBuffer,但临时缓冲区使用缓存， 聚集写/发散读时没有缓存临时缓冲区 GC影响 每次创建或者释放的时候都调用一次System.gc() java垃圾回收机制自动回收 JAVA NIO Channel所有的NIO操作始于通道，通道是数据来源或数据写入的目的地，java.nio 包中主要实现的以下几个 Channel： FileChannel：文件通道，用于文件的读和写 DatagramChannel：用于 UDP 连接的接收和发送 SocketChannel：把它理解为 TCP 连接通道，简单理解就是 TCP 客户端 ServerSocketChannel：TCP 对应的服务端，用于监听某个端口进来的请求 Java NIO Selector java.nio.channels.Selector 支持IO多路复用的抽象实体 注册Selectable Channel SelectionKey —— 表示Selector和被注册的channel之间关系的一份凭证 SelectionKey保存channel感兴趣的事件 Selector.select 更新所有就绪的 SelectionKey 的状态, 并返回就绪的channel个数 迭代Selected Key集合并处理就绪channel Selector基本操作 创建Selector 1Selector selector = Selector.open(); 注册Channel到Selector 123SocketChannel channel = SocketChannel.open();channel.configureBlocking(false);SelectionKey key = channel.register(selector, SelectionKey.OP_READ); register的第二个参数是一个“关注集合”，代表关注的channel状态，有四种基础类型可供监听, 用SelectionKey中的常量表示如下： 1234SelectionKey.OP_CONNECTSelectionKey.OP_ACCEPTSelectionKey.OP_READSelectionKey.OP_WRITE 从Selector中选择channel一旦向Selector注册了一个或多个channel后，就可以调用select来获取channel, select()方法会返回所有处于就绪状态的channel, select方法具体如下： 123int select()int select(long timeout)int selectNow() select()方法的返回值是一个int，代表有多少channel处于就绪了。也就是自上一次select后有多少channel进入就绪。 selectedKeys()在调用select并返回了有channel就绪之后，可以通过选中的key集合来获取channel，这个操作通过调用selectedKeys()方法： 1Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Selector编程模板 1234567891011121314151617181920212223Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys();Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator();while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove();&#125; Netty核心概念Netty 提供异步的、事件驱动的网络应用程序框架和工具,用以快速开发高性能、高可靠性的网络服务器和客户端程序。 Netty主要组件 Netty Server启动主要流程 设置服务端ServerBootStrap启动参数 1234group(parentGroup, childGroup):channel(NioServerSocketChannel): 设置通道类型handler()：设置NioServerSocketChannel的ChannelHandlerPipelinechildHandler(): 设置NioSocketChannel的ChannelHandlerPipeline 通过ServerBootStrap的bind方法启动服务端，bind方法会在parentGroup中注册NioServerScoketChannel，监听客户端的连接请求 会创建一个NioServerSocketChannel实例，并将其在parentGroup中进行注册 Netty Server执行主要流程 Client发起连接CONNECT请求，parentGroup中的NioEventLoop不断轮循是否有新的客户端请求，如果有，ACCEPT事件触发 ACCEPT事件触发后，parentGroup中NioEventLoop会通过NioServerSocketChannel获取到对应的代表客户端的NioSocketChannel，并将其注册到childGroup中 childGroup中的NioEventLoop不断检测自己管理的NioSocketChannel是否有读写事件准备好，如果有的话，调用对应的ChannelHandler进行处理 Netty EventLoop EventLoopGroup 包括多个EventLoop 多个EventLoop之间不交互 EventLoop 每个EventLoop对应一个线程 所有连接(channel)都将注册到一个EventLoop，并且只注册到一个，整个生命周期中都不会变化 每个EventLoop管理着多个连接(channel) EventLoop来处理连接(Channel)上的读写事件 ServerBootstrap 包括2个不同类型的EventLoopGroup: Parent EventLoop: 负责处理Accept事件，接收请求 Child EventLoop：负责处理读写事件 EventExecutor视图 EventExecutorGroup里面有一个EventExecutor数组，保存了多个EventExecutor; EventExecutorGroup是不干什么事情的，当收到一个请后，他就调用next()获得一个它里面的EventExecutor，再调用这个executor的方法； next(): EventExecutorChooser.next()定义选择EventExecutor的策略； ByteBuf类型 根据内存的位置 HeapByteBuf 基于数组- 内部为一个字节数组 (byte array) hasArray()返回True array()返回其内部的数组，可以对数组进行直接操作 DirectByteBuf 堆外内存 具有更好的性能 创建和释放开销更大 根据是否使用内存池 Pooled vs Unpooled 根据是否使用Unsafe操作(Unsafe) Safe vs Unsafe 复合缓冲区（CompositeByteBuf） 多个ByteBuf组合的视图 一个ByteBuf列表，可动态的添加和删除其中的 ByteBuf 可能既包含堆缓冲区，也包含直接缓冲区 ByteBuf分配不直接通过new来创建，而是通过ByteBufAllocator来创建 UnpooledByteBufAllocator PooledByteBufAllocator ChannelHandler业务处理核心逻辑，用户自定义, Netty 提供2个重要的 ChannelHandler 子接口： ChannelInboundHandler - 处理进站数据和所有状态更改事件 ChannelOutboundHandler - 处理出站数据，允许拦截各种操作 ChannelPiplineChannelPipeline是ChannelHandler容器 包括一系列的ChannelHandler实例,用于拦截流经一个 Channel 的入站和出站事件 每个Channel都有一个其ChannelPipeline 可以修改 ChannelPipeline 通过动态添加和删除 ChannelHandler 定义了丰富的API调用来回应入站和出站事件 ChannelHandlerContextChannelHandlerContext表示 ChannelHandler 和 ChannelPipeline 之间的关联，在 ChannelHandler 添加到 ChannelPipeline 时创建 Netty线程模型 Reactor模式 - Doug Lea 单线程Reactor 多线程Reactor 所有逻辑都在I/O线程中完成，不开启单独线程。图中对应的TheadPool是在io处理handler中额外开启的业务线程池。 Multiple Reactor Netty与Reactor模式 单线程Reactor 123EventLoopGroup bossGroup = new NioEventLoopGroup(1);ServerBootStrap bootStrap = new ServerBootStrap();bootStrap.group(bossGroup, bossGroup); // 监听和处理都由一个线程完成 多线程Reactor 1234EventLoopGroup bossGroup = new NioEventLoopGroup(1);ServerBootStrap bootStrap = new ServerBootStrap();bootStrap.group(bossGroup, bossGroup); // 监听和处理都由一个线程完成// 在handler中额外使用线程池处理业务 Multiple Reactor 12345EventLoopGroup bossGroup = new NioEventLoopGroup(1);EventLoopGroup workerGroup = new NioEventLoopGroup();ServerBootStrap bootStrap = new ServerBootStrap();bootStrap.group(bossGroup, workerGroup); // 在handler中额外使用线程池处理业务 Boss EventLoopGroup Worker EventLoopGroup Netty Start Process ServerBootStrap Netty编码解码 半包粘包问题 TCP/IP协议 面向“流”协议 MSS: Maxitum Segment Size 最大分段大小，表示TCP数据包每次能够传输的最大数据分段 发送方/接收方缓冲区 （Nagle算法） 解决思路 基本思路就是不断从TCP缓冲区中读取数据，每次读取完都需要判断是否是一个完整的数据包 若当前读取的数据不足以拼接成一个完整的业务数据包，那就保留该数据，继续从tcp缓冲区中读取，直到得到一个完整的数据包 定长 分隔符 基于长度的变长包 若当前读到的数据加上已经读取的数据足够拼接成一个数据包，那就将已经读取的数据拼接上本次读取的数据，够成一个完整的业务数据包传递到业务逻辑，多余的数据仍然保留，以便和下次读到的数据尝试拼接 常用编码解码器编码解码器的作用就是将原始的字节数据与自定义的消息对象进行互相转换，目前业界主流的序列化框架有： ProtoBuf Jboss Marshalling Java Serialization Netty常用的自带编解码器有： LineBasedFrameDecoder（\n, \r\n) 回车换行解码器 配合StringDecoder DelimiterBasedFrameDecoder 分隔符解码器 FixedLengthFrameDecoder 固定长度解码器 LengthFieldBasedFrameDecoder 基于包头’不固定长度‘解码器(私有协议最常用) 参数说明 maxFrameLength：包的最大长度 lengthFieldOffset：长度属性的起始位（偏移位），包中存放长度属性字段的起始位置 lengthFieldLength：长度属性的长度 lengthAdjustment：长度调节值，在总长被定义为包含包头长度时，修正信息长度 initialBytesToStrip：跳过的字节数，根据需要跳过lengthFieldLength个字节，以便接收端直接接受到不含“长度属性”的内容 Netty拆包的基类 - ByteToMessageDecoder 内部维护了一个数据累积器cumulation，每次读取到数据都会不断累加，然后尝试对累加到的数据进行拆包，拆成一个完整的业务数据包 每次都将读取到的数据通过内存拷贝的方式， 累积到cumulation中 调用子类的decode方法对累积的数据尝试进行拆包]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring mvc 设计思想与体系结构]]></title>
    <url>%2F2020%2F01%2F06%2FSpring-mvc-%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B8%8E%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[一、spring mvc 设计思想与体系结构组成 回顾servlet 与jsp 执行过程 流程说明： 请求Servlet 处理业务逻辑 设置业务Model forward jsp Servlet jsp Servlet 解析封装html 返回 提问：这个是一个MVC应用场景吗？ spring mvc本质上还是在使用Servlet处理，并在其基础上进行了封装简化了开发流程，提高易用性、并使用程序逻辑结构变得更清晰 基于注解的URL映谢 http表单参数转换 全局统一异常处理 拦截器的实现 spring mvc 执行流程 整个过程是如何实现的？ dispatchServlet 如何找到对应的Control？ 如何执行调用Control 当中的业务方法？ 回答这些问题之前我们先来认识一下spring mvc 体系结构 spring mvc 体系结构 HandlerMapping url与控制器的映谢 HandlerAdapter 控制器执行适配器 ViewResolver 视图仓库 view 具体解析视图 HandlerExceptionResolver 异常捕捕捉器 HandlerInterceptor 拦截器 体系结构UML 二、mvc 执行流程解析 知识点 mvc 具体执行流程 HandlerMapping详解 HandlerAdapter 详解 ViewResolver与View详解 HandlerExceptionResolver详解 HandlerInterceptor 详解 mvc 各组件执行流程 HandlerMapping详解其为mvc中url路径与Control对像的映射，DispatcherServlet 就是基于此组件来寻找对应的Control，如果找不到就会报Not Found mapping 的异常。 HandlerMapping 接口方法 HandlerMapping 接口结构 目前主流的三种mapping 如下： BeanNameUrlHandlerMapping基于ioc name 中以 / 开头的Bean时行 注册至映谢. SimpleUrlHandlerMapping基于手动配置 url 与control 映谢 RequestMappingHandlerMapping基于@RequestMapping注解配置对应映谢 演示基于 BeanNameUrlHandlerMapping 配置映谢。 编写mvc 文件 12&lt;!--简单控制器--&gt;&lt;bean id="/user.do" class="com.tuling.mvc.control.BeanNameControl"/&gt; beanname control 控制器 1234567public class BeanNameControl implements HttpRequestHandler &#123; @Override public void handleRequest(HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException &#123; request.getRequestDispatcher("/WEB-INF/page/userView.jsp").forward(request, response); &#125;&#125; 当IOC 中实例化这些类之后 DispatcherServlet 就会通过org.springframework.web.servlet.DispatcherServlet#getHandler() 方法基于request查找对应Handler。 但找到对应的Handler之后我们发现他是一个Object类型，并没有实现特定接口。如何调用Handler呢？ HandlerAdapter详解这里spring mvc 采用适配器模式来适配调用指定Handler，根据Handler的不同种类采用不同的Adapter, 其Handler与 HandlerAdapter 对应关系如下: Handler类别 对应适配器 描述 Controller SimpleControllerHandlerAdapter 标准控制器，返回ModelAndView HttpRequestHandler HttpRequestHandlerAdapter 业务自行处理 请求，不需要通过modelAndView 转到视图 Servlet SimpleServletHandlerAdapter 基于标准的servlet 处理 HandlerMethod RequestMappingHandlerAdapter 基于@requestMapping对应方法处理 HandlerAdapter 接口方法 HandlerAdapter 接口结构图 演示基于Servlet 处理 SimpleServletHandlerAdapter 12345&lt;!-- 配置控制器 --&gt;&lt;bean id="/hello.do" class="com.tuling.mvc.control.HelloServlet"/&gt;&lt;!-- 配置适配器 --&gt;&lt;bean class="org.springframework.web.servlet.handler.SimpleServletHandlerAdapter"/&gt; 1234567// 标准Servletpublic class HelloServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; resp.getWriter().println("hello luban "); &#125;&#125; 上述例子中当IOC 中实例化这些类之后 DispatcherServlet 就会通过org.springframework.web.servlet.DispatcherServlet#getHandlerAdapter() 方法查找对应handler的适配器 ，如果找不到就会报 No adapter for handler 。 ViewResolver 与View 详解找到应的Adapter 之后就会基于适配器调用业务处理，处理完之后业务方会返回一个ModelAndView ，在去查找对应的视图进行处理。其在org.springframework.web.servlet.DispatcherServlet#resolveViewName() 中遍历 viewResolvers 列表查找，如果找不到就会报一个 Could not resolve view with name异常。 下一步就是基于ViewResolver.resolveViewName()获取对应View来解析生成Html并返回 。对应VIEW结构如下： 至此整个正向流程就已经走完了，如果此时程序处理异常 MVC 该如何处理呢？ HandlerExceptionResolver详解该组件用于指示当出现异常时 mvc 该如何处理。 dispatcherServlet 会调用org.springframework.web.servlet.DispatcherServlet#processHandlerException() 方法，遍历 handlerExceptionResolvers 处理异常，处理完成之后返回errorView 跳转到异常视图。 演示自定义异常捕捉 123456public class SimpleExceptionHandle implements HandlerExceptionResolver &#123; @Override public ModelAndView resolveException(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; return new ModelAndView("error"); &#125;&#125; 12&lt;!-- 演示异常配置 --&gt;&lt;bean class="com.tuling.mvc.control.SimpleExceptionHandle"/&gt; HandlerExceptionResolver 结构 除了上述组件之外 spring 中还引入了 我Interceptor 拦截器 机制，类似于Filter。 HandlerInterceptor详解演示HandlerInterceptor 1234567891011121314151617public class SimpleHandlerInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println("preHandle"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; System.out.println("postHandle"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; System.out.println("afterCompletion"); &#125;&#125; 12&lt;!--配置interceptor 组件--&gt;&lt;bean class="com.tuling.mvc.control.SimpleHandlerInterceptor"&gt;&lt;/bean&gt; 其实现机制是基于 HandlerExecutionChain 分别在 doDispatch 方法中执行以下方法： preHandle ：业务处理前执行 postHandle：业务处理后（异常则不执行） afterCompletion：视图处理后 具体逻辑源码参见：org.springframework.web.servlet.DispatcherServlet#doDispatch 方法。 三、注解配置 演示基于注解配置mvc mapping 1234567891011&lt;context:component-scan base-package="com.tuling.mvc.control" /&gt;&lt;!-- 注解驱动 --&gt;&lt;mvc:annotation-driven/&gt;&lt;!-- 视图仓库 --&gt;&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/page/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;property name="viewClass" value="org.springframework.web.servlet.view.JstlView" /&gt; &lt;/bean&gt; 1234567// 注解方法@RequestMapping("/hello.do")public ModelAndView hello() &#123; ModelAndView mv = new ModelAndView("userView"); mv.addObject("name", "luban"); return mv;&#125; 提问 为什么基于 &lt;mvc:annotation-driven/&gt; 配置就能实现mvc 的整个配置了，之前所提到的 handlerMapping 、与 handlerAdapter 组件都不适用了？ 只要查看以类的源就可以知晓其中原因： 认识 NamespaceHandler 接口 查看 MvcNamespaceHandler 查看AnnotationDrivenBeanDefinitionParser 结论在 &lt;mvc:annotation-driven /&gt; 对应的解析器，自动向 ioc 里面注册了两个BeanDefinition。分别是：RequestMappingHandlerMapping 与BeanNameUrlHandlerMapping.]]></content>
      <categories>
        <category>Spring</category>
        <category>Spring Mvc</category>
      </categories>
      <tags>
        <tag>原理</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring事务底层基本原理]]></title>
    <url>%2F2020%2F01%2F03%2FSpring%E4%BA%8B%E5%8A%A1%E5%BA%95%E5%B1%82%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[数据库的事务基本特性事物是区分文件存储系统与Nosql数据库重要特性之一，其存在的意义是为了保证即使在并发情况下也能正确的执行crud操作。怎样才算是正确的呢？这时提出了事物需要保证的四个特性即ACID： A: 原子性(atomicity)事物中各项操作，要么全做要么全不做，任何一项操作的失败都会导致整个事物的失败； C: 一致性(consistency)事物结束后系统状态是一致的； I: 隔离性(isolation)并发执行的事物彼此无法看到对方的中间状态； D: 持久性(durability)事物完成后所做的改动都会被持久化，即使发生灾难性的失败。 在高并发的情况下，要完全保证其ACID特性是非常困难的，除非把所有的事物串行化执行，但带来的负面的影响将是性能大打折扣。很多时候我们有些业务对事物的要求是不一样的，所以数据库中设计了四种隔离级别，供用户基于业务进行选择。 脏读 :一个事物读取到另一事物未提交的更新数据 不可重复读 :在同一事物中,多次读取同一数据返回的结果有所不同, 换句话说, 后续读取可以读到另一事物已提交的更新数据. 相反, “可重复读”在同一事物中多次读取数据时, 能够保证所读数据一样, 也就是后续读取不能读到另一事物已提交的更新数据。 幻读 :查询表中一条数据如果不存在就插入一条，并发的时候却发现，里面居然有两条相同的数据。这就幻读的问题。 数据库默认隔离级别： Oracle中默认级别是 Read committed mysql 中默认级别 Repeatable read。另外要注意的是mysql 执行一条查询语句默认是一个独立的事物，所以看上去效果跟Read committed一样。 查看mysql 的默认隔离级别 SELECT @@tx_isolation Spring对事务的支持与使用spring 事物相关API说明spring 事物是在数据库事物的基础上进行封装扩展，其主要特性如下： 支持原有的数据事物的隔离级别； 加入了事物传播的概念，提供多个事物的合并或隔离的功能； 提供声明式事物，让业务代码与事物分离，事物变得更易用； 怎么样去使用Spring事物呢？spring提供了三个接口供使用事物。分别是： TransactionDefinition事物定义 PlatformTransactionManager事物管理 TransactionStatus事物运行时状态 接口结构图： 基于API实现事物123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class SpringTransactionExample &#123; private static String url = "jdbc:mysql://192.168.0.147:3306/luban2"; private static String user = "root"; private static String password = "123456"; public static Connection openConnection() throws ClassNotFoundException, SQLException &#123; Class.forName("com.mysql.jdbc.Driver"); Connection conn = DriverManager.getConnection("jdbc:mysql://192.168.0.147:3306/luban2", "root", "123456"); return conn; &#125; public static void main(String[] args) &#123; final DataSource ds = new DriverManagerDataSource(url, user, password); final TransactionTemplate template = new TransactionTemplate(); template.setTransactionManager(new DataSourceTransactionManager(ds)); template.execute(new TransactionCallback&lt;Object&gt;() &#123; @Override public Object doInTransaction(TransactionStatus status) &#123; Connection conn = DataSourceUtils.getConnection(ds); Object savePoint = null; try &#123; &#123; // 插入 PreparedStatement prepare = conn. prepareStatement("insert INTO account (accountName,user,money) VALUES (?,?,?)"); prepare.setString(1, "111"); prepare.setString(2, "aaaa"); prepare.setInt(3, 10000); prepare.executeUpdate(); &#125; // 设置保存点 savePoint = status.createSavepoint(); &#123; // 插入 PreparedStatement prepare = conn. prepareStatement("insert INTO account (accountName,user,money) VALUES (?,?,?)"); prepare.setString(1, "222"); prepare.setString(2, "bbb"); prepare.setInt(3, 10000); prepare.executeUpdate(); &#125; &#123; // 更新 PreparedStatement prepare = conn. prepareStatement("UPDATE account SET money= money+1 where user=?"); prepare.setString(1, "asdflkjaf"); Assert.isTrue(prepare.executeUpdate() &gt; 0, ""); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (Exception e) &#123; System.out.println("更新失败"); if (savePoint != null) &#123; status.rollbackToSavepoint(savePoint); &#125; else &#123; status.setRollbackOnly(); &#125; &#125; return null; &#125; &#125;); &#125;&#125; 声明示事物我们前面是通过调用API来实现对事物的控制，这非常的繁琐，与直接操作JDBC事物并没有太多的改善，所以Spring提出了声明示事物，使我们对事物的操作变得非常简单，甚至不需要关心它。 配置spring.xml 1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd"&gt; &lt;context:annotation-config/&gt; &lt;context:component-scan base-package="com.tuling.service.**"&gt; &lt;/context:component-scan&gt; &lt;bean class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; &lt;!-- similarly, don't forget the PlatformTransactionManager --&gt; &lt;bean id="txManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; &lt;!-- don't forget the DataSource --&gt; &lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;constructor-arg name="url" value="jdbc:mysql://192.168.0.147/luban2"/&gt; &lt;constructor-arg name="username" value="root"/&gt; &lt;constructor-arg name="password" value="123456"/&gt; &lt;/bean&gt; &lt;tx:annotation-driven transaction-manager="txManager"&gt;&lt;/tx:annotation-driven&gt;&lt;/beans&gt; 编写服务类 1234567@Transactionalpublic void addAccount(String name, int initMenoy) &#123; String accountid = new SimpleDateFormat("yyyyMMddhhmmss").format(new Date()); jdbcTemplate.update("insert INTO account (accountName,user,money) VALUES (?,?,?)", accountid, name, initMenoy); // 人为报错, 触发回滚 int i = 1 / 0;&#125; 事物传播机制 类别 事务传播类型 说明 支持当前事务 PROPAGATION_REQUIRED （必须的） 如果当前没有事物，就新建一个事物，如果已经存在一个事物中， 加入到这个事物中。这是最常见的选择 PROPAGATION_SUPPORTS （支持） 支持当前事物，如果当前没有事物，就以非事物方式执行 PROPAGATION_MANDATORY （强制） 使用当前的事物，如果当前没有事物，就抛出异常 不支持当前事物 PROPAGATION_REQUIRES_NEW (隔离) 新建事物，如果当前存在事物，把当前事物挂起 PROPAGATION_NOT_SUPPORTED (不支持) 以非事物方式执行操作，如果当前存在事物，就把当前事物挂起 PROPAGATION_NEVER (强制非事物) 以非事物方式执行，如果当前存在事物，则抛出异常 嵌套事物 PROPAGATION_NESTED （嵌套事物） 如果当前存在事物，则在嵌套事物内执行。如果当前没有事物， 则执行与PROPAGATION_REQUIRED类似的操作。 常用事物传播机制： PROPAGATION_REQUIRED这个也是默认的传播机制； PROPAGATION_NOT_SUPPORTED可以用于发送提示消息，站内信、短信、邮件提示等。不属于并且不应当影响主体业务逻辑，即使发送失败也不应该对主体业务逻辑回滚； PROPAGATION_REQUIRES_NEW总是新启一个事物，这个传播机制适用于不受父方法事物影响的操作，比如某些业务场景下需要记录业务日志，用于异步反查，那么不管主体业务逻辑是否完成，日志都需要记录下来，不能因为主体业务逻辑报错而丢失日志； 演示常用事物的传播机制 用例1:创建用户时初始化一个帐户，表结构和服务类如下。 表结构 服务类 功能描述 user UserService 创建用户并添加账户 account AccountService 添加账户 UserSerivce.createUser(name) 实现代码: 1234567@Transactionalpublic void createUser(String name) &#123; // 新增用户基本信息 jdbcTemplate.update("INSERT INTO `user` (name) VALUES(?)", name); //调用accountService添加帐户 accountService.addAccount(name, 10000); ｝ AccountService.addAccount(name,initMoney) 实现代码（方法的最后有一个异常）: 1234567@Transactional(propagation = Propagation.REQUIRED)public void addAccount(String name, int initMoney) &#123; String accountid = new SimpleDateFormat("yyyyMMddhhmmss").format(new Date()); jdbcTemplate.update("insert INTO account (accountName,user,money) VALUES (?,?,?)", accountid, name, initMenoy); // 出现分母为零的异常 int i = 1 / 0;&#125; 实验预测： AOP事务底层实现原理讲事物原理之前我们先来做一个实验，当场景五的环境改变，把 addAccount 方法移至UserService类下，其它配置和代码不变： 1234567891011121314@Override@Transactionalpublic void createUser(String name) &#123; jdbcTemplate.update("INSERT INTO `user` (name) VALUES(?)", name); addAccount(name, 10000); // 人为报错 int i = 1 / 0;&#125;@Transactional(propagation = Propagation.REQUIRES_NEW)public void addAccount(String name, int initMoney) &#123; String accountid = new SimpleDateFormat("yyyyMMddhhmmss").format(new Date()); jdbcTemplate.update("insert INTO account (accountName,user,money) VALUES (?,?,?)", accountid, name, initMoney);&#125; 经过测试我们发现得出的结果与场景五并不一至，required_new 没有起到其对应的作用。原因在于 spring 声明示事物使用动态代理实现，而当调用同一个类的方法时，是会不会走代理逻辑的，自然事物的配置也会失效。 通过一个动态代理的实现来模拟这种场景 UserService.java 123456public interface UserService &#123; void createUser(String name); void addAccount(String name, int initMoney);&#125; UserServiceImpl.java 12345678910111213141516171819202122232425262728public class UserServiceImpl implements UserService &#123; private static String url = "jdbc:mysql://10.101.38.255:8036/tuling"; private static String user = "root"; private static String password = "xxxxx"; private static DataSource dataSource; private static JdbcTemplate jdbcTemplate; static &#123; dataSource = new DriverManagerDataSource(url, user, password); jdbcTemplate = new JdbcTemplate(dataSource); &#125; @Override public void createUser(String name) &#123; jdbcTemplate.update("INSERT INTO `user` (name) VALUES(?)", name); addAccount(name, 10000); // 人为报错 // int i = 1 / 0; &#125; @Override public void addAccount(String name, int initMoney) &#123; String accountid = new SimpleDateFormat("yyyyMMddhhmmss").format(new Date()); jdbcTemplate.update("insert INTO account (accountName,user,money) VALUES (?,?,?)", accountid, name, initMoney); &#125;&#125; TransactionProxy.java 123456789101112131415161718192021public class TransactionProxy &#123; private static UserService userSerivce = new UserServiceImpl(); public static void main(String[] args) &#123; UserService proxyUserService = (UserService) Proxy.newProxyInstance(TransactionProxy.class.getClassLoader(), new Class[] &#123;UserService.class&#125;, new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; System.out.println("开启事物:" + method.getName()); return method.invoke(userSerivce, args); &#125; finally &#123; System.out.println("关闭事物:" + method.getName()); &#125; &#125; &#125;); proxyUserService.createUser("luban"); proxyUserService.addAccount("austin", 10); &#125;&#125; 当我们调用 createUser 方法时, 仅打印了 createUser 的事物开启、关闭，并没有打印addAccount方法的事物开启、关闭，由此可见 addAccount 的事物配置是失效的。]]></content>
      <categories>
        <category>Spring</category>
        <category>Spring Mvc</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 核心组件]]></title>
    <url>%2F2020%2F01%2F02%2FSpring-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[实体Bean的创建基于Class构建1&lt;bean class=&quot;com.tuling.spring.HelloSpring&quot;&gt;&lt;/bean&gt; 这是最常规的方法，其原理是在spring底层会基于class属性通过反射进行构建。 构造方法构建1234&lt;bean class=&quot;com.tuling.spring.HelloSpring&quot;&gt; &lt;constructor-arg name=&quot;name&quot; type=&quot;java.lang.String&quot; value=&quot;luban&quot;/&gt; &lt;constructor-arg index=&quot;1&quot; type=&quot;java.lang.String&quot; value=&quot;sex&quot; /&gt;&lt;/bean&gt; 如果需要基于参数进行构建，就采用构造方法构建，其对应属性如下：name: 构造方法参数变量名称type: 参数类型index: 参数索引，从0开始value: 参数值，spring 会自动转换成参数实际类型值ref: 引用容器的其它对象 静态工厂方法创建123&lt;bean class="com.tuling.spring.HelloSpring" factory-method="build"&gt; &lt;constructor-arg name="type" type="java.lang.String" value="B"/&gt;&lt;/bean&gt; 如果你正在对一个对象进行A/B测试 ，就可以采用静态工厂方法的方式创建，其于策略创建不同的对像或填充不同的属性。该模式下必须创建一个静态工厂方法，并且方法返回该实例，spring 会调用该静态方法创建对象。 123456789public static HelloSpring build(String type) &#123; if (type.equals("A")) &#123; return new HelloSpring("luban", "man"); &#125; else if (type.equals("B")) &#123; return new HelloSpring("diaocan", "woman"); &#125; else &#123; throw new IllegalArgumentException("type must A or B"); &#125;&#125; FactoryBean创建12&lt;!-- 返回的并不是LubanFactoryBean实例，而是被LubanFactoryBean包装的实例 --&gt;&lt;bean class="com.tuling.spring.LubanFactoryBean" id="lubanFactoryBean"&gt;&lt;/bean&gt; 指定一个Bean工厂来创建对象，对象构建初始化完全交给该工厂来实现。配置Bean时指定该工厂类的类名。 1234567891011121314151617// LubanFactoryBean只是起到一层包装代理作用 public class LubanFactoryBean implements FactoryBean &#123; // 真正创建的bean实例 @Override public Object getObject() throws Exception &#123; return new HelloSpring(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; return HelloSpring.class; &#125; @Override public boolean isSingleton() &#123; return false; &#125;&#125; Bean的基本特性作用范围很多时候Bean对象是无状态的 ，而有些又是有状态的, 无状态的对象我们采用单例即可，而有状态则必须是多例的模式，通过scope即可创建 scope=“prototype”scope=“singleton” 12&lt;bean class="com.tuling.spring.HelloSpring" scope="prototype"&gt;&lt;/bean&gt; 如果一个Bean设置成prototype我们可以 通过BeanFactoryAware获取 BeanFactory对象即可每次获取的都是新对像。 生命周期Bean对象的创建、初始化、销毁即是Bean的生命周期。通过 init-method、destroy-method属性可以分别指定期构建方法与初始方法。 1&lt;bean class="com.tuling.spring.HelloSpring" init-method="init" destroy-method="destroy"&gt;&lt;/bean&gt; 如果觉得麻烦，可以让Bean去实现 InitializingBean.afterPropertiesSet()、DisposableBean.destroy()方法。分别对应初始和销毁方法。 加载机制指示Bean在何时进行加载。设置lazy-init即可，其值如下： true: 懒加载，即延迟加载false: 非懒加载，容器启动时即创建对象default: 默认，采用default-lazy-init中指定值，如果default-lazy-init 没指定就是false 1234&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"default-lazy-init="true"&gt; 什么时候使用懒加载？懒加载会容器启动的更快，而非懒加载可以容器启动时更快的发现程序当中的错误 ，选择哪一个就看追求的是启动速度，还是希望更早的发现错误，一般我们会选择后者。 Bean的构建过程spring.xml文件中保存了我们对Bean的描述配置，BeanFactory会读取这些配置然后生成对应的Bean。这是我们对ioc原理的一般理解。但在深入一些我们会有更多的问题: 配置信息最后是谁JAVA中哪个对象承载的？ 这些承载对象是谁业读取XML文件并装载的？ 这些承载对象又是保存在哪里？ BeanDefinition（Bean定义）ioc实现中我们在xml中描述的Bean信息最后都将保存至BeanDefinition （定义）对象中，其中xml bean 与BeanDefinition 是一对一的关系。 由此可见，xml bean中设置的属性最后都会体现在BeanDefinition中。如: XML-bean BeanDefinition class beanClassName scope scope lazy-init lazyInit constructor-arg ConstructorArgument property MutablePropertyValues factory-method factoryMethodName destroy-method AbstractBeanDefinition.destroyMethodName init-method AbstractBeanDefinition.initMethodName autowire AbstractBeanDefinition.autowireMode id name BeanDefinition属性结构 BeanDefinitionRegistry（Bean注册器）在上表中我们并没有看到 xml bean 中的 id 和name属性， 没有体现在定义中，原因是ID其作为当前Bean的存储key注册到了BeanDefinitionRegistry 注册器中。name 作为别名key 注册到了 AliasRegistry 注册中心。其最后都是指向其对应的BeanDefinition。 BeanDefinitionReader（Bean定义读取）至此我们学习了 BeanDefinition 中存储了Xml Bean信息，而BeanDefinitionRegister 基于ID和name 保存了Bean的定义。接下要学习的是从xml Bean到BeanDefinition然后在注册至BeanDefinitionRegister 整个过程。 上图中可以看出Bean的定义是由BeanDefinitionReader 从xml 中读取配置并构建出 BeanDefinitionReader, 然后在基于别名注册到BeanDefinitionRegister中. BeanDefinitionReader结构 方法说明： loadBeanDefinitions(Resource resource) 基于资源装载Bean定义并注册至注册器 int loadBeanDefinitions(String location) 基于资源路径装载Bean定义并注册至注册器 BeanDefinitionRegistry getRegistry() 获取注册器 ResourceLoader getResourceLoader() 获取资源装载器 BeanDefinitionReader装载过程 123456789101112//创建一个简单注册器BeanDefinitionRegistry register = new SimpleBeanDefinitionRegistry();//创建bean定义读取器BeanDefinitionReader reader = new XmlBeanDefinitionReader(register);// 创建资源读取器DefaultResourceLoader resourceLoader = new DefaultResourceLoader();// 获取资源Resource xmlResource = resourceLoader.getResource("spring.xml");// 装载Bean的定义reader.loadBeanDefinitions(xmlResource);// 打印构建的Bean 名称System.out.println(Arrays.toString(register.getBeanDefinitionNames()); Beanfactory(bean 工厂)有了Bean的定义就相当于有了产品的配方，接下来就是要把这个配方送到工厂进行生产了。在ioc当中Bean的构建是由 BeanFactory 负责的。其结构如下： 方法说明： getBean(String) 基于ID或name 获取一个Bean ** T getBean(Class requiredType) ** 基于Bean的类别获取一个Bean（如果出现多个该类的实例，将会报错。但可以指定 primary=“true” 调整优先级来解决该错误 ） Object getBean(String name, Object… args) 基于名称获取一个Bean，并覆盖默认的构造参数 boolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) 指定Bean与指定Class 是否匹配 以上方法中重点要关注getBean，当用户调用getBean的时候就会触发 Bean的创建动作: 基本BeanFactory获取一个Bean, 以下是运行时的栈信息： 1234567891011121314151617181920// 其反射实例化Beanjava.lang.reflect.Constructor.newInstance(Unknown Source:-1)BeanUtils.instantiateClass()//基于实例化策略 实例化BeanSimpleInstantiationStrategy.instantiate()AbstractAutowireCapableBeanFactory.instantiateBean()// 执行Bean的实例化方法AbstractAutowireCapableBeanFactory.createBeanInstance()AbstractAutowireCapableBeanFactory.doCreateBean()// 执行Bean的创建AbstractAutowireCapableBeanFactory.createBean()// 缓存中没有，调用指定Bean工厂创建BeanAbstractBeanFactory$1.getObject()// 从单例注册中心获取Bean缓存DefaultSingletonBeanRegistry.getSingleton()AbstractBeanFactory.doGetBean()// 获取BeanAbstractBeanFactory.getBean()// 调用的客户类com.tuling.spring.BeanFactoryExample.main() Bean创建时序图 从调用过程可以总结出以下几点： 调用 BeanFactory.getBean() 会触发Bean的实例化 DefaultSingletonBeanRegistry 中缓存了单例Bean Bean的创建与初始化是由AbstractAutowireCapableBeanFactory完成的 BeanFactory 与 ApplicationContext区别BeanFactory 可以去做IOC当中的大部分事情，为什么还要去定义一个ApplicationContext 呢？ ApplicationContext 结构图 从图中可以看到 ApplicationContext 它由 BeanFactory 接口派生而来，因而提供了BeanFactory 所有的功能。除此之外context包还提供了以下的功能： MessageSource, 提供国际化的消息访问 资源访问，如URL和文件 事件传播，实现了ApplicationEventPublisher接口的bean 载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层 Spring advice执行顺序advice执行顺序，就是拦截器链的执行顺序 1、单个切面的场景，around开始 –&gt; before –&gt; aound结束–&gt; after –&gt; AfterReturning 2、对于多个切面的场景，可以通过 @order(序号)，来调整执行顺序。 demo参考： Spring多个AOP执行先后顺序 源码分析：前置通知拦截器： MethodBeforeAdviceInterceptor#invoke拦截器链：ReflectiveMethodInvocation#proceedproceed 根据 currentInterceptorIndex 来确定当前应执行哪个拦截器，并在调用拦截器的 invoke 方法时，将自己作为参数传给该方法 参考资料Spring AOP 源码分析 - 拦截器链的执行过程Spring AOP 源码分析（生成代理对象） IOC 容器只存放单例bean吗结论：IOC 容器只存放单例bean IOC容器初始化的时候，会将所有bean初始化在 singletonObjects 这个CurrentHashMap 中， bean是单例的。 在获取bean的时候，首先会从singletonObjects去取值，通过debug，发现如果scope是单例，则可以获取到bean，如果scope是多例，则获取不到bean，需要 从一个叫 mergedBeanDefinitions 的CurrentHashMap中去获取bean的定义，然后再根据bean的scope去决定如何创建bean，如果scope=prototype，则每次都会创建一个新的实例。 猜想：IOC在初始化时，只会将 scope = singleton（单例）的对象进行实例化，而不会去实例化scope=prototype的对象（多例）； 证实：找到singletonObjects.put方法，debug看一下singletonObjects.put的前提条件是什么 源码分析：AbstractBeanFactory#doGetBean 单例的场景，直接从 singletonObjects 这个Map中获取bean 多例的场景，发现从 singletonObjects 中拿不到值 接下来，发现它是从一个叫 mergedBeanDefinitions 的HashMap中获取了RootBeanDefinition，里面包含了bean的一些基础信息。 最后根据 bean的scope属性,来做处理，如果作用域是单例，则直接从容器中获取，如果作用域是多例，则创建一个实例，当然，作用域还有其它，自己可以一一去验证 Scope为request的bean是否会放入IOC容器这个应该和IOC没有关系，request对象只是一个参数。场景分析：SpringMVC在接受一个http请求后，会根据URl去匹配具体的bean（可以这样理解：bean=Map.get(url)，这个map是在容器初始化的时候创建的），然后通过反射获取class实例，最终执行method.invoke方法的时候，会将request对象的值封装到args数组中，当然args可能还包含response对象、注解参数、非注解参数的值。]]></content>
      <categories>
        <category>Spring</category>
        <category>Spring Mvc</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java堆默认大小]]></title>
    <url>%2F2019%2F12%2F31%2FJava%E5%A0%86%E9%BB%98%E8%AE%A4%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[PrintFlagsFinal -XX:+PrintFlagsFinal打印所有可设置的参数及它们的默认值(从JDK 6 update 21开始才可以用) 示例：16G内存的Centos系统 123456789101112$ java -XX:+PrintFlagsFinal -version | grep -E 'HeapSize|PermSize|ThreadStackSize' intx CompilerThreadStackSize = 0 &#123;pd product&#125; uintx ErgoHeapSizeLimit = 0 &#123;product&#125; uintx HeapSizePerGCThread = 87241520 &#123;product&#125; uintx InitialHeapSize := 264241152 &#123;product&#125; uintx LargePageHeapSizeThreshold = 134217728 &#123;product&#125; uintx MaxHeapSize := 4215275520 &#123;product&#125; intx ThreadStackSize = 1024 &#123;pd product&#125; intx VMThreadStackSize = 1024 &#123;pd product&#125;java version "1.8.0_152"Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode) 可以看出默认的:堆最大大小(MaxHeapSize)：4215275520/(1024.0 * 1024 * 1024) = 3.93G初始堆大小(InitialHeapSiz): 266338304/(1024.0 * 1024) = 254MHeapSizePerGCThread: 87241520 / (1024.0 * 1024) = 83.20M PrintCommandLineFlags -XX:+PrintCommandLineFlags 打印出所有出现在命令行上的可选JVM参数。可用来了解JVM的参数设置，例如堆空间大小、垃圾收集器等。默认情况下，此选项是禁用的，并且不打印标记。 示例： 12345$ java -XX:+PrintCommandLineFlags -version-XX:InitialHeapSize=263453376 -XX:MaxHeapSize=4215254016 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC java version "1.8.0_152"Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Bean生命周期]]></title>
    <url>%2F2019%2F12%2F30%2FSpring-Bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[Spring Bean生命周期生命周期流程首先看下生命周期图： 再来一张执行过程： Spring Bean的生命周期只有四个阶段。要彻底搞清楚Spring的生命周期，首先要把这四个阶段牢牢记住。实例化和属性赋值对应构造方法和setter方法的注入，初始化和销毁是用户能自定义扩展的两个阶段。在这四步之间穿插的各种扩展点。 实例化 Instantiation 属性赋值 Populate 初始化 Initialization 销毁 Destruction 实例化 -&gt; 属性赋值 -&gt; 初始化 -&gt; 销毁 主要逻辑都在doCreateBean()方法中，逻辑很清晰，就是顺序调用以下三个方法，这三个方法与三个生命周期阶段一一对应，非常重要，在后续扩展接口分析中也会涉及。 createBeanInstance() -&gt; 实例化populateBean() -&gt; 属性赋值initializeBean() -&gt; 初始化 源码如下，能证明实例化，属性赋值和初始化这三个生命周期的存在。关于本文的Spring源码都将忽略无关部分，便于理解： 123456789101112131415161718192021// 忽略了无关代码protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (instanceWrapper == null) &#123; // 实例化阶段！ instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // Initialize the bean instance. Object exposedObject = bean; try &#123; // 属性赋值阶段！ populateBean(beanName, mbd, instanceWrapper); // 初始化阶段！ exposedObject = initializeBean(beanName, exposedObject, mbd); &#125;&#125; 至于销毁，是在容器关闭时调用的，详见ConfigurableApplicationContext#close() 在谈生命周期之前有一点需要先明确： Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 AOP扩展bean生命周期Spring生命周期相关的常用切入方式非常多，主要有下列方式： 注解方式在 bean 初始化时会经历几个阶段，首先可以使用注解 @PostConstruct, @PreDestroy 来在 bean 的创建和销毁阶段进行调用。 两个生命周期接口InitializingBean, DisposableBean还可以实现 InitializingBean, DisposableBean 这两个接口，也是在初始化以及销毁阶段调用。实例化和属性赋值都是Spring帮助我们做的，能够自己实现的就只有 初始化 和 销毁 两个生命周期阶段. InitializingBean对应生命周期的初始化阶段，在源码的 invokeInitMethods(beanName, wrappedBean, mbd) 方法中调用。 有一点需要注意，因为Aware方法都是执行在初始化方法之前，所以可以在初始化方法中放心大胆的使用Aware接口获取的资源，这也是我们自定义扩展Spring的常用方式。 除了实现InitializingBean接口之外还能通过注解或者xml配置的方式指定初始化方法，至于这几种定义方式的调用顺序其实没有必要记。因为这几个方法对应的都是同一个生命周期，只是实现方式不同，我们一般只采用其中一种方式。 DisposableBean类似于InitializingBean，对应生命周期的销毁阶段，以ConfigurableApplicationContext#close() 方法作为入口，实现是通过循环取所有实现了DisposableBean接口的Bean然后调用其`destroy() 方法; 自定义初始化和销毁方法也可以自定义方法用于在初始化、销毁阶段调用 实现 Aware 接口Aware类型的接口的作用就是让我们能够拿到Spring容器中的一些资源。基本都能够见名知意，Aware之前的名字就是可以拿到什么资源，例如BeanNameAware可以拿到BeanName，以此类推。调用时机需要注意：所有的Aware方法都是在初始化阶段之前调用的！ Aware接口具体可以分为两组。如下排列顺序同样也是Aware接口的执行顺序: Aware Group1 BeanNameAware BeanClassLoaderAware BeanFactoryAware Aware Group2 EnvironmentAware EmbeddedValueResolverAware 实现该接口能够获取Spring EL解析器，用户的自定义注解需要支持spel表达式的时候可以使用，非常方便。 ApplicationContextAware(ResourceLoaderAware、ApplicationEventPublisherAware、MessageSourceAware) 这几个接口可能让人有点懵，实际上这几个接口可以一起记，其返回值实质上都是当前的ApplicationContext对象，因为ApplicationContext是一个复合接口 12public interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver &#123;&#125; 这里涉及一道面试题，ApplicationContext 和 BeanFactory的区别，可以从ApplicationContext 继承的这几个接口入手，除去 BeanFactory 相关的两个接口就是ApplicationContext独有的功能. BeanPostProcessor, InstantiationAwareBeanPostProcessor接口增强处理器(容器级别)，实现 BeanPostProcessor 接口，Spring 中所有 bean 在做初始化时都会调用该接口中的两个方法，， 正因为如此，这些接口的功能非常强大，Spring内部扩展也经常使用这些接口，例如自动注入以及AOP的实现都和他们有关。 这是Spring扩展中最重要的两个接口！ InstantiationAwareBeanPostProcessor作用于实例化阶段的前后； BeanPostProcessor作用于初始化阶段的前后； InstantiationAwareBeanPostProcessor实际上继承了 BeanPostProcessor接口，严格意义上来看他们是两父子; Aware调用时机源码分析详情如下，忽略了部分无关代码。代码位置就是initializeBean方法详情，这也说明了Aware都是在初始化阶段之前调用的！ 12345678910111213141516171819// 见名知意，初始化阶段调用的方法protected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) &#123; // 这里调用的是Group1中的三个Bean开头的Aware invokeAwareMethods(beanName, bean); Object wrappedBean = bean; // 这里调用的是Group2中的几个Aware， // 而实质上这里就是前面所说的BeanPostProcessor的调用点！ // 也就是说与Group1中的Aware不同，这里是通过BeanPostProcessor（ApplicationContextAwareProcessor）实现的。 wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); // 下文即将介绍的InitializingBean调用点 invokeInitMethods(beanName, wrappedBean, mbd); // BeanPostProcessor的另一个调用点 wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); return wrappedBean;&#125; 可以看到并不是所有的Aware接口都使用同样的方式调用。 Bean××Aware都是在代码中直接调用的; ApplicationContext相关的Aware都是通过BeanPostProcessor#postProcessBeforeInitialization()实现的。 具体流程可以看一下 ApplicationContextAwareProcessor 这个类的源码，就是判断当前创建的Bean是否实现了相关的Aware方法，如果实现了会调用回调方法将资源传递给Bean。 至于Spring为什么这么实现，应该没什么特殊的考量。也许和Spring的版本升级有关。基于对修改关闭，对扩展开放的原则，Spring对一些新的Aware采用了扩展的方式添加。 BeanPostProcessor的调用时机也能在这里体现，包围住 invokeInitMethods 方法，也就说明了在初始化阶段的前后执行。 关于Aware接口的执行顺序，其实只需要记住第一组在第二组执行之前就行了. 实例DemoUser.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * @author Austin * @since 2019/12/2 21:01 Mon */public class User implements InitializingBean, DisposableBean, BeanNameAware, ApplicationContextAware, BeanClassLoaderAware, BeanFactoryAware, EnvironmentAware &#123; private ApplicationContext applicationContext; private String username; /** * 自定义方法用于在初始化 */ public void initMethod() &#123; System.out.println("调用Bean的函数(initMethod)"); &#125; @PostConstruct public void postConstructor() &#123; System.out.println("调用Bean的函数(postConstruct)"); &#125; public User() &#123; System.out.println("调用Bean的函数(constructor)"); &#125; public void setUsername(String username) &#123; System.out.println("调用Bean的函数(setName/setAttribute)"); this.username = username; &#125; @PreDestroy public void preDestroy() &#123; System.out.println("调用Bean的函数(preDestroy)"); &#125; @Override public void destroy() throws Exception &#123; System.out.println("调用Bean的函数(destroy())"); &#125; public void destroyMethod() &#123; System.out.println("调用Bean的函数(destroyMethod)"); &#125; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("调用Bean的函数(afterPropertiesSet)"); &#125; @Override public void setBeanName(String name) &#123; System.out.println("调用BeanNameAware的(setBeanName)函数"); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; System.out.println("调用ApplicationContextAware的(setApplicationContext)函数"); this.applicationContext = applicationContext; &#125; @Override public void setBeanClassLoader(ClassLoader classLoader) &#123; System.out.println("调用BeanClassLoaderAware的(setBeanClassLoader)函数"); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; System.out.println("调用BeanFactoryAware的(setBeanFactory)函数"); &#125; @Override public void setEnvironment(Environment environment) &#123; System.out.println("调用EnvironmentAware的(setEnvironment)函数"); &#125;&#125; CustomBeanPostProcessor.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * @author Austin * @since 2019/12/2 21:10 Mon */@Componentpublic class CustomBeanPostProcessor implements InstantiationAwareBeanPostProcessor, BeanPostProcessor &#123; /** * 实例化之前调用 */ @Override public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123; if (beanClass == User.class)&#123; System.out.println("调用InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation()函数"); &#125; return null; &#125; /** * 实例化之后调用 */ @Override public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException &#123; if (bean.getClass() == User.class)&#123; System.out.println("调用InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation()函数"); &#125; return true; &#125; /** * 预初始化，初始化之前调用 */ @Nullable @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (bean.getClass() == User.class) &#123; System.out.println("调用BeanPostProcessor的postProcessBeforeInitialization()函数"); &#125; return bean; &#125; /** * 后初始化 bean 初始化完成调用 */ @Nullable @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean.getClass() == User.class) &#123; System.out.println("调用BeanPostProcessor的postProcessAfterInitialization()函数"); &#125; return bean; &#125;&#125; BootStrap.java123456789101112131415161718/** * @author Austin * @since 2019/12/2 21:13 Mon */@SpringBootApplicationpublic class BootStrap &#123; public static void main(String[] args) &#123; SpringApplication.run(BootStrap.class, args); &#125; @Bean(initMethod = "initMethod", destroyMethod = "destroyMethod") public User user() &#123; User user = new User(); user.setUsername("xxxx"); return user; &#125;&#125; 运行结果： 12345678910111213141516171819202122232425调用InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation()函数调用Bean的函数(constructor)调用Bean的函数(setName/setAttribute)调用InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation()函数调用BeanNameAware的(setBeanName)函数调用BeanClassLoaderAware的(setBeanClassLoader)函数调用BeanFactoryAware的(setBeanFactory)函数调用EnvironmentAware的(setEnvironment)函数调用ApplicationContextAware的(setApplicationContext)函数调用BeanPostProcessor的postProcessBeforeInitialization()函数调用Bean的函数(postConstruct)调用Bean的函数(afterPropertiesSet)调用Bean的函数(initMethod)调用BeanPostProcessor的postProcessAfterInitialization()函数调用Bean的函数(preDestroy)调用Bean的函数(destroy())调用Bean的函数(destroyMethod) 总结Spring Bean的生命周期分为四个阶段和多个扩展点。扩展点又可以分为影响多个Bean和影响单个Bean。整理如下： 四个阶段 实例化 Instantiation 属性赋值 Populate 初始化 Initialization 销毁 Destruction 多个扩展点 影响多个Bean BeanPostProcessor InstantiationAwareBeanPostProcessor 影响单个Bean Aware Aware Group1 BeanNameAware BeanClassLoaderAware BeanFactoryAware Aware Group2 EnvironmentAware EmbeddedValueResolverAware ApplicationContextAware (ResourceLoaderAware、ApplicationEventPublisherAware、MessageSourceAware) 生命周期 InitializingBean DisposableBean 参考请别再问Spring Bean的生命周期了！Spring Bean 生命周期]]></content>
      <categories>
        <category>Spring</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>Bean</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis核心组件及常见问题总结]]></title>
    <url>%2F2019%2F12%2F27%2FMybatis%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[mybatis核心组件mybatis-config.xmlXML 配置文件（configuration XML）中包含了对 MyBatis 系统的核心设置，包含获取数据库连接实例的数据源（DataSource）和决定事务作用域和控制方式的事务管理器（TransactionManager） SqlSessionFactoryBuilder 作用SqlSessionFactoryBuilder通过类名就可以看出这个类的主要作用就是创建一个SqlSessionFactory，通过输入mybatis配置文件的字节流或者字符流，生成XMLConfigBuilder，XMLConfigBuilder创建一个Configuration，Configuration这个类中包含了mybatis的配置的一切信息，mybatis进行的所有操作都需要根据Configuration中的信息来进行。 作用域（Scope）和生命周期可以重用 SqlSessionFactoryBuilder 来创建多个 SqlSessionFactory 实例，但是最好还是不要让其一直存在 ,以保证所有的 XML 解析资源开放给更重要的事情, 这个类可以被实例化、使用和丢弃，一旦创建了 SqlSessionFactory，就不再需要它了。因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量） SqlSessionFactory接口 概念sql会话工厂，用于创建SqlSession 作用域（Scope）和生命周期SqlSessionFactory 一旦被创建就应该在应用的运行期间一直存在，没有任何理由对它进行清除或重建，最佳作用域是应用作用域。有很多方法可以做到，最简单的就是使用单例模式或者静态单例模式。 如何创建使用xml构建 123String resource = "org/mybatis/example/mybatis-config.xml";InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); java代码构建 123456DataSource dataSource = BlogDataSourceFactory.getBlogDataSource();TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment("development", transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.addMapper(BlogMapper.class);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); SqlSession接口 概念SqlSession是MyBatis的一个重要接口，定义了数据库的增删改查以及事务管理的常用方法。SqlSession还提供了查找Mapper接口的有关方法。 作用域（Scope）和生命周期每个线程都应该有它自己的 SqlSession 实例, SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域，每次收到的 HTTP请求，就可以打开一个 SqlSession，返回一个响应，就关闭它。 如何创建 123456SqlSession session = sqlSessionFactory.openSession();try &#123; // do work&#125; finally &#123; session.close();&#125; Mapper接口 概念承载了实际的业务逻辑，其生命周期比较短，由SqlSession创建,用于将Java对象和实际的SQL语句对应起来。Mapper接口是指程序员自行定义的一个数据操纵接口，类似于通常所说的DAO接口。跟DAO不同的地方在于Mapper接口只需要程序员定义，不需要程序员去实现，MyBatis会自动为Mapper接口创建动态代理对象。Mapper接口的方法通常与Mapper配置文件中的select、insert、update、delete等XML结点存在一一对应关系。 实现方式 (1)使用XML配置文件的方式。 (2)使用注解方式。 (3)直接使用MyBatis提供的API。 mybatis的优缺点优点 易于上手和掌握。 sql写在xml里，便于统一管理和优化。 解除sql与程序代码的耦合。 提供映射标签，支持对象与数据库的orm字段关系映射 提供对象关系映射标签，支持对象关系组建维护 提供xml标签，支持编写动态sql。 缺点 sql工作量很大，尤其是字段多、关联表多时，更是如此。 sql依赖于数据库，导致数据库移植性差。 由于xml里标签id必须唯一，导致DAO中方法不支持方法重载。 字段映射标签和对象关系映射标签仅仅是对映射关系的描述，具体实现仍然依赖于sql。（比如配置了一对多Collection标签，如果sql里没有join子表或查询子表的话，查询后返回的对象是不具备对象关系的，即Collection的对象为null） DAO层过于简单，对象组装的工作量较大。 不支持级联更新、级联删除。 编写动态sql时,不方便调试，尤其逻辑复杂时。 提供的写动态sql的xml标签功能简单（连struts都比不上），编写动态sql仍然受限，且可读性低。 若不查询主键字段，容易造成查询出的对象有“覆盖”现象。 参数的数据类型支持不完善。（如参数为Date类型时，容易报没有get、set方法，需在参数上加@param） 多参数时，使用不方便，功能不够强大。（目前支持的方法有map、对象、注解@param以及默认采用012索引位的方式） 缓存使用不当，容易产生脏数据。 Mybatis常见问题传统 JDBC 的弊端 1、jdbc 底层没有用连接池、操作数据库需要频繁的创建和关联链接。消耗很大的资源 2、写原生的 jdbc 代码在 java 中，一旦我们要修改 sql 的话，java 需要整体编译，不利于系 统维护 3、使用 PreparedStatement 预编译的话对变量进行设置 123 数字，这样的序号不利于维护 4、返回 result 结果集也需要硬编码。 什么是Mybatis？ 1、Mybatis是一个半ORM（对象关系映射）框架，它内部封装了JDBC，开发时只需要关注SQL语句本身，不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程。程序员直接编写原生态sql，可以严格控制sql执行性能，灵活度高。 2、MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 3、通过xml 文件或注解的方式将要执行的各种 statement 配置起来，并通过java对象和 statement中sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程）。 Mybatis执行流程 mybatis核心概念 名称 意义 Configuration 管理 mysql-config.xml全局配置关系类 SqlSessionFactory Session 管理工厂接口 Session SqlSession是一个面向用户(程序员)的接口。SqlSession 中提 供了很多操作数据库的方法 Executor 执行器是一个接口(基本执行器、缓存执行器) 作用:SqlSession 内部通过执行器操作数据库 MappedStatement 底层封装对象 作用:对操作数据库存储封装，包括 sql 语句、输入输出参数 StatementHandler 具体操作数据库相关的 handler 接口 ResultSetHandler 具体操作数据库返回结果的 handler 接口 Mybatis 全局配置详解 MyBatis与Hibernate有哪些不同？1、Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。 2、Mybatis直接编写原生态sql，可以严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。 3、Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件，如果用hibernate开发可以节省很多代码，提高效率。 #{} 和 ${} 的区别是什么？ #{} 是预编译处理，Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值； ${}是字符串替换，Mybatis在处理${}时，就是把${}替换成变量的值原样拼接在SQL中； 使用#{}可以有效的防止SQL注入，提高系统安全性 Dao接口的工作原理是什么？Dao接口里的方法能重载吗？Dao接口即Mapper接口。接口的全限名，就是mapper映射文件中的namespace的值； 接口的方法名，就是映射文件中Mapper的Statement的id值；接口方法内的参数，就是传递给sql的参数。 Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MapperStatement。 在Mybatis中，每一个 &lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt; 标签，都会被解析为一个MapperStatement对象。举例： com.mybatis3.mappers.StudentDao.findStudentById ，可以唯一找到namespace为 com.mybatis3.mappers.StudentDao 下面 id 为 findStudentById 的 MapperStatement。 Mapper接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻找策略。 Mapper接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Mapper接口生成代理对象proxy，代理对象会拦截接口方法，转而执行MapperStatement所代表的sql，然后将sql执行结果返回。 Mybatis是如何进行分页的？分页插件的原理是什么？Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页。可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数. Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。 在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。 它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。 当然了，不光是Mybatis，几乎所有的包括Hibernate，支持延迟加载的原理都是一样的 Mybatis的一级、二级缓存1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。 2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态), 可在它的映射文件中配置 &lt;cache/&gt; ； 3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。 Mybatis都有哪些Executor执行器？Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。 ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用Statement对象。 BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。 简述Mybatis的插件运行原理，以及如何编写一个插件。Mybatis仅可以编写针对： ParameterHandler ResultSetHandler StatementHandler Executor 这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 编写插件： ① 实现Mybatis的Interceptor接口并覆写intercept()方法； ② 给插件编写注解，指定要拦截哪一个接口的哪些方法即可； ③ 在配置文件中配置你编写的插件。 如何获取自动生成的(主)键值一般插入数据的话，如果我们想要知道刚刚插入的数据的主键是多少，我们可以通过以下的方式来获取 需求： user对象插入到数据库后，新记录的主键要通过user对象返回，通过user获取主键值。 解决思路： 通过LAST_INSERT_ID()获取刚插入记录的自增主键值，在insert语句执行后，执行select LAST_INSERT_ID() 就可以获取自增主键。 mysql: 123456&lt;insert id="insertUser" parameterType="cn.itcast.mybatis.po.User"&gt; &lt;selectKey keyProperty="id" order="AFTER" resultType="int"&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; INSERT INTO USER(username,birthday,sex,address) VALUES(#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;)&lt;/insert&gt; Mybatis动态sql Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能; Mybatis提供了9种动态sql标签：trim|where|set|foreach|if|choose|when|otherwise|bind。 其执行原理为，使用OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能; Mybatis比IBatis比较大的几个改进 a.增加接口绑定,包括注解绑定sql和xml绑定Sql , b.动态sql由原来的节点配置变成OGNL表达式, c. 在一对一,一对多的时候引进了association, 在一对多的时候引入了collection节点, 不过都是在resultMap里面配置 参考面试官都会问的Mybatis面试题 Mybatis常见面试题]]></content>
      <categories>
        <category>数据库中间件</category>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式ID生成器]]></title>
    <url>%2F2019%2F12%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[基本要求 全局唯一 趋势递增 效率高（生成、使用、索引） 控制并发 常用策略 Twitter雪花算法SnowFlake 1) 1位，不用。二进制中最高位为1的都是负数，但是我们生成的id一般都使用整数，所以这个最高位固定是0 2) 41位，用来记录时间戳（毫秒）。 3) 41位可以表示2^41−1个数字，如果只用来表示正整数（计算机中正数包含0），可以表示的数值范围是：0 至 2^41−1，减1是因为可表示的数值范围是从0开始算的，而不是1。也就是说41位可以表示2^41−1个毫秒的值，转化成单位年则是(2^41−1)/(1000∗60∗60∗24∗365)=69年 4) 10位，用来记录工作机器id。可以部署在2^10=1024个节点，包括5位datacenterId和5位workerId 5) 5位（bit）可以表示的最大正整数是2^5−1=31，即可以用0、1、2、3、….31这32个数字，来表示不同的datecenterId或workerId 6) 12位，序列号，用来记录同毫秒内产生的不同id。12位（bit）可以表示的最大正整数是2^12−1=4095，即可以用0、1、2、3、….4094这4095个数字，来表示同一机器同一时间截（毫秒)内产生的4095个ID序号由于在Java中64bit的整数是long类型，所以在Java中SnowFlake算法生成的id就是long来存储的。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成mybatis自定义插件开发]]></title>
    <url>%2F2019%2F12%2F20%2FSpringboot%E9%9B%86%E6%88%90mybatis%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[mybatis架构 mybatis工作原理 执行流程： 读取核心配置文件并返回InputStream流对象。 根据InputStream流对象解析出Configuration对象，然后创建SqlSessionFactory工厂对象 根据一系列属性从SqlSessionFactory工厂中创建SqlSession 从SqlSession中调用Executor执行数据库操作&amp;&amp;生成具体SQL指令 对执行结果进行二次封装 提交与事务 mybatis插件简介mybatis插件就是在执行数据库操作的时候，对于特定方法进行拦截增强，做一些额外的处理的一种方式。myabtis的插件的增强原理是利用动态代理实现的，可以对数据库操作的执行类做拦截，mybatis主要操作流程如下： mybatis中的几个操作数据库的执行类是：Executor、StatementHandler、ParameterHandler、ResultSetHandler，其中： Executor 是总的执行者，他就像一个大总管，用于协调管理其他执行者。 StatementHandler 拦截Sql语法构建的处理, 是用于生成Statement或者PreparedStatement的执行者，同时他会调用ParameterHandler进行对sql语句中的参数设值，设置完了之后会通过StatementHandler 去调用sql在数据库中执行，最后返回一个结果集，通过ResultSetHandler将结果集和对应的实体进行映射填充数据，之后会把结果实体返回给StatementHandler。 ParameterHandler ：拦截参数的处理 ResultSetHandler ：拦截结果集的处理 所以，我们对这几个执行者进行拦截，比如对于StatementHandler 拦截，即是对于sql操作进行拦截，Mybatis自定义插件必须实现Interceptor接口： 12345public interface Interceptor &#123; Object intercept(Invocation invocation) throws Throwable; Object plugin(Object target); void setProperties(Properties properties);&#125; intercept方法：拦截器具体处理逻辑方法 plugin方法：根据签名signatureMap生成动态代理对象 setProperties方法：设置Properties属性 下面就对于这个StatementHandler 进行拦截做一个分页实例。 分页插件需求拦截数据sql,实现分页功能 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * @author Austin * @since 2019/9/1 11:37 Sun */@Component@Intercepts(@Signature( type = StatementHandler.class, method = "prepare", args = &#123;Connection.class, Integer.class&#125;))public class PagePlugin implements Interceptor &#123; @Value("$&#123;mybatis.page.plugin.dialect&#125;") private String dialect; @Value("$&#123;mybatis.page.plugin.pageSqlId&#125;") private String pageSqlId; // 插件需要做的事情 @Override public Object intercept(Invocation invocation) throws Throwable &#123; // 确定哪些方法需要做分页 StatementHandler statementHandler = (StatementHandler) invocation.getTarget(); // 获取原始sql BoundSql boundSql = statementHandler.getBoundSql(); String sql = boundSql.getSql(); System.out.println("原始sql: " + sql); MetaObject metaObject = MetaObject.forObject(statementHandler, SystemMetaObject.DEFAULT_OBJECT_FACTORY, SystemMetaObject.DEFAULT_OBJECT_WRAPPER_FACTORY, new DefaultReflectorFactory()); MappedStatement mappedStatement = (MappedStatement) metaObject.getValue("delegate.mappedStatement"); // //sql语句类型 select、delete、insert、update // String sqlCommandType = mappedStatement.getSqlCommandType().toString(); // 获取mapper接口中的方法名 String mapperMethodName = mappedStatement.getId(); Object paramObj = boundSql.getParameterObject(); if (mapperMethodName.matches(".*ByPage$")) &#123; Map&lt;String, Object&gt; params = (Map&lt;String, Object&gt;) paramObj; PageInfo pageInfo = (PageInfo) params.get("page"); // map.put("page", pageInfo) String countSql = "select count(1) from (" + sql + ") temp "; System.out.println("查询总数sql: " + countSql); Connection connection = (Connection) invocation.getArgs()[0]; PreparedStatement countStatement = connection.prepareStatement(countSql); ParameterHandler parameterHandler = (ParameterHandler) metaObject.getValue("delegate.parameterHandler"); parameterHandler.setParameters(countStatement); ResultSet rs = countStatement.executeQuery(); if (rs.next()) &#123; pageInfo.setTotalNumber(rs.getInt(1)); &#125; rs.close(); countStatement.close(); // 改造sql limit count String pageSql = this.generatePageSql(sql, pageInfo); System.out.println("分页sql: " + pageSql); // 改造后的sql放回 metaObject.setValue("delegate.boundSql.sql", pageSql); &#125; // 执行流程提交mybatis return invocation.proceed(); &#125; public String generatePageSql(String sql, PageInfo pageInfo) &#123; StringBuffer sb = new StringBuffer(); if (dialect.equals("mysql")) &#123; sb.append(sql); sb.append(" limit " + pageInfo.getStartIndex() + " ," + pageInfo.getTotalSelect()); &#125; return sb.toString(); &#125; @Override public Object plugin(Object target) &#123; return Plugin.wrap(target, this); &#125; @Override public void setProperties(Properties properties) &#123; &#125;&#125; myabtis自定义插件只需要实现Interceptor接口即可，并且注解@Intercepts以及@Signature配置需要拦截的对象，其中 type是需要拦截的对象Class， method是对象里面的方法， args是方法参数类型。 注入插件到拦截链这里有两种方式注入 方式一：直接注入 1Spring boot项目中只需要在拦截器类上加 @Component 注解即可。 方式二：通过myabtis配置加入到拦截链中(多个拦截器时，这种方式可以控制拦截顺序） 1234567891011121314151617181920212223@Configuration@MapperScan(&#123;"com.springboot.demo.mapper"&#125;)public class MapperConfig &#123; //将插件加入到mybatis插件拦截链中 @Bean public ConfigurationCustomizer configurationCustomizer() &#123; return new ConfigurationCustomizer() &#123; @Override public void customize(Configuration configuration) &#123; //插件拦截链采用了责任链模式，执行顺序和加入连接链的顺序有关 MyPlugin myPlugin = new MyPlugin(); //设置参数，比如阈值等，可以在配置文件中配置，这里直接写死便于测试 Properties properties = new Properties(); //这里设置慢查询阈值为1毫秒，便于测试 properties.setProperty("time", "1"); myPlugin.setProperties(properties); configuration.addInterceptor(myPlugin); &#125; &#125;; &#125;&#125;]]></content>
      <categories>
        <category>数据库中间件</category>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java编译执行有第三方依赖的类]]></title>
    <url>%2F2019%2F12%2F20%2Fjava%E7%BC%96%E8%AF%91%E6%89%A7%E8%A1%8C%E6%9C%89%E7%AC%AC%E4%B8%89%E6%96%B9%E4%BE%9D%E8%B5%96%E7%9A%84%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[有时候在进行开发的过程中，需要自己写个测试类来进行某个局部功能的测试，在测试的过程中，需要引入第三方jar包或者公司其他成员的帮助类，比如说：我需要测试一个net.sf.json.JSONObject解析数据的时候中文乱码问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import net.sf.json.JSONException;import net.sf.json.JSONObject;import net.sf.json.JsonConfig; import java.io.*;import java.net.URLEncoder; /** * Created by 神器 on 2017/3/29. */public class ZyhqErrorMsgDemo &#123; public static void main(String[] args) &#123; InputStream inputStream = null; InputStreamReader inputStreamReader = null; BufferedReader in = null; try&#123; String filePath = "/home/1.txt"; File file = new File(filePath); String code = codeString(filePath); System.out.println("文件类型:" + code); inputStream = new FileInputStream(file); inputStreamReader = new InputStreamReader(inputStream, "UTF-8"); in = new BufferedReader(inputStreamReader); String firstLine = in.readLine();//过滤掉首行，以便循环体从第二行（申赎明细处理结果） String line = ""; while ((line = in.readLine()) != null) &#123; System.out.println("line:=====&gt;"+ line); System.out.println(); System.out.println(); JSONObject jsonObject = JSONObject.fromObject(line); String status = jsonObject.getString("status"); String objectStr = jsonObject.getString("content"); String errorMessage = null; try&#123; errorMessage = jsonObject.get("errorMessage").toString(); &#125;catch (JSONException ex) &#123; System.out.println("there is no errorMessage from Zyhq holding respon file"); &#125; System.out.println("errorMessage:------&gt;" + errorMessage); &#125; in.close(); inputStreamReader.close(); inputStream.close(); &#125;catch (Exception ex)&#123; System.out.println("下载文件并解析文件内容时出错"+ex.getStackTrace()); &#125;finally &#123; if (in != null)&#123; try &#123; in.close(); &#125; catch (IOException e) &#123; &#125; &#125; if (inputStreamReader != null)&#123; try &#123; inputStreamReader.close(); &#125; catch (IOException e) &#123; &#125; &#125; if (inputStream != null)&#123; try &#123; inputStream.close(); &#125; catch (IOException e) &#123; &#125; &#125; &#125; &#125; /** * 判断文件的编码格式 * @param fileName :file * @return 文件编码格式 * @throws Exception */ private static String codeString(String fileName) throws Exception&#123; BufferedInputStream bin = new BufferedInputStream( new FileInputStream(fileName)); int p = (bin.read() &lt;&lt; 8) + bin.read(); String code = null; switch (p) &#123; case 0xefbb: code = "UTF-8"; break; case 0xfffe: code = "Unicode"; break; case 0xfeff: code = "UTF-16BE"; break; default: code = "GBK"; &#125; return code; &#125;&#125; 写好之后上传到服务器上，需要自己在java文件目录上传好依赖的第三方jar包，然后编译： 1javac -cp commons-beanutils-1.8.3.jar:commons-collections-3.2.1.jar:commons-lang-2.6.jar:commons-logging-1.2.jar:json-lib-2.4-jdk15.jar: ZyhqErrorMsgDemo.java 切记：最后一个jar包后的：后面要加上一个空格再引入自己的Java文件 执行的命令: 1javac -cp commons-beanutils-1.8.3.jar:commons-collections-3.2.1.jar:commons-lang-2.6.jar:commons-logging-1.2.jar:json-lib-2.4-jdk15.jar: ZyhqErrorMsgDemo]]></content>
      <categories>
        <category>教程</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Guice使用入门]]></title>
    <url>%2F2019%2F12%2F20%2FGuice%E4%BD%BF%E7%94%A8%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Guice是谷歌推出的一个轻量级依赖注入框架，帮助我们解决Java项目中的依赖注入问题。如果使用过Spring的话，会了解到依赖注入是个非常方便的功能。不过假如只想在项目中使用依赖注入，那么引入Spring未免大材小用了。这时候我们可以考虑使用Guice。本文参考了Guice官方文档，详细信息可以直接查看Guice文档。 基本使用如果使用Maven的话，添加下面的依赖项。 12345&lt;dependency&gt; &lt;groupId&gt;com.google.inject&lt;/groupId&gt; &lt;artifactId&gt;guice&lt;/artifactId&gt; &lt;version&gt;4.2.2&lt;/version&gt;&lt;/dependency&gt; 快速开始Guice的注入非常方便，不需要配置文件。 12345678910111213141516171819202122232425262728// 被依赖的dao@Singleton // 打上了这个标记说明是单例的，否则Guice每次回返回一个新的对象public class UserDao&#123; public void say()&#123; System.out.println("dao is saying"); &#125;&#125;// service，依赖 UserDaopublic class UserService &#123; @Inject private UserDao mUserDao; public void say() &#123; return mUserDao.say(); &#125;&#125;// 启动类public class Start &#123; public static void main(final String[] args) &#123; //这步就是我们问Guice去要对象 final Injector injector = Guice.createInjector(); final UserService userService = injector.getInstance(UserService.class); userService.say(); &#125;&#125; 结果输出： dao is saying复制代码可以看到没有任何的xml配置，唯一需要做的，就是在需要注入的属性上打上@inject。使用 Guice.createInjector() 启动。通常需要尽早在程序中创建注入器。这样 Guice 能够帮助您创建大部分对象. 该demo中，并没有用到Module，也成功运行了，是因为之前没有涉及到接口，当只是依赖 确切的实现类 的时候，Guice会自动的找到需要注入的实现类 依赖绑定链式绑定我们在绑定依赖的时候不仅可以将父类和子类绑定，还可以将子类和更具体的子类绑定。下面的例子中，当我们需要TransactionLog的时候，Guice最后会为我们注入MySqlDatabaseTransactionLog对象。 1234567public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(TransactionLog.class).to(DatabaseTransactionLog.class); bind(DatabaseTransactionLog.class).to(MySqlDatabaseTransactionLog.class); &#125;&#125; 注解绑定当我们需要将多个同一类型的对象注入不同对象的时候，就需要使用注解区分这些依赖了。最简单的办法就是使用@Named注解进行区分。 首先需要在要注入的地方添加@Named注解。 1234567public class RealBillingService implements BillingService &#123; @Inject public RealBillingService(@Named("Checkout") CreditCardProcessor processor, TransactionLog transactionLog) &#123; ... &#125; 然后在绑定中添加annotatedWith方法指定@Named中指定的名称。由于编译器无法检查字符串，所以Guice官方建议我们保守地使用这种方式。 123bind(CreditCardProcessor.class) .annotatedWith(Names.named("Checkout")) .to(CheckoutCreditCardProcessor.class); 如果希望使用类型安全的方式，可以自定义注解。 12345@BindingAnnotation @Target(&#123; FIELD, PARAMETER, METHOD &#125;) @Retention(RUNTIME)public @interface PayPal &#123;&#125; 然后在需要注入的类上应用。 1234567public class RealBillingService implements BillingService &#123; @Inject public RealBillingService(@PayPal CreditCardProcessor processor, TransactionLog transactionLog) &#123; ... &#125; 在配置类中，使用方法也和@Named类似。 123bind(CreditCardProcessor.class) .annotatedWith(PayPal.class) .to(PayPalCreditCardProcessor.class); 实例绑定有时候需要直接注入一个对象的实例，而不是从依赖关系中解析。如果我们要注入基本类型的话只能这么做。 123456789101112131415161718192021222324252627282930bind(String.class) .annotatedWith(Names.named("JDBC URL")) .toInstance("jdbc:mysql://localhost/pizza"); bind(Integer.class) .annotatedWith(Names.named("login timeout seconds")) .toInstance(10);``` 如果使用`toInstance()`方法注入的实例比较复杂的话，可能会影响程序启动。这时候可以使用`@Provides`方法代替。### @Provides方法当一个对象很复杂，无法使用简单的构造器来生成的时候，我们可以使用`@Provides`方法，也就是在配置类中生成一个注解了`@Provides`的方法。在该方法中我们可以编写任意代码来构造对象。```javapublic class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; ... &#125; @Provides TransactionLog provideTransactionLog() &#123; DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setJdbcUrl("jdbc:mysql://localhost/pizza"); transactionLog.setThreadPoolSize(30); return transactionLog; &#125;&#125; @Provides方法也可以应用@Named和自定义注解，还可以注入其他依赖，Guice会在调用方法之前注入需要的对象。 123456@Provides @PayPalCreditCardProcessor providePayPalCreditCardProcessor(@Named("PayPal API key") String apiKey) &#123; PayPalCreditCardProcessor processor = new PayPalCreditCardProcessor(); processor.setApiKey(apiKey); return processor;&#125; Provider绑定如果项目中存在多个比较复杂的对象需要构建，使用@Provides方法会让配置类变得比较乱。我们可以使用Guice提供的Provider接口将复杂的代码放到单独的类中。办法很简单，实现Provider&lt;T&gt;接口的get方法即可。在Provider类中，我们可以使用@Inject任意注入对象。 1234567891011121314public class DatabaseTransactionLogProvider implements Provider&lt;TransactionLog&gt; &#123; private final Connection connection; @Inject public DatabaseTransactionLogProvider(Connection connection) &#123; this.connection = connection; &#125; public TransactionLog get() &#123; DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setConnection(connection); return transactionLog; &#125;&#125; 在配置类中使用toProvider方法绑定到Provider上即可。 123456public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; bind(TransactionLog.class) .toProvider(DatabaseTransactionLogProvider.class); &#125; 作用域默认情况下Guice会在每次注入的时候创建一个新对象。如果希望创建一个单例依赖的话，可以在实现类上应用@Singleton注解。 1234@Singletonpublic class InMemoryTransactionLog implements TransactionLog &#123; /* everything here should be threadsafe! */&#125; 或者也可以在配置类中指定。 123bind(TransactionLog.class) .to(InMemoryTransactionLog.class) .in(Singleton.class); 在@Provides方法中也可以指定单例。 12345@Provides @SingletonTransactionLog provideTransactionLog() &#123; ...&#125; 如果一个类型上存在多个冲突的作用域，Guice会使用bind()方法中指定的作用域。如果不想使用注解的作用域，可以在bind()方法中将对象绑定为Scopes.NO_SCOPE。 Guice和它的扩展提供了很多作用域，有单例Singleton，Session作用域SessionScoped，Request请求作用域RequestScoped等等。我们可以根据需要选择合适的作用域。 参考：Google-Guice入门教程 Guice 快速入门]]></content>
      <categories>
        <category>教程</category>
        <category>Guice</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Guice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka进阶-基本原理]]></title>
    <url>%2F2019%2F12%2F20%2FKafka%E8%BF%9B%E9%98%B6-%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[集群架构图 使用场景 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、Hbase、Solr等。 消息系统：解耦和生产者和消费者、缓存消息等。 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。 流式处理：比如spark streaming和storm 事件源 如何做到高吞吐、低延迟Kafka写数据的大致方式：先写操作系统的页缓存（Page Cache）,然后由操作系统自行决定何时刷到磁盘。 因此 Kafka 达到高吞吐、低延迟的原因主要有以下4点： 页缓存是在内存中分配的，所以消息写入的速度很快。 Kafka不必和底层的文件系统进行交互，所有繁琐的I/O操作都由操作系统来处理。 Kafka采用追加写的方式，避免了磁盘随机写操作。 使用以Sendfile为代表的零拷贝技术提高了读取数据的效率。 PS: 使用页缓存而非堆内存还有一个好处，就是当Kafka broker的进程崩溃时，堆内存的数据会丢失，但是页缓存的数据依然存在，重启Kafka broker后可以继续提供服务。 Producer工作流程 序列化消息 &amp;&amp; 计算partition根据key和value的配置对消息进行序列化,然后计算partition： ProducerRecord对象中如果指定了partition，就使用这个partition; 否则根据key和topic的partition数目取余; 如果key也没有的话就随机生成一个counter，使用这个counter来和partition数目取余。这个counter每次使用的时候递增。 发送到batch &amp;&amp; 唤醒Sender线程根据topic-partition获取对应的batchs（Dueue&lt;ProducerBatch&gt;），然后将消息append到batch中. 如果有batch满了则唤醒Sender线程。队列的操作是加锁执行，所以batch内消息是有序的，后续的Sender操作为异步操作。 Sender把消息有序发到broker（tp replia leader）确定tp relica leader 所在的broker Kafka中每台broker都保存了kafka集群的metadata信息，metadata信息里包括了每个topic的所有partition的信息: leader, leader_epoch, controller_epoch, isr, replicas等; Kafka客户端从任一broker都可以获取到需要的metadata信息; sender线程通过metadata信息可以知道tp leader的brokerId producer也保存了metada信息，同时根据metadata更新策略（定期更新metadata.max.age.ms、失效检测，强制更新)：检查到metadata失效以后，调用metadata.requestUpdate()强制更新 幂等性发送为实现Producer的幂等性，Kafka引入了Producer ID（即PID）和Sequence Number。对于每个PID，该Producer发送消息的每个&lt;Topic, Partition&gt;都对应一个单调递增的Sequence Number。同样，Broker端也会为每个&lt;PID, Topic, Partition&gt;维护一个序号，并且每Commit一条消息时将其对应序号递增。对于接收的每条消息，如果其序号比Broker维护的序号）大一，则Broker会接受它，否则将其丢弃： 如果消息序号比Broker维护的序号差值比一大，说明中间有数据尚未写入，即乱序，此时Broker拒绝该消息，Producer抛出InvalidSequenceNumber 如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer抛出DuplicateSequenceNumber Sender处理broker发来的produce response一旦broker处理完Sender的produce请求，就会发送produce response给Sender，此时producer将执行我们为send()设置的回调函数。至此producer的send执行完毕。 Consumer工作流程Poll消息 消费者通过fetch线程拉消息（单线程） 消费者通过心跳线程来与broker发送心跳。超时会认为挂掉 每个consumer group在broker上都有一个coordnator来管理，消费者加入和退出，以及消费消息的位移都由coordnator处理。 位移管理consumer的消息位移代表了当前group对topic-partition的消费进度，consumer宕机重启后可以继续从该offset开始消费。 在kafka0.8之前，位移信息存放在zookeeper上，由于zookeeper不适合高并发的读写，新版本Kafka把位移信息当成消息，发往 __consumers_offsets 这个 topic 所在的 broker，__consumers_offsets 默认有50个分区。消息的key 是 groupId+topic_partition, value 是offset. Kafka Group状态 Empty：初始状态，Group 没有任何成员，如果所有的 offsets 都过期的话就会变成 Dead PreparingRebalance：Group 正在准备进行 Rebalance AwaitingSync：Group 正在等待 group leader 的分配方案 Stable：稳定的状态（Group is stable）； Dead： Group 内已经没有成员，并且它的 Metadata 已经被移除 重平衡Reblance当一些原因导致consumer对partition消费不再均匀时，kafka 会自动执行rebalance，使得consumer对partition的消费再次平衡。 什么时候发生rebalance？： 组订阅topic数变更 topic partition数变更 consumer成员变更 Reblance过程 举例1 consumer被检测为崩溃引起的rebalance比如心跳线程在 timeout 时间内没和 broker 发送心跳，此时 coordinator 认为该group应该进行rebalance。接下来其他consumer发来fetch请求后，coordinator将回复他们进行rebalance通知。当consumer成员收到请求后，只有leader会根据分配策略进行分配，然后把各自的分配结果返回给coordinator。 这个时候只有consumer leader返回的是实质数据，其他返回的都为空。收到分配方法后，coordinator将会把分配策略同步给各consumer. 举例2 consumer加入引起的rebalance 使用 join 协议，表示有consumer 要加入到group中 使用 sync 协议，根据分配规则进行分配 Rebalance机制存在的问题在大型系统中，一个topic可能对应数百个consumer实例。 这些consumer陆续加入到一个空消费组将导致多次的rebalance； 此外consumer 实例启动的时间不可控，很有可能超出coordinator确定的rebalance timeout(即max.poll.interval.ms)，将会再次触发rebalance，而每次rebalance的代价又相当地大，因为很多状态都需要在rebalance前被持久化，而在rebalance后被重新初始化 新版本改进通过延迟进入 PreparingRebalance 状态减少 rebalance 次数 新版本新增了 group.initial.rebalance.delay.ms 参数。空消费组接受到成员加入请求时，不立即转化到 PreparingRebalance 状态来开启rebalance。当时间超过group.initial.rebalance.delay.ms后，再把group状态改为PreparingRebalance（开启rebalance）。 实现机制是在coordinator底层新增一个group状态：InitialRebalance。假设此时有多个consumer陆续启动，那么group状态先转化为 InitialRebalance，待group.initial.rebalance.delay.ms 时间后，再转换为PreparingRebalance（开启rebalance）. 消息传输一致Kafka提供3种消息传输一致性语义：最多1次，最少1次，恰好1次。 at most once: 消费者fetch消息, 然后保存offset,然后处理消息; 当client保存offset之后,但是在消息处理过程中consumer进程失效(crash), 导致部分消息未能继续处理.那么此后可能其他consumer会接管,但是因为offset已经提前保存,那么新的consumer将不能fetch到offset之前的消息(尽管它们尚没有被处理), 这就是”at most once”.** 可能会出现数据丢失情况;** at least once: 消费者fetch消息, 然后处理消息, 然后保存offset. 如果消息处理成功之后, 但是在保存offset阶段zookeeper异常或者consumer失效,导致保存offset操作未能执行成功, 这就导致接下来再次fetch时可能获得上次已经处理过的消息,这就是”at least once”.可能会重传数据，有可能出现数据被重复处理的情况; exactly once：并不是指真正只传输1次，只不过有一个机制。确保不会出现“数据被重复处理”和“数据丢失”的情况。消费者的场景中可以采取以下方案来得到“恰好1次”的一致性语义： 最少1次 ＋ 消费者的输出中额外增加已处理消息最大编号：由于已处理消息最大编号的存在，不会出现重复处理消息的情况 Broker设计原理Broker 是Kafka 集群中的节点。负责处理生产者发送过来的消息，消费者消费的请求。以及集群节点的管理等。 broker消息存储 Kafka的消息以二进制的方式紧凑地存储，节省了很大空间 此外消息存在 ByteBuffer 而不是堆，这样broker进程挂掉时，数据不会丢失，同时避免了gc问题 通过零拷贝和顺序寻址，让消息存储和读取速度都非常快 处理fetch请求的时候通过 zero-copy 加快速度 broker状态数据broker设计中，每台机器都保存了相同的状态数据。主要包括以下： Controller所在的broker ID，即保存了当前集群中controller是哪台broker; 集群中所有broker的信息：比如每台broker的ID、机架信息以及配置的若干组连接信息 集群中所有节点的信息：严格来说，它和上一个有些重复，不过此项是按照broker ID和监听器类型进行分组的。 对于超大集群来说，使用这一项缓存可以快速地定位和查找给定节点信息，而无需遍历上一项中的内容，算是一个优化吧 集群中所有分区的信息：所谓分区信息指的是分区的leader、ISR和AR信息以及当前处于offline状态的副本集合。 这部分数据按照 topic-partitionID 进行分组，可以快速地查找到每个分区的当前状态。（注：AR表示assigned replicas，即创建topic时为该分区分配的副本集合） broker负载均衡 分区数量负载：各台broker的partition数量应该均匀partition Replica分配算法如下： 将所有Broker（假设共n个Broker）和待分配的Partition排序; 将第i个Partition分配到第（i mod n）个Broker上 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上 容量大小负载：每台broker的硬盘占用大小应该均匀在kafka1.1之前，Kafka能够保证各台broker上partition数量均匀，但由于每个partition内的消息数不同，可能存在不同硬盘之间内存占用差异大的情况。在Kafka1.1中增加了副本跨路径迁移功能 kafka-reassign-partitions.sh，我们可以结合它和监控系统，实现自动化的负载均衡 Kafaka重要参数 acks producer收到多少broker的答复才算真的发送成功acks = 0 : 不接收发送结果acks = all 或者 -1: 表示发送消息时，不仅要写入本地日志，还要等待所有副本写入成功。acks = 1: 写入本地日志即可，是上述二者的折衷方案，也是默认值。 retries 默认为 0，即不重试，立即失败。一个大于 0 的值，表示重试次数。 buffer.memory 指定 producer 端用于缓存消息的缓冲区的大小，默认 32M；适当提升该参数值，可以增加一定的吞吐量, 但是batch太大会增大延迟，可搭配linger_ms参数使用 linger_ms 如果batch太大，或者producer qps不高，batch添加的会很慢，我们可以强制在linger_ms时间后发送batch数据 batch.size producer 会将发送分区的多条数据封装在一个 batch 中进行发送，这里的参数指的就是 batch 的大小。该参数值过小的话，会降低吞吐量，过大的话，会带来较大的内存压力。默认为 16K，建议合理增加该值。 丢失数据的场景及解决方案consumer端不是严格意义的丢失，其实只是漏消费了。设置了 auto.commit.enable=true ，当 consumer fetch 了一些数据但还没有完全处理掉的时候，刚好到 commit interval 触发了提交 offset 操作，接着 consumer 挂掉。这时已经fetch的数据还没有处理完成但已经被commit掉，因此没有机会再次被处理，数据丢失。 解决方案：enable.auto.commit=false 关闭自动提交位移，在消息被完整处理之后再手动提交位移 producer端I/O 线程发送消息之前，producer 崩溃， 则 producer 的内存缓冲区的数据将丢失 解决方案： 同步发送，性能差，不推荐。 仍然异步发送，通过“无消息丢失配置”（来自胡夕的《Apache Kafka 实战》）极大降低丢失的可能性： block.on.buffer.full = true 尽管该参数在0.9.0.0已经被标记为“deprecated”，但鉴于它的含义非常直观，所以这里还是显式设置它为true，使得producer将一直等待缓冲区直至其变为可用。否则如果producer生产速度过快耗尽了缓冲区，producer将抛出异常 acks=all 很好理解，所有follower都响应了才认为消息提交成功，即”committed” retries = MAX 无限重试，直到你意识到出现了问题:) max.in.flight.requests.per.connection = 1 限制客户端在单个连接上能够发送的未响应请求的个数。设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。注意：设置此参数是为了避免消息乱序 使用KafkaProducer.send(record, callback)而不是send(record)方法 自定义回调逻辑处理消息发送失败 callback逻辑中最好显式关闭producer：close(0) 注意：设置此参数是为了避免消息乱序 unclean.leader.election.enable=false 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失 replication.factor &gt;= 3 这个完全是个人建议了，参考了Hadoop及业界通用的三备份原则 min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用 保证replication.factor &gt; min.insync.replicas 如果两者相等，当一个副本挂掉了分区也就没法正常工作了。通常设置replication.factor = min.insync.replicas + 1 即可 如何选择Partiton的数量 在创建 Topic 的时候可以指定 Partiton 数量，也可以在创建完后手动修改。但 Partiton 数量只能增加不能减少。中途增加 Partiton，partition里面的message不会重新进行分配，原来的partition里面的message数据不会变，新加的这个partition刚开始是空的，随后进入这个topic的message就会重新参与所有partition的load balance。 Partition 的数量直接决定了该 Topic 的并发处理能力。但也并不是越多越好。Partition 的数量对消息延迟性会产生影响。 一般建议选择 Broker Num * Consumer Num，这样平均每个 Consumer 会同时读取 Broker 数目个 Partition ， 这些 Partition 压力可以平摊到每台 Broker 上 controller的职责在 kafka 集群中，某个 broker 会被选举承担特殊的角色，即控制器（controller)，用于管理和协调 kafka 集群，具体职责如下： 管理副本和分区的状态 更新集群元数据信息 创建、删除 topic 分区重分配 leader 副本选举 topic 分区扩展 broker 加入、退出集群 受控关闭 controller leader选举 节点异常情形leader挂了（leader failover）当 leader 挂了之后，controller 默认会从 ISR 中选择一个 replica 作为 leader 继续工作，条件是新 leader 必须有挂掉 leader 的所有数据。 如果为了系统的可用性，而容忍降低数据的一致性的话，可以将 unclean.leader.election.enable = true，开启 kafka 的“脏 leader 选举”。当 ISR 中没有 replica，则选一个幸存的replica作为leader 继续响应请求，如此操作提高了 Kafka 的分区容忍度，但是数据一致性降低了。 broker挂了（broker failover）broker上面有很多 partition 和多个 leader 。因此至少需要处理如下内容： 更新该 broker 上所有 follower 的状态 重新给 leader 在该 broker 上的 partition 选举 leader 选举完成后，要更新 partition 的状态，比如谁是 leader 等 kafka 集群启动后，所有的 broker 都会被 controller 监控，一旦有 broker 宕机，ZK 的监听机制会通知到 controller， controller 拿到挂掉 broker 中所有的 partition，以及它上面的存在的 leader，然后从 partition的 ISR 中选择一个 follower 作为 leader，更改 partition 的 follower 和 leader 状态。 controller挂了（controller failover） 由于每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失，所有存活的 broker 收到 fire 的通知 存活的 broker 收到 fire 的通知后，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。 Zookeeper在Kafka中作用 管理 broker 与 consumer 的动态加入与离开。（Producer 不需要管理，随便一台计算机都可以作为Producer 向 Kakfa Broker 发消息） 触发负载均衡，当 broker 或 consumer 加入或离开时会触发负载均衡算法，使得一个 consumer group 内的多个 consumer 的消费负载平衡。（因为一个 comsumer 消费一个或多个partition，一个 partition 只能被一个 consumer 消费） 维护消费关系及每个 partition 的消费信息 Page Cache带来的好处Linux 总会把系统中还没被应用使用的内存挪来给 Page Cache，在命令行输入free，或者 cat /proc/meminfo ，“Cached”的部分就是 Page Cache。 Page Cache 中每个文件是一棵 Radix 树（又称 PAT 位树, 一种多叉搜索树），节点由 4k 大小的 Page 组成，可以通过文件的偏移量（如 0x1110001）快速定位到某个Page。 当写操作发生时，它只是将数据写入 Page Cache 中，并将该页置上 dirty 标志。 当读操作发生时，它会首先在 Page Cache 中查找，如果有就直接返回，没有的话就会从磁盘读取文件写入 Page Cache 再读取。 可见，只要生产者与消费者的速度相差不大，消费者会直接读取之前生产者写入Page Cache的数据，大家在内存里完成接力，根本没有磁盘访问。 而比起在内存中维护一份消息数据的传统做法，这既不会重复浪费一倍的内存，Page Cache 又不需要 GC （可以放心使用60G内存了），而且即使 Kafka 重启了，Page Cache 还依然在。 参考：[1] Kafka常见问题 [知乎][2] Kafka系统设计开篇[3] Kafka史上最详细原理总结]]></content>
      <categories>
        <category>中间件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>基本原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql锁机制]]></title>
    <url>%2F2019%2F12%2F20%2FMysql%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[总览MySQL/InnoDB的加锁，一直是一个常见的话题。例如，数据库如果有高并发请求，如何保证数据完整性？产生死锁问题如何排查并解决？下面是不同锁等级的区别 表级锁开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高 ，并发度最低。 页面锁开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 行级锁开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 查看数据库支持的存储引擎： SHOW ENGINES InnoDB存储引擎乐观锁用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。 何谓数据版本？ 即为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实现。 当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加1。 当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据。 举例：1、数据库表三个字段，分别是id、value、version select id,value,version from TABLE where id = #{id} 2、每次更新表中的value字段时，为了防止发生冲突，需要这样操作 update TABLEset value=2,version=version+1where id=#{id} and version=#{version} 悲观锁悲观锁就是在操作数据时，认为此操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作，这点跟java中的synchronized很相似，所以悲观锁需要耗费较多的时间。 另外与乐观锁相对应的，悲观锁是由数据库自己实现了的，要用的时候，我们直接调用数据库的相关语句就可以了。 说到这里，由悲观锁涉及到的另外两个锁概念就出来了，它们就是共享锁与排它锁。共享锁和排它锁是悲观锁的不同的实现，它俩都属于悲观锁的范畴。 共享锁共享锁又称读锁 (read lock)，是读操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。当如果事务对读锁进行修改操作，很可能会造成死锁。如下图所示。 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获得共享锁的事务只能读数据，不能修改数据。 在查询语句后面增加 LOCK IN SHARE MODE，Mysql会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。 其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 加上共享锁后，对于update，insert，delete语句会自动加排它锁。 排它锁排他锁Exclusive Lock（也叫writer lock）又称写锁。 名词解释：若某个事物对某一行加上了排他锁，只能这个事务对其进行读写，在此事务结束之前，其他事务不能对其进行加任何锁，其他进程可以读取, 不能进行写操作，需等待其释放。 若事务 1 对数据对象A加上X锁，事务 1 可以读A也可以修改A，其他事务不能再对A加任何锁，直到事物 1 释放A上的锁。这保证了其他事务在事物 1 释放A上的锁之前不能再读取和修改A。排它锁会阻塞所有的排它锁和共享锁. 读取为什么要加读锁呢？ 防止数据在被读取的时候被别的线程加上写锁。 排他锁使用方式：在需要执行的语句后面加上for update就可以了 1select status from TABLE where id=1 for update; 排他锁，也称写锁，独占锁，当前写操作没有完成前，它会阻断其他写锁和读锁。 排他锁之所以能阻止update,delete等操作是因为update，delete操作会自动加排他锁，也就是说即使加了排他锁也无法阻止select操作。而select XXX for update 语法可以对select操作加上排他锁。 所以为了防止更新丢失可以在select时加上for update加锁, 这样就可以阻止其余事务的select for update (但注意无法阻止select). 要使用排他锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。 行锁多个事务操作同一行数据时，后来的事务处于阻塞等待状态。这样可以避免了脏读等数据一致性的问题。后来的事务可以操作其他行数据，解决了表锁高并发性能低的问题。 12345678910# Transaction-Amysql&gt; set autocommit = 0;mysql&gt; update innodb_lock set v='1001' where id=1;mysql&gt; commit;# Transaction-Bmysql&gt; update innodb_lock set v='2001' where id=2;Query OK, 1 row affected (0.37 sec)mysql&gt; update innodb_lock set v='1002' where id=1; ## 被事务A阻塞Query OK, 1 row affected (37.51 sec) 现实：当执行批量修改数据脚本的时候，行锁升级为表锁。其他对订单的操作都处于等待中，，， 原因：InnoDB只有在通过索引条件检索数据时使用行级锁，否则使用表锁！ 而模拟操作正是通过id去作为检索条件，而id又是MySQL自动创建的唯一索引，所以才忽略了行锁变表锁的情况 总结：InnoDB的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁。 行锁的劣势开销大；加锁慢；会出现死锁 行锁的优势锁的粒度小，发生锁冲突的概率低；处理并发的能力强 加锁的方式自动加锁。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁；对于普通SELECT语句，InnoDB不会加任何锁；当然我们也可以显示的加锁： 从上面的案例看出，行锁变表锁似乎是一个坑，可MySQL没有这么无聊给你挖坑。这是因为MySQL有自己的执行计划。 当你需要更新一张较大表的大部分甚至全表的数据时。而你又傻乎乎地用索引作为检索条件。一不小心开启了行锁(没毛病啊！保证数据的一致性！)。可MySQL却认为大量对一张表使用行锁，会导致事务执行效率低，从而可能造成其他事务长时间锁等待和更多的锁冲突问题，性能严重下降。所以MySQL会将行锁升级为表锁，即实际上并没有使用索引。 我们仔细想想也能理解，既然整张表的大部分数据都要更新数据，在一行一行地加锁效率则更低。其实我们可以通过explain命令查看MySQL的执行计划，你会发现key为null。表明MySQL实际上并没有使用索引，行锁升级为表锁也和上面的结论一致。 注意：行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。 行锁根据锁定范围又分为间隙锁、临键锁和记录锁。 临健锁当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的临键锁（Next-Key锁）。 危害(坑)：若执行的条件是范围过大，则InnoDB会将整个范围内所有的索引键值全部锁定，很容易对性能造成影响。 间隙锁 记录锁记录锁在锁定非主键索引时，也会一并锁定主键； 表锁Innodb 的行锁是在有索引的情况下，没有索引的表是锁定全表的。 在Innodb引擎中既支持行锁也支持表锁，那么什么时候会锁住整张表，什么时候只锁住一行呢？ 只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。 行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。 行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。 死锁（Deadlock）所谓死锁：是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。由于资源占用是互斥的，当某个进程提出申请资源后，使得有关进程在无外力协助下，永远分配不到必需的资源而无法继续运行，这就产生了一种特殊现象死锁。 解除正在死锁的状态有两种方法：第一种： 1、查询是否锁表 1show OPEN TABLES where In_use &gt; 0; 2、查询进程（如果您有SUPER权限，您可以看到所有线程。否则，您只能看到您自己的线程） 1show processlist 3、杀死进程id（就是上面命令的id列） 1kill id 第二种： 1、查看当前的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX; 2、查看当前锁定的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 3、查看当前等锁的事务 1SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; 4、杀死进程 1kill 进程ID 产生死锁的四个必要条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 如果系统资源充足，进程的资源请求都能够得到满足，死锁出现的可能性就很低，否则就会因争夺有限的资源而陷入死锁。其次，进程运行推进顺序与速度不同，也可能产生死锁。虽然不能完全避免死锁，但可以使死锁的数量减至最少。将死锁减至最少可以增加事务的吞吐量并减少系统开销，因为只有很少的事务回滚，而回滚会取消事务执行的所有工作。由于死锁时回滚的操作由应用程序重新提交。 下列方法有助于最大限度地降低死锁： 按同一顺序访问对象。 避免事务中的用户交互。 保持事务简短并在一个批处理中。 使用低隔离级别。 使用绑定连接。 MyISAM存储引擎InnoDB和MyISAM的最大不同点有两个： InnoDB支持事务(transaction)；MyISAM不支持事务 Innodb默认采用行锁， MyISAM是默认采用表锁。 MyISAM不适合高并发 共享读锁对MyISAM表的读操作（加读锁），不会阻塞其他进程对同一表的读操作，但会阻塞对同一表的写操作。只有当读锁释放后，才能执行其他进程的写操作。在锁释放前不能读其他表。 独占写锁对MyISAM表的写操作（加写锁），会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其他进程的读写操作。在锁释放前不能写其他表。 总结： 表锁，读锁会阻塞写，不会阻塞读。而写锁则会把读写都阻塞。 表锁的加锁/解锁方式：MyISAM在执行查询语句(SELECT)前,会自动给涉及的所有表加读锁,在执行更新操作 (UPDATE、DELETE、INSERT等)前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用LOCK TABLE命令给MyISAM表显式加锁。 如果用户想要显示的加锁可以使用以下命令：锁定表 1LOCK TABLES tbl_name &#123;READ | WRITE&#125;,[ tbl_name &#123;READ | WRITE&#125;,…] 解锁表 1UNLOCK TABLES 在用 LOCK TABLES 给表显式加表锁时, 必须同时取得所有涉及到表的锁。 在执行 LOCK TABLES 后，只能访问显式加锁的这些表，不能访问未加锁的表; 如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。 在自动加锁的情况下也基本如此，MyISAM 总是一次获得 SQL 语句所需要的全部锁。这也正是 MyISAM 表不会出现死锁(Deadlock Free)的原因。 对表test_table增加读锁： 12LOCK TABLES test_table READ UNLOCK test_table 对表test_table增加写锁 12LOCK TABLES test_table WRITEUNLOCK test_table 当使用 LOCK TABLES 时，不仅需要一次锁定用到的所有表,而且,同一个表在 SQL 语句中出现多少次，就要通过与 SQL 语句中相同的别名锁定多少次，否则也会出错！比如如下SQL语句： 1select a.first_name,b.first_name, from actor a,actor b where a.first_name = b.first_name; 该Sql语句中，actor表以别名的方式出现了两次，分别是a,b，这时如果要在该Sql执行之前加锁就要使用以下Sql: 1lock table actor as a read, actor as b read; 并发插入上文说到过 MyISAM 表的读和写是串行的, 但这是就总体而言的。在一定条件下,MyISAM表也支持查询和插入操作的并发进行。 MyISAM存储引擎有一个系统变量concurrent_insert,专门用以控制其并发插入的行为,其值分别可以为0、1或2。 0: 不允许并发插入;1: 如果MyISAM表中没有空洞(即表的中间没有被删除的行), MyISAM允许在一个进程读表的同时,另一个进程从表尾插入记录。这也是MySQL 的默认设置;2: 无论MyISAM表中有没有空洞, 都允许在表尾并发插入记录; 可以利用MyISAM存储引擎的并发插入特性,来解决应用中对同一表查询和插入的锁争用。 MyISAM的锁调度MyISAM 存储引擎的读锁和写锁是互斥的，读写操作是串行的。那么，一个进程请求某个 MyISAM 表的读锁，同时另一个进程也请求同一表的写锁，MySQL 如何处理呢? 答案是写进程先获得锁。 不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前！这是因为 MySQL 认为写请求一般比读请求要重要。这也正是 MyISAM 表不太适合于有大量更新操作和查询操作应用的原因，因为大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况有时可能会变得非常糟糕！ 幸好我们可以通过一些设置来调节 MyISAM 的调度行为。 通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利。 通过执行命令SET LOWPRIORITYUPDATES=1, 使该连接发出的更新请求优先级降低。 通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性, 降低该语句的优先级。 另外, MySQL也供了一种折衷的办法来调节读写冲突, 即给系统参数max_write_lock_count 设置一个合适的值, 当一个表的读锁达到这个值后, MySQL就暂时将写请求的优先级降低, 给读进程一定获得锁的机会。 总结 MySQL的MyISAM引擎支持表级锁。 表级锁分为两种：共享读锁、互斥写锁。这两种锁都是阻塞锁。 可以在读锁上增加读锁，不能在读锁上增加写锁。在写锁上不能增加写锁。 默认情况下，MySql在执行查询语句之前会加读锁，在执行更新语句之前会执行写锁。 如果想要显示的加锁/解锁的花可以使用LOCK TABLES和UNLOCK来进行。 在使用LOCK TABLES之后，在解锁之前，不能操作未加锁的表。 在加锁时，如果显示的指明是要增加读锁，那么在解锁之前，只能进行读操作，不能执行写操作。 如果一次Sql语句要操作的表以别名的方式多次出现，那么就要在加锁时都指明要加锁的表的别名。 MyISAM存储引擎有一个系统变量concurrent_insert, 专门用以控制其并发插入的行为,其值分别可以为0、1或2。 由于读锁和写锁互斥，那么在调度过程中，默认情况下，MySql会本着写锁优先的原则。可以通过low-priority-updates来设置。 利用锁实现事务脏读修改加上排他锁，那么其他事务就无法读取到正在修改的数据行； 不可重复读查询加上读锁，则其他事务就无法修改； 幻读范围查询会通过间隙锁使其他事务无法插入范围数据，这不会出现幻读； 参考全面了解mysql锁机制（InnoDB）与问题排查]]></content>
      <categories>
        <category>Mysql</category>
        <category>锁机制</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis底层数据结构]]></title>
    <url>%2F2019%2F12%2F20%2FRedis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[5种基本数据类型 对于每种数据结构，实际上都有自己底层的 内部编码 实现，而且是多种实现。这样 Redis 会在合适的 场景 选择合适的 内部编码，如图所示： 可以看到，每种 数据结构 都有 两种以上 的 内部编码实现。例如 list 数据结构 包含了 linkedlist 和 ziplist 两种 内部编码。同时有些 内部编码，例如 ziplist，可以作为多种外部数据结构的内部实现，可以通过 object encoding 命令查询内部编码： 1234127.0.0.1:6379&gt; object encoding hello"embstr"127.0.0.1:6379&gt; object encoding mylist"quicklist" Redis 这样设计有两个好处： 其一：可以改进内部编码，而对外的数据结构和命令没有影响。例如：Redis3.2 提供的 quicklist，结合了 ziplist 和 linkedlist 两者的优势，为 列表类型 提供了一种更加高效的内部编码实现。 其二：不同内部编码可以在不同场景下发挥各自的优势。例如 ziplist 比较节省内存，但是在列表元素比较多的情况下，性能会有所下降，这时候Redis会根据配置，将列表类型的内部实现 转换为linkedlist。 String底层结构字符串类型是Redis最基础的数据结构。字符串类型的值实际可以是字符串（简单 和 复杂 的字符串，例如 JSON、XML）、数字（整数、浮点数），甚至是 二进制（图片、音频、视频），但是值最大不能超过 512MB。 字符串类型的内部编码有 3 种： int：8个字节的长整型。 embstr：小于等于39个字节的字符串。 raw：大于39个字节的字符串。 Redis 会根据当前值的 类型 和 长度 决定使用哪种内部编码实现。 Hash底层结构大部分编程语言都提供了哈希（hash）类型，它们的叫法可能是 哈希、字典、关联数组。在 Redis 中，哈希类型是指键值本身又是一个键值对结构。 压缩列表(ziplist)当哈希类型元素个数 小于 hash-max-ziplist-entries 配置（默认 512 个）、同时所有值都小于 hash-max-ziplist-value 配置（默认 64 字节）时，Redis会使用ziplist 作为哈希的内部实现，ziplist使用更加紧凑的结构实现多个元素的连续存储，所以在节省内存方面比hashtable更加优秀. 哈希表(hashtable)当哈希类型无法满足 ziplist 的条件时，Redis会使用hashtable作为哈希的内部实现，因为此时 ziplist 的 读写效率 会下降，而 hashtable 的读写时间复杂度为 O(1)。 下面的示例演示了哈希类型的内部编码，以及相应的变化。 当field个数比较少，且没有大的value时，内部编码为ziplist： 1234127.0.0.1:6379&gt; hmset hashkey f1 v1 f2 v2OK127.0.0.1:6379&gt; object encoding hashkey"ziplist" 当有value大于64字节时，内部编码会由 ziplist 变为 hashtable： 1234127.0.0.1:6379&gt; hset hashkey f3 "one string is bigger than 64 byte...忽略..."OK127.0.0.1:6379&gt; object encoding hashkey"hashtable" 当field个数超过 512，内部编码也会由 ziplist 变为hashtable： 1234127.0.0.1:6379&gt; hmset hashkey f1 v1 f2 v2 f3 v3 ... f513 v513OK127.0.0.1:6379&gt; object encoding hashkey"hashtable" List底层结构列表（list）类型是用来存储多个有序的字符串。在Redis中，可以对列表的两端进行 插入（push）和 弹出（pop）操作，还可以获取 指定范围 的 元素列表、获取 指定索引下标 的 元素 等， 一个列表最多可以存储 2^32 - 1 个元素。 列表是一种比较灵活的数据结构，它可以充当 栈 和 队列 的角色，在实际开发上有很多应用场景。 列表类型的内部编码有两种： ziplist（压缩列表）当列表的元素个数小于 list-max-ziplist-entries 配置（默认 512 个），同时列表中 每个元素的值都小于 list-max-ziplist-value 配置时（默认 64字节），Redis会选用 ziplist来作为列表的内部实现来减少内存的使用。 linkedlist（链表）当列表类型无法满足ziplist的条件时， Redis会使用 linkedlist 作为列表的内部实现。 Set底层结构集合（set）类型也是用来保存多个字符串元素，但和列表类型不一样的是，集合中不允许有重复元素，并且集合中的元素是无序的，不能通过索引下标获取元素， 一个集合最多可以存储 2^32 - 1 个元素。 集合类型的内部编码有两种： intset（整数集合）当集合中的元素都是整数且元素个数小于 set-max-intset-entries 配置（默认 512个）时，Redis会选用 intset 来作为集合的内部实现，从而减少内存的使用。 hashtable（哈希表）当集合类型无法满足intset的条件时，Redis会使用hashtable作为集合的内部实现。 参考深入剖析Redis系列(一) - Redis入门简介与主从搭建 深入剖析Redis系列(二) - Redis哨兵模式与高可用集群 深入剖析Redis系列(三) - Redis集群模式搭建与原理详解 深入剖析Redis系列(四) - Redis数据结构与全局命令概述 深入剖析Redis系列(五) - Redis数据结构之字符串 深入剖析Redis系列(七) - Redis数据结构之列表 深入剖析Redis系列(八) - Redis数据结构之集合]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP TIME_WAIT状态]]></title>
    <url>%2F2019%2F12%2F18%2FTCP-TIME-WAIT%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[TIME_WAIT状态是TCP连接中主动关闭连接的一方会进入的状态，在发出最后一个ACK包之后，主动关闭方进入TIME_WAIT状态，从而确保: ACK包到达对端 等待网络中之前迷路的数据包完全消失，防止端口被复用的时候收到迷路包从而出现收包错误 TIME_WAIT状态会持续2MSL（max segment lifetime）的时间，一般1分钟到4分钟。在这段时间内端口不能被重新分配使用。 TIME_WAIT并不会占用过多的系统资源，但是可以通过修改内核参数/etc/sysctl.conf来限制TIME_WAIT数量。 四次挥手过程先来了解TCP四次挥手的过程： 第一次：主机 1（可以是客户端，也可以是服务器端），设置 Sequence Number 和 Acknowledgment Number，向主机 2 发送一个 FIN 报文段；此时，主机1进入 FIN_WAIT_1 状态；这表示主机 1 没有数据要发送给主机 2 了； 第二次：主机 2 收到了主机 1 发送的 FIN 报文段，向主机 1 回一个 ACK 报文段，Acknowledgment Number 为 Sequence Number 加 1 ；主机 1 进入 FIN_WAIT_2 状态；主机 2 告诉主机 1，我“同意”你的关闭请求； 第三次：主机 2 向主机 1 发送FIN报文段，请求关闭连接，同时主机 2 进入LAST_ACK 状态； 第四次：主机 1 收到主机 2 发送的 FIN报文段，向主机2发送 ACK 报文段，然后主机1进入 TIME_WAIT 状态；主机 2 收到主机 1 的ACK报文段以后，就关闭连接；此时，主机1等待 2 * MSL 后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。 注： MSL是指Max Segment Lifetime，即一个IP数据包能在网络中生存的最长时间，超过这个时间，IP数据包将在网络中消失。每种TCP协议的实现方法均要指定一个合适的MSL值，如RFC1122给出的建议值为2分钟，又如Berkeley体系的TCP实现通常选择30秒作为MSL值。这意味着TIME_WAIT的典型持续时间为1-4分钟。 TCP四次挥手过程中通信双方状态解析 FIN_WAIT_1其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是： FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。（主动方） FIN_WAIT_2实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。（主动方） CLOSE_WAIT表示在等待关闭。当对方close一个SOCKET后发送FIN报文给你，你自然会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。（被动方） LAST_ACK被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED状态了。（被动方） TIME_WAIT表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED状态了。如果FIN WAIT1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（主动方） CLOSED表示SOCKET连接已中断 为什么会有TIME_WAIT状态 可靠地实现TCP全双工连接的可靠终止TCP协议在关闭连接的四次握手过程中，最终ACK是由主动关闭连接的一端发出的，如果这个ACK丢失，被动方将重发最终的FIN，因此主机1就必须维护状态信息TIME_WAIT 允许它发送最终的ACK。如果主机1不维持TIME_WAIT的状态，而是处于CLOSED状态，那么主机1将响应RST（reset）数据包，主机2收到后将此数据报解释成一个异常（Java中会抛出connection reset的SocketException）。因而，要实现TCP全双工连接的正常终止，必须处理终止过程中四个数据包任何一个分节丢失的情况，主动关闭连接的主机1必须维持TIME_WAIT的状态。 保证此次连接的重复数据段从网络中消失TCP数据包可能由于路由器异常而“迷路”，在“迷路”期间，TCP发送端可能因确认超时而重发这个分节，“迷路”的分节在路由器恢复正常后也会被发送到最终的目的地，这个迟到的“迷路”数据包到达时可能会引起问题。在关闭“前一个连接”之后，马上又建立起一个相同的IP和端口之间的“新连接”，这会导致“前一个连接”的迷路重复分组在“前一个连接”终止后到达，从而被“新连接”接收到了。 为了避免以上情况，TCP/IP协议不允许处于TIME_WAIT状态的连接启动一个新的可用连接，因为TIME_WAIT状态持续2MSL，这就可以保证当成功建立一个新TCP连接的时候，来自旧连接重复分组已经在网络中消失。 出现太多TIME_WAIT危害 在高并发短连接的TCP服务器上，当服务器处理完请求后会立刻按照主动正常关闭连接。这个场景下，会出现大量socket处于TIMEWAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上。 解释下这个场景。主动正常关闭TCP连接，都会出现TIMEWAIT。为什么我们要关注这个高并发短连接呢？有两个方面需要注意： ① 高并发可以让服务器在短时间范围内同时占用大量端口，而端口有个0~65535的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了。 ② 在这个场景中，短连接表示“业务处理+传输数据的时间 远远小于 TIMEWAIT超时的时间”的连接。 这里有个相对长短的概念，比如，取一个web页面，1秒钟的http短连接处理完业务，在关闭连接之后，这个业务用过的端口会停留在TIMEWAIT状态几分钟，而这几分钟，其他HTTP请求来临的时候是无法占用此端口的。单用这个业务计算服务器的利用率会发现，服务器干正经事的时间和端口（资源）被挂着无法被使用的时间的比例是 1 ：几百，服务器资源严重浪费。 说个题外话，从这个意义出发来考虑服务器性能调优的话，长连接业务的服务就不需要考虑TIMEWAIT状态。同时，假如你对服务器业务场景非常熟悉，你会发现，在实际业务场景中，一般长连接对应的业务的并发量并不会很高. 综合这两个方面，持续的到达一定量的高并发短连接，会使服务器因端口资源不足而拒绝为一部分客户服务。 TIME_WAIT太多怎么解决修改/etc/sysctl.conf： 12345678910111213# 表示开启重用# 允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_reuse = 1 # 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。# net.ipv4.tcp_timestamps 开启时，net.ipv4.tcp_tw_recycle开启才能生效net.ipv4.tcp_tw_recycle = 1 # 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭net.ipv4.tcp_timestamps = 1 # 用来设置保持在FIN_WAIT_2状态的时间net.ipv4.tcp_fin_timeout = 2 保存后sysctl -p生效 地址reuse问题在写一个unix server程序时，经常需要在命令行重启它，绝大多数时候工作正常，但是某些时候会抛出异常 bind: address already in use，于是重启失败。 上面这个就是地址reuse问题，就是由于TIME_WAIT状态产生的，我们有以下方案来解决这个问题： SO_REUSEADDR这个socket选项通知内核：如果端口忙，但TCP状态位于TIME_WAIT，可以重用端口。 一个socket由相关五元组构成: 协议、本地地址、本地端口、远程地址、远程端口。SO_REUSEADDR仅仅表示可以重用本地本地地址、本地端口，整个相关五元组还是唯一确定的。所以，重启后的服务程序有可能收到非期望数据。必须慎重使用SO_REUSEADDR选项。 一般来说，一个端口释放后会等待两分钟之后才能再被使用，SO_REUSEADDR是让端口释放后立即就可以被再次使用。 SO_REUSEADDR用于对TCP处于TIME_WAIT状态下的socket，才可以重复绑定使用。server程序总是应该在调用bind()之前设置SO_REUSEADDR选项。先调用close()的一方会进入TIME_WAIT状态。 SO_REUSEADDR允许启动一个监听服务器并捆绑其众所周知端口，即使以前建立的将此端口用做他们的本地端口的连接仍存在。这通常是重启监听服务器时出现，若不设置此选项，则bind时将出错。 SO_LINGERLinux网络编程中，socket的选项很多。其中几个比较重要的选项就包括SO_LINGER。 在默认情况下,当调用close()关闭socket的使用，close()会立即返回,但是,如果send buffer中还有数据，系统会试着先把send buffer中的数据发送出去，SO_LINGER选项则是用来修改这种默认操作的。 SO_LINGER是一个socket选项，可以通过set sockopt API进行设置，使用起来比较简单，但其实现机制比较复杂，且字面意思上比较难理解。SO_LINGER的值用如下数据结构表示： 1234struct linger &#123; int l_onoff //0 = off, nonzero = on(开关) int l_linger //linger time(延迟时间)&#125; 其取值和处理如下： 设置 l_onoff 为0，则该选项关闭，l_linger的值被忽略，等于内核缺省情况，close()调用会立即返回给调用者，如果可能将会传输任何未发送的数据； 设置 l_onoff 为非0，l_linger为0，当调用close()的时候,TCP连接会立即断开。send buffer中未被发送的数据将被丢弃，并向对方发送一个RST信息。值得注意的是，由于这种方式，不是以4次握手方式结束TCP连接，所以，TCP连接将不会进入TIME_WAIT状态，这样会导致新建立的可能和旧连接的数据造成混乱。这种关闭方式称为“强制”或“失效”关闭。通常会看到 Connection reset by peer 之类的错误； 设置 l_onoff 为非0，l_linger为非0，在这种情况下，会使得close()返回得到延迟。调用close()去关闭socket的时候，内核将会延迟。也就是说，如果send buffer中还有数据尚未发送，该进程将会被休眠直到一下任何一种情况发生： a. send buffer中的所有数据都被发送并且得到对方TCP的应答消息； b.延迟时间消耗完。在延迟时间被消耗完之后，send buffer中的所有数据都将会被丢弃。这种关闭称为“优雅的”关闭。 因此，在正常情况下，在socket调用close()之前设置SO_LINGER超时为0都不是个好的选择。但也有些情况下需要使用SO_LINGER： 如果server返回无效数据或者超时时，SO_LINGER有助于避免卡在CLOSE_WAIT或TIME_WAIT的状态； 如果必须启动有数千个客户端连接的app，则可以考虑设置SO_LINGER，从而避免数千个socket处于TIME_WAIT状态，从而减少可用端口在服务重启后，新客户端连接受到的影响； 总结通过上面的讨论，我们知道TIME_WAIT状态是友好的，并不是多余的。TCP要保证在所有可能的情况下使得所有的数据都能够正确送达。当你关闭一个socket时，主动关闭一端的socket将进入TIME_WAIT状态，而被动关闭的一方则进入CLOSED状态，这的确能够保证所有的数据都被传送。 当一个socket关闭的时候，是通过两端四次挥手完成的，当一端调用close()时，就说明本端没有数据要传送了，这好像看来在挥手完成以后，socket就可以处于CLOSED状态了，其实不然，原因是这样安排状态有两个问题： 第一，我们没有任何机制保证最后的一个ACK能够正常传输； 第二，网络仍然可能有残余的数据包，我们也必须能够正常处理。 TIME_WAIT状态就是为了解决这两个问题而生的。服务端为了解决这个TIME_WAIT问题，可选的方式有3种： 保证由客户端主动发起关闭 关闭的时候使用RST方式（set SO_LINGER） 对处于TIME_WAIT状态的TPC允许重用（set SO_REUSEADDR） 参考：TCP/IP中TIME_WAIT状态详解TCP——-为什么会有TIME_WAIT状态 ?]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>Tcp</tag>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM基础 - 入门篇]]></title>
    <url>%2F2019%2F12%2F13%2FJVM%E5%9F%BA%E7%A1%80-%E5%85%A5%E9%97%A8%E7%AF%87%2F</url>
    <content type="text"><![CDATA[JDK体系结构 JVM内存模型一张图描述JVM内存模型 JVM包括两个子系统和两个组件： 两个子系统 ClassLoader（类装载）根据给定的全限定名类名(如：java.lang.Object)来装载 class文件 到 运行时数据区 中的 方法区。程序中可以继承 java.lang.ClassLoader 类来实现自己的ClassLoader。 ExecutionEngine（执行引擎）执行classes中的指令。任何JVM specification实现(JDK)的核心都是Execution engine，不同的JDK例如Sun的JDK和IBM的JDK好坏主要就取决于他们各自实现的Execution engine的好坏。 两个组件 Native Interface(本地接口)与native libraries交互，是其它编程语言交互的接口。当调用native方法的时候，就进入了一个全新的并且不再受虚拟机限制的世界，所以也很容易出现JVM无法控制的native heap OutOfMemory。 Runtime Data Area（运行时数据区）这就是我们常说的JVM的内存。主要分为五个部分： 方法区有时候也成为永久代，该区域是被线程共享的。 作用方法区主要用来存储已被虚拟机加载的类的信息、常量、静态变量 和 即时编译器(JIT)编译后的代码 等数据. GC在该区内很少发生垃圾回收，但是并不代表不发生GC，在这里进行的GC主要是 对方法区里的常量池和对类型的卸载，但回收效率很低，当方法区无法满足内存需求时，会报 OOM 异常； 方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量并不一定是编译时确定，运行时生成的常量也会存在这个常量池中。 方法区和元数据区是不同jdk版本对JVM协议的不同实现； 虚拟机栈虚拟机栈也就是我们平常所称的栈内存, 它为java方法服务，每个方法在执行的时候都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接 和 方法出口等信息。 虚拟机栈是线程私有的，它的生命周期与线程相同。每个方法从调用到执行过程，就对应着栈桢在虚拟机栈中从入栈到出栈的过程。 栈桢虚拟机栈由多个栈桢（Stack Frame）组成。一个线程会执行一个或多个方法，一个方法对应一个栈桢。 局部变量表局部变量表里存储的是基本数据类型、returnAddress类型（指向一条字节码指令的地址）和对象引用，这个对象引用有可能是指向对象起始地址的一个指针，也有可能是代表对象的句柄或者与对象相关联的位置。局部变量所需的内存空间在编译器间确定。 操作数栈操作数栈的作用主要用来存储运算结果以及运算的操作数，它不同于局部变量表通过索引来访问，而是压栈和出栈的方式 每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接. 动态链接就是将常量池中的符号引用在运行期转化为直接引用。 本地方法栈本地方法栈和虚拟机栈类似，虚拟机栈是为虚拟机执行Java方法而准备的，而本地方法栈为虚拟机执行Native本地方法而准备的。 堆（Heap）Java堆是所有线程所共享的一块内存。 在虚拟机启动时创建，几乎所有的对象实例、数组都在这里存放，对于大多数应用来说，堆是JVM管理的内存中最大的一块区域，也是最容易发生OOM的区域，因此该区域经常发生垃圾回收操作。 大多数JVM都会将堆实现为大小可扩展的，通过-Xmx、-Xms等参数控制。 新生的对象默认放在Eden区, Eden区满了会触发minor GC/yong GC; 程序计数器占用内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理和线程恢复等功能都需要依赖这个计数器完成。该内存区域是唯一一个java虚拟机规范没有规定任何OOM情况的区域。 JVM内存快照示例基于上述原理，现在写一个简单的代码来举例描述下具体各个区域是怎么分配的。 Math.java 12345678910111213141516public class Math &#123; public static final int initData = 666; public int compute() &#123; int a = 1; int b = 2; int c = (a + b) * 10; return c; &#125; public static void main(String[] args) &#123; Math math = new Math(); int result = math.compute(); System.out.println(result); &#125;&#125; javap是jdk自带的一个工具在jdk安装目录的/bin下面可以找到，可以对代码反编译，也可以查看java编译器生成的字节码，对代码的执行过程进行分析，了解jvm内部的工作。 通过以下指令可以得到Java字节码指令： javac Math.javajavap -c Math.class &gt; Math.txt Math.txt(为了便于编译，Math.txt中去掉了User相关内容) 12345678910111213141516171819202122232425262728293031323334353637383940Compiled from "Math.java"public class Math &#123; public static final int initData; public static User user = new User(); public Math(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public int compute(); Code: 0: iconst_1 1: istore_1 2: iconst_2 3: istore_2 4: iload_1 5: iload_2 6: iadd 7: bipush 10 9: imul 10: istore_3 11: iload_3 12: ireturn public static void main(java.lang.String[]); Code: 0: new #2 // class Math 3: dup 4: invokespecial #3 // Method "&lt;init&gt;":()V 7: astore_1 8: aload_1 9: invokevirtual #4 // Method compute:()I 12: istore_2 13: getstatic #5 // Field java/lang/System.out:Ljava/io/PrintStream; 16: iload_2 17: invokevirtual #6 // Method java/io/PrintStream.println:(I)V 20: return&#125; 通过查询javap 字节码指令集，可以看到每一步操作: 12345678910111213141516171819栈和局部变量操作 将常量压入栈的指令 aconst_null 将null对象引用压入栈 iconst_m1 将int类型常量-1压入栈 iconst_0 将int类型常量0压入栈 iconst_1 将int类型常量1压入栈 iconst_2 将int类型常量2压入栈 iconst_3 将int类型常量3压入栈 iconst_4 将int类型常量4压入栈 iconst_5 将int类型常量5压入栈 lconst_0 将long类型常量0压入栈 lconst_1 将long类型常量1压入栈 fconst_0 将float类型常量0压入栈 fconst_1 将float类型常量1压入栈 dconst_0 将double类型常量0压入栈 dconst_1 将double类型常量1压入栈 bipush 将一个8位带符号整数压入栈 sipush 将16位带符号整数压入栈 .... 此时内存区域如下图所示： 垃圾回收Minor GC和Full GC区别 Minor GC/Young GC: 指新生代发生的垃圾收集动作，Minor GC非常频繁，回收速度一般比较快； Major GC/Full GC: 一般会回收老年代，年轻代，方法区（永久区）的垃圾，Major GC的速度一般会比Minor GC慢10倍以上。 什么时候回收 Minor GC触发条件当Eden区满时，触发Minor GC。 Full GC触发条件 （1）调用System.gc时，系统建议执行Full GC，但是不必然执行 （2）老年代空间不足 （3）方法区空间不足 （4）通过Minor GC后进入老年代的平均大小大于老年代的可用内存 （5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小、 怎么回收从GC的底层机制可以看出，对于可以搜索到的对象进行复制操作，对于搜索不到的对象，调用finalize()方法进行释放。 具体过程：当GC线程启动时，会通过可达性分析法把Eden区和From Space区的存活对象复制到To Space区，然后把Eden Space和From Space区的对象释放掉。当GC轮训扫描To Space区一定次数后，把依然存活的对象复制到老年代，然后释放To Space区的对象。 对于用可达性分析法搜索不到的对象，GC并不一定会回收该对象。要完全回收一个对象，至少需要经过两次标记的过程。 第一次标记对于一个没有其他引用的对象，筛选该对象是否有必要执行finalize()方法，如果没有执行必要，则意味可直接回收。（筛选依据：是否复写或执行过finalize()方法；因为finalize方法只能被执行一次）。 第二次标记如果被筛选判定位有必要执行，则会放入FQueue队列，并自动创建一个低优先级的finalize线程来执行释放操作。如果在一个对象释放前被其他对象引用，则该对象会被移除FQueue队列。 JVM内存分配与回收策略对象优先在Eden区分配大多数情况下，对象在新生代中Eden区分配，当Eden区没有足够空间时，虚拟机将发起一次Minor GC. 长期存活的对象将进入老年代 既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须识别哪些对象应该放在新生代，哪些需要放在老年代。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在Eden区出生并经过一次minor gc后仍然存活，并且大小能被Survivor容纳的话，将会移动到另一个Survivor区，并将对象年龄设为1。对象在Survivor中每熬过一次minor gc，年龄就增1岁，当增加到一定大小（默认为15岁），就会晋升到老年代。对象晋升到老年代的年龄阈值可以通过-XX:MaxTenuringThreshold来设置。 大对象直接进入老年代 大对象就是需要大量连续内存空间的对象，比如：长字符串、数组 JVM参数-神器:PretenureSizeThreshold可以设置大对象的大小，如果对象超过了设置大小，在创建时就会直接进入老年代，不会进入年轻代，这个参数只在Serial和ParNew两个收集器下有效 例如 -神器:PretenureSizeThreshold=1000000 -XX:+UseSerialGC 这么做的目的：避免为大对象分配内存时的复制操作而降低效率； 对象动态年龄判断 当前放置对象的Survivor区域里（另一块Survivor为空），一批对象的总大小大于这块Survivor区域内存大小的50%，那么此时大于这批对象年龄最大值的对象，就可以直接提前进入老年代了； 例如Survivor区域里现有一批对象：年龄为1对象 + 年龄为2对象 + ... + 年龄为n对象 所占空间总和超过了Survivor区总大小的50%，此时就会把年龄大于n的对象提前放入老年代； 老年代空间分配担保机制年轻代每次Minor gc之前，JVM都会计算下老年代剩余可用空间： 如果老年代剩余可用空间小于年轻代里现有的所有对象的大小之和（包括垃圾对象），就会看一个 -神器:-HandlePromotionFailure （JDK1.8默认就设置了）的参数是否配置了，如果有这个参数，就会看看老年代的可用内存大小，是否大于之前每次minor gc后进入老年代的对象平均大小； 如果上一步结果是小于或者没有设置该参数，JVM就会发起一次Full GC，对老年代和年轻代一起进行垃圾回收； 如果上一步结果是大于该参数，正常进行Minor GC;当然如果Minor GC后，剩余存活对象里需要移动到老年代的总大小超过了老年代可用空间，还是会触发Full GC; 如果回收完还是没有足够空间存放新建的对象，就会发生 OOM； 垃圾对象判断如何判断一个对象是否可以被回收，常见的做法有两种： 引用计数法给对象头中添加一个引用计数器，每当有一个地方引用它，就给它的计数器+1；当引用失效，计数器就-1；只要计数器为0，就表示当前对象没有被使用，可以被回收。 优点引用计数收集器可以很快的执行，交织在程序运行中。对程序需要不被长时间打断的实时环境比较有利。 缺点无法检测出循环引用。如父对象有一个对子对象的引用，子对象反过来引用父对象。这样，他们的引用计数永远不可能为0. Java的引用类型一般分为四种： 强引用普通的变量引用； 软引用将对象用SoftReference软引用类型的对象包裹，正常情况下不会被回收，但是GC做完之后发现释放不出空间存放新对象，就会把这些软引用的对象回收掉。软引用可用来实现对内存敏感度不高的高速缓存。 1public static SoftReference&lt;User&gt; user = new SoftReference&lt;User&gt;(new User()); 弱引用将对象用WeakReference弱引用类型的对象包裹，弱引用跟没引用差不多，GC会直接回收掉，很少用。 1public static WeakReference&lt;User&gt; user = new WeakReference&lt;User&gt;(new User()); 虚引用也成为幽灵引用或者幻影引用，它是最弱的一种引用关系，几乎不用。 可达性分析算法这个算法的基本思想是通过一系列被称为”GC Roots”的对象作为起始点，从这些节点向下搜索，找到的对象都标记为非垃圾对象，剩余的都为垃圾对象； GC Roots: 线程栈的本地变量、静态属性、常量、本地方法栈的变量等； finalize()方法最终判定对象是否存活即使在可达性分析算法中不可达的对象，也并非是 『非死不可』，标记完之后只是暂时处于 『缓刑』 阶段，要真正宣告一个对象死亡，至少要经历再次标记过程。 标记的前提是：对象在进行可达性分析之后发现没有与任何GC Roots相连的引用链。 第一次标记并进行一次筛选对象如果没有覆盖finalize()方法，将会直接被回收； 第二次标记如果这个对象覆盖了finalize()方法，只要在该方法中重新与引用链上的任何一个对象建立了关联，就可以拯救自己，不会被回收。 如何判断一个类是无用类：类需要同时满足下面3个条件才能算是 无用类： 该类所有的实例都已经被回收，也就是堆中不存在该类的任何实例； 加载该类的ClassLoader已经被回收； 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法； 垃圾收集算法常用的垃圾回收算法有：标记-清除算法、复制算法、标记-整理算法、分代收集算法。目前主流的JVM（HotSpot）采用的是分代收集算法。 标记-清除算法标记-清除算法采用从根集合进行扫描，对存活的对象对象标记，标记完毕后，再扫描整个空间中未被标记的对象，进行回收。标记-清除算法不需要进行对象的移动，并且仅对不存活的对象进行处理，在存活对象比较多的情况下极为高效，但由于标记-清除算法直接回收不存活的对象，因此会造成内存碎片。 复制算法 标记-整理算法 分代收集算法 垃圾收集器 Serial收集器 ParNew收集器 Parallel Scavenge收集器 CMS收集器 G1收集器 JVM优化能否对JVM调优，让其几乎不发生Full GC?评估对象大小和生命周期，调整年轻代大小和Eden/Survivor区比例，保证minor gc就能够回收基本所有对象，避免对象因为过大或年龄太大进入老年代。 需要放入survivor区的对象大于survivor区大小的50%时，会触发担保机制，直接放入老年代； 扩展阅读【搞定Jvm面试】Java 内存区域揭秘附常见面试题解析]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据结构及常见使用场景]]></title>
    <url>%2F2019%2F12%2F06%2FRedis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%8F%8A%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[Redis基础数据类型及使用场景 String 缓存功能：String字符串是最常用的数据类型，不仅仅是Redis，各个语言都是最基本类型，因此，利用Redis作为缓存，配合其它数据库作为存储层，利用Redis支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。 计数器：许多系统都会使用Redis作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。 共享用户Session：用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存Cookie，但是可以利用Redis将用户的Session集中管理，在这种模式只需要保证Redis的高可用，每次用户Session的更新和获取都可以快速完成。大大提高效率。 Hash 这个是类似 Map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 Redis 里，然后每次读写缓存的时候，可以就操作 Hash 里的某个字段。但是这个的场景其实还是多少单一了一些，因为现在很多对象都是比较复杂的，比如你的商品对象可能里面就包含了很多属性，其中也有对象。我自己使用的场景用得不是那么多。 ListList 是有序列表，这个还是可以玩儿出很多花样的。 比如可以通过 List 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 List 实现分页查询，这个是很棒的一个功能，基于 Redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 比如可以搞个简单的消息队列，从 List 头怼进去，从 List 屁股那里弄出来。List本身就是我们在开发过程中比较常用的数据结构了，热点数据更不用说了。 消息队列：Redis的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过Lpush命令从左边插入数据，多个数据消费者，可以使用BRpop命令阻塞的“抢”列表尾部的数据。 文章列表或者数据分页展示的应用。比如，我们常用的博客网站的文章列表，当用户量越来越多时，而且每一个用户都有自己的文章列表，而且当文章多时，都需要分页展示，这时可以考虑使用Redis的列表，列表不但有序同时还支持按照范围内获取元素，可以完美解决分页查询功能。大大提高查询效率。 SetSet 是无序集合，会自动去重的那种。 去重： 直接基于 Set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 JVM 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于Redis进行全局的 Set 去重。 可以基于 Set 玩 交集、并集、差集 的操作。比如交集吧，我们可以把两个人的好友列表整一个交集，看看俩人的共同好友是谁？对吧。反正这些场景比较多，因为对比很快，操作也简单，两个查询一个Set搞定。 Sorted SetSorted set 是排序的 Set，去重并可以排序，写进去的时候给一个分数，自动根据分数排序。 有序集合的使用场景与集合类似，但是set集合不是自动有序的，而Sorted set可以利用分数进行成员间的排序，而且是插入时就排序好。所以当你需要一个有序且不重复的集合列表时，就可以选择Sorted set数据结构作为选择方案。 排行榜：有序集合经典使用场景。例如视频网站需要对用户上传的视频做排行榜，榜单维护可能是多方面：按照时间、按照播放量、按照获得的赞数等。 用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 Redis更多应用场景 Redis 和 Memcached 区别Memcache先来看看 MC 的特点： MC 处理请求时使用多线程异步IO 的方式，可以合理利用 CPU 多核的优势，性能非常优秀； MC 功能简单，使用内存存储数据； MC 的内存结构以及钙化问题； MC 对缓存的数据可以设置失效期，过期后的数据会被清除； 失效的策略采用延迟失效，就是当再次使用数据时检查是否失效； 当容量存满时，会对缓存中的数据进行剔除，剔除时除了会对过期 key 进行清理，还会按 LRU 策略对数据进行剔除。 另外，使用 MC 有一些限制，这些限制在现在的互联网场景下很致命，成为大家选择Redis、MongoDB的重要原因： key 不能超过 250 个字节； value 不能超过 1M 字节； key 的最大失效时间是 30 天； 只支持 K-V 结构，不提供持久化和主从同步功能。 Redis简单说一下 Redis 的特点，方便和 Memcache 比较。 与 MC 不同的是，Redis 采用单线程模式处理请求。这样做的原因有 2 个： 一个是因为采用了非阻塞的异步事件处理机制； 另一个是缓存数据都是内存操作 IO 时间不会太长，单线程可以避免线程上下文切换产生的代价。 Redis 支持持久化，所以 Redis 不仅仅可以用作缓存，也可以用作 NoSQL 数据库。 相比 MC，Redis 还有一个非常大的优势，就是除了 K-V 之外，还支持复杂的数据结构，例如 list、set、sorted set、hash 等。 Redis 提供主从同步机制，以及原生支持集群模式，能够提供高可用服务, 在 redis3.x 版本中，便能支持 Cluster 模式; 而 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比由于 Redis 只使用单核，而 Memcached 可以使用多核，所以平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。而在 100k 以上的数据中，Memcached 性能要高于 Redis，虽然 Redis 最近也在存储大数据的性能上进行优化，但是比起 Remcached，还是稍有逊色。 Redis 线程模型Redis 内部使用 文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 Socket，根据 Socket 上的事件来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 Socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 Socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 Socket，会将 Socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。 高级用法Bitmap位图是支持按 bit 位来存储信息，可以用来实现 布隆过滤器（BloomFilter）； HyperLogLog供不精确的去重计数功能，比较适合用来做大规模数据的去重统计，例如统计 UV； Geospatial可以用来保存地理位置，并作位置距离计算或者根据半径计算位置等。有没有想过用Redis来实现附近的人？或者计算最优地图路径？ pub/sub功能是订阅发布功能，可以用作简单的消息队列。 Pipeline可以批量执行一组指令，一次性返回全部结果，可以减少频繁的请求应答。 LuaRedis 支持提交 Lua 脚本来执行一系列的功能, 利用他的原子性。 事务最后一个功能是事务，但 Redis 提供的不是严格的事务，Redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。 Redisson实现分布式锁 Redis集群 参考[1]《吊打面试官》系列-Redis基础[2]《吊打面试官》系列-缓存雪崩、击穿、穿透[3]《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU[4]《吊打面试官》系列-Redis终章凛冬将至、FPX新王登基[5]《吊打面试官》系列-秒杀系统设计]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>入门</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat系统架构[转载]]]></title>
    <url>%2F2019%2F12%2F06%2FTomcat%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84-%E8%BD%AC%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[Tomcat顶层架构先上一张Tomcat的顶层结构图，如下 简化清晰版 Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，用于具体提供服务。 Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下： Connector用于处理连接相关的事情，并提供Socket与Request和Response相关的转化; Container用于封装和管理Servlet，以及具体处理Request请求； 一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接 示意图如下（Engine、Host、Context下边会说到）： 多个 Connector 和一个 Container 就形成了一个 Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给它生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。 另外，上述的包含关系或者说是父子关系，都可以在tomcat的conf目录下的server.xml配置文件中看出，如下是删除了注释内容之后的一个完整的server.xml配置文件（Tomcat版本为8.0） 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version='1.0' encoding='utf-8'?&gt;&lt;Server port="8005" shutdown="SHUTDOWN"&gt; &lt;Listener className="org.apache.catalina.startup.VersionLoggerListener" /&gt; &lt;Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" /&gt; &lt;Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" /&gt; &lt;Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" /&gt; &lt;Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name="UserDatabase" auth="Container" type="org.apache.catalina.UserDatabase" description="User database that can be updated and saved" factory="org.apache.catalina.users.MemoryUserDatabaseFactory" pathname="conf/tomcat-users.xml" /&gt; &lt;/GlobalNamingResources&gt; &lt;Service name="Catalina"&gt; &lt;Connector port="8060" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; &lt;Connector port="8009" protocol="AJP/1.3" redirectPort="8443" /&gt; &lt;Engine name="Catalina" defaultHost="localhost"&gt; &lt;Realm className="org.apache.catalina.realm.LockOutRealm"&gt; &lt;Realm className="org.apache.catalina.realm.UserDatabaseRealm" resourceName="UserDatabase"/&gt; &lt;/Realm&gt; &lt;Host name="localhost" appBase="webapps" unpackWARs="true" autoDeploy="true"&gt; &lt;Context path="" docBase="/xxx/target/bigdata-web" debug="0" reloadable="true" crossContext="true" allowLinking="true"/&gt; &lt;Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs" prefix="localhost_access_log" suffix=".txt" pattern="%h %l %u %t &amp;quot;%r&amp;quot; %s %b" /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 上边的配置文件，还可以通过下边的一张结构图更清楚的理解： Server标签设置的端口号为8005，shutdown=”SHUTDOWN” ，表示在8005端口监听“SHUTDOWN”命令，如果接收到了就会关闭Tomcat。一个Server有一个Service，当然还可以进行配置。Service左边的内容都属于Container的，Service下边是多个Connector。 Tomcat顶层架构小结 （1）Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container； （2） Server掌管着整个Tomcat的生死大权； （4）Service 是对外提供服务的； （5）Connector用于接受请求并将请求封装成Request和Response来具体处理； （6）Container用于封装和管理Servlet，以及具体处理request请求； 以上是整个Tomcat顶层的分层架构和各个组件之间的关系以及作用，但对于绝大多数的开发人员来说Server和Service对我们来说确实很远，而我们开发中绝大部分进行配置的内容是属于Connector和Container的，所以接下来介绍一下Connector和Container。 Connector和Container的微妙关系 由上述内容我们大致可以知道一个请求发送到Tomcat之后，首先经过Service然后会交给我们的Connector，Connector用于接收请求并将接收的请求封装为Request和Response来具体处理，Request和Response封装完之后再交由Container进行处理，Container处理完请求之后再返回给Connector，最后在由Connector通过Socket将处理的结果返回给客户端，这样整个请求的就处理完了！ Connector最底层使用的是Socket来进行连接的，Request和Response是按照HTTP协议来封装的，所以Connector同时需要实现TCP/IP协议和HTTP协议！ Tomcat既然处理请求，那么肯定需要先接收到这个请求，接收请求这个东西我们首先就需要看一下Connector！ Connector架构分析Connector用于接受请求并将请求封装成Request和Response，然后交给Container进行处理，Container处理完之后在交给Connector返回给客户端。 因此，我们可以把Connector分为四个方面进行理解： （1）Connector如何接受请求的？（2）如何将请求封装成Request和Response的？（3）封装完之后的Request和Response如何交给Container进行处理的？（4）Container处理完之后如何交给Connector并返回给客户端的？ 首先看一下Connector的结构图，如下所示： Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。 其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。 Endpoint 用来处理底层Socket的网络连接 Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的 Endpoint 的抽象实现AbstractEndpoint里面定义的 Acceptor 和 AsyncTimeout 两个内部类和一个 Handler 接口。 Acceptor用于监听请求AsyncTimeout用于检查异步Request的超时Handler 用于处理接收到的Socket，在内部调用Processor进行处理 Processor 用于将Endpoint接收到的Socket封装成Request Processor用来实现HTTP协议的 Adapter 用于将Request交给Container进行具体的处理 Adapter将请求适配到Servlet容器进行具体的处理 至此，我们应该很轻松的回答（1）（2）（3）的问题了，但是（4）还是不知道，那么我们就来看一下Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？ Container架构分析Container用于封装和管理Servlet，以及具体处理Request请求，在Connector内部包含了4个子容器，结构图如下： 4个子容器的作用分别是： Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine； Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点； Context：代表一个应用程序，对应着平时开发的一套程序，或者一个 WEB-INF 目录以及下面的 web.xml 文件； Wrapper：每一Wrapper封装着一个Servlet； 下面找一个Tomcat的文件目录对照一下，如下图所示： Context和Host的区别 Context表示一个应用，Tomcat中默认的配置下 webapps 下的每一个文件夹目录都是一个Context，其中ROOT目录中存放着主应用，其他目录存放着子应用，而整个 webapps 就是一个Host站点。 我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：www.ledouit.com. 如果是Host（webapps）下的其他应用，则可以使用 www.ledouit.com/docs 进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。 看到这里我们知道Container是什么，但是还是不知道Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？别急！下边就开始探讨一下Container是如何进行处理的！ Container如何处理请求的Container处理请求是使用 Pipeline-Valve 管道来处理的！（Valve是阀门之意） Pipeline-Valve 是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的请求返回，再让下一个处理着继续处理。但是！Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点： 每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做 BaseValve，BaseValve是不可删除的； 在上层容器的管道的BaseValve中会调用下层容器的管道。 我们知道 Container 包含四个子容器，而这四个子容器对应的 BaseValve 分别是： StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve Pipeline的处理流程图如下（图D）： （1）Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）； （2）在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。 （3）当执行到StandardWrapperValve的时候，会在 StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！ （4）当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。 Servlet生命周期 Tomcat优化硬件优化单个服务器所能提供CPU、内存、硬盘的性能对处理能力有决定性影响，所以说服务器性能好，Tomcat也不会太差 Tomcat本身优化Tomcat 的自身参数的优化，这块很像 ApacheHttp Server。修改一下 xml 配置文件中的参数，调整最大连接数，超时等. Connector 连接器的配置Tomcat连接器的三种方式： bio、nio 和 apr，三种方式性能差别很大，apr的性能最优， bio 的性能最差。而 Tomcat 7 使用的 Connector 默认就启用的 Apr 协议，但需要系统安装 Apr 库，否则就会使用 bio 方式。 配置文件优化配置文件优化其实就是对 server.xml 优化，可以提大大提高 Tomcat 的处理请求的能力，下面我们来看 Tomcat 容器内的优化。 默认配置下，Tomcat 会为每个连接器创建一个绑定的线程池（最大线程数 200），服务启动时，默认创建了 5 个空闲线程随时等待用户请求。 首先，打开 ${TOMCAT_HOME}/conf/server.xml，搜索【&lt;Executor name=”tomcatThreadPool”】，开启并调整为: 12&lt;Executor name="tomcatThreadPool" namePrefix="catalina-exec-" maxThreads="500" minSpareThreads="20" maxSpareThreads="50" maxIdleTime="60000"/&gt; 注意， Tomcat 7 在开启线程池前，一定要安装好 Apr 库，并可以启用，否则会有错误报出，shutdown.sh 脚本无法关闭进程。 然后，修改&lt;Connector …&gt;节点，增加 executor 属性，搜索【port=”8080”】，调整为: 1234567891011121314&lt;Connector executor="tomcatThreadPool" port="8080" protocol="HTTP/1.1" URIEncoding="UTF-8" connectionTimeout="30000" enableLookups="false" disableUploadTimeout="false" connectionUploadTimeout="150000" acceptCount="300" keepAliveTimeout="120000" maxKeepAliveRequests="1" compression="on" compressionMinSize="2048" compressableMimeType="text/html,text/xml,text/javascript,text/css,text/plain,image/gif,image/jpg,image/png" redirectPort="8443" /&gt; 其中： maxThreads : Tomcat使用线程来处理接收的每个请求，这个值表示 Tomcat 可创建的最大的线程数，默认值是 200 minSpareThreads：最小空闲线程数，Tomcat 启动时的初始化的线程数，表示即使没有人使用也开这么多空线程等待，默认值是 10。 maxSpareThreads：最大备用线程数，一旦创建的线程超过这个值，Tomcat 就会关闭不再需要的 socket 线程。 上边配置的参数，最大线程 500（一般服务器足以），要根据自己的实际情况合理设置，设置越大会耗费内存和 CPU，因为 CPU 疲于线程上下文切换，没有精力提供请求服务了，最小空闲线程数 20，线程最大空闲时间 60 秒，当然允许的最大线程连接数还受制于操作系统的内核参数设置，设置多大要根据自己的需求与环境。当然线程可以配置在“tomcatThreadPool”中，也可以直接配置在“Connector”中，但不可以重复配置。 URIEncoding：指定 Tomcat 容器的 URL 编码格式，语言编码格式这块倒不如其它 WEB 服务器软件配置方便，需要分别指定。 connnectionTimeout： 网络连接超时，单位：毫秒，设置为 0 表示永不超时，这样设置有隐患的。通常可设置为 30000 毫秒，可根据检测实际情况，适当修改。 enableLookups： 是否反查域名，以返回远程主机的主机名，取值为：true 或 false，如果设置为false，则直接返回IP地址，为了提高处理能力，应设置为 false。 disableUploadTimeout：上传时是否使用超时机制。 connectionUploadTimeout：上传超时时间，毕竟文件上传可能需要消耗更多的时间，这个根据你自己的业务需要自己调，以使Servlet有较长的时间来完成它的执行，需要与上一个参数一起配合使用才会生效。 acceptCount：指定当所有可以使用的处理请求的线程数都被使用时，可传入连接请求的最大队列长度，超过这个数的请求将不予处理，默认为100个。 keepAliveTimeout：长连接最大保持时间（毫秒），表示在下次请求过来之前，Tomcat保持该连接多久，默认是使用 connectionTimeout 时间，-1 为不限制超时。 maxKeepAliveRequests：表示在服务器关闭之前，该连接最大支持的请求数。超过该请求数的连接也将被关闭，1表示禁用，-1表示不限制个数，默认100个，一般设置在100~200之间。 compression：是否对响应的数据进行 GZIP 压缩，off：表示禁止压缩；on：表示允许压缩（文本将被压缩）、force：表示所有情况下都进行压缩，默认值为off，压缩数据后可以有效的减少页面的大小，一般可以减小1/3左右，节省带宽。 compressionMinSize：表示压缩响应的最小值，只有当响应报文大小大于这个值的时候才会对报文进行压缩，如果开启了压缩功能，默认值就是2048。 compressableMimeType：压缩类型，指定对哪些类型的文件进行数据压缩。 noCompressionUserAgents=”gozilla, traviata”： 对于以下的浏览器，不启用压缩。 如果已经对代码进行了动静分离，静态页面和图片等数据就不需要Tomcat处理了，那么也就不需要配置在 Tomcat 中配置压缩了。 以上是一些常用的配置参数属性，当然还有好多其它的参数设置，还可以继续深入的优化，HTTP Connector 与 AJP Connector 的参数属性值，可以参考官方文档的详细说明：https://tomcat.apache.org/tomcat-7.0-doc/config/http.html 禁用AJP连接器AJP（Apache JServer Protocol）AJPv13协议是面向包的。WEB服务器和Servlet容器通过TCP连接来交互；为了节省SOCKET创建的昂贵代价，WEB服务器会尝试维护一个永久TCP连接到servlet容器，并且在多个请求和响应周期过程会重用连接。如图:在Nginx+tomcat的架构中，禁用AJP连接器： JVM优化根据服务器物理内容情况配置相关参数优化tomcat性能。 当应用程序需要的内存超出堆的最大值时虚拟机就会提示内存溢出，并且导致应用服务崩溃。因此一般建议堆的最大值设置为可用内存的最大值的80%。 Tomcat默认可以使用的内存为128MB，在较大型的应用项目中，这点内存是不够的，需要调大. Unix下，在文件/bin/catalina.sh的前面，增加如下设置： JAVA_OPTS=’-Xms【初始化内存大小】 -Xmx【可以使用的最大内存】 -XX:PermSize=64M -XX:MaxPermSize=128m’ 需要把几个参数值调大。例如： JAVA_OPTS=’-Xms256m -Xmx512m’ 表示初始化内存为256MB，可以使用的最大内存为512MB。 参数详解: 123456-server 启用jdk 的 server 版；-Xms java虚拟机初始化时的最小内存；-Xmx java虚拟机可使用的最大内存；-XX:PermSize 内存永久代保留区域-XX:MaxPermSize 内存最大永久代保留区域 -Xmn jvm最小内存 原文链接： 四张图带你了解Tomcat系统架构–让面试官颤抖的Tomcat回答系列]]></content>
      <categories>
        <category>Web</category>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Web</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java动态代理]]></title>
    <url>%2F2019%2F11%2F29%2FJava%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是代理我们大家都知道微商代理，简单地说就是代替厂家卖商品，厂家“委托”代理为其销售商品。关于微商代理，首先我们从他们那里买东西时通常不知道背后的厂家究竟是谁，也就是说，“委托者”对我们来说是不可见的;其次，微商代理主要以朋友圈的人为目标客户，这就相当于为厂家做了一次对客户群体的“过滤”。我们把微商代理和厂家进一步抽象，前者可抽象为代理类，后者可抽象为委托类(被代理类)。通过使用代理，通常有两个优点，并且能够分别与我们提到的微商代理的两个特点对应起来： 优点一：可以隐藏委托类的实现; 优点二：可以实现客户与委托类间的解耦，在不修改委托类代码的情况下能够做一些额外的处理。 代理模式代理模式：给某一个对象提供一个代理，并由代理对象来控制对真实对象的访问。代理模式是一种结构型设计模式。 代理模式角色分为 3 种： Subject（抽象主题角色）：定义代理类和真实主题的公共对外方法，也是代理类代理真实主题的方法； RealSubject（真实主题角色）：真正实现业务逻辑的类； Proxy（代理主题角色）：用来代理和封装真实主题； 代理模式的结构比较简单，其核心是代理类，为了让客户端能够一致性地对待真实对象和代理对象，在代理模式中引入了抽象层 如果根据字节码的创建时机来分类，可以分为静态代理和动态代理： 静态代理就是在程序运行前就已经存在代理类的字节码文件，代理类和真实主题角色的关系在运行前就确定了。 动态代理源码是在程序运行期间由JVM根据反射等机制动态的生成，所以在运行前并不存在代理类的字节码文件。 静态代理代码示例我们先通过实例来学习静态代理，然后理解静态代理的缺点，再来学习本文的主角：动态代理 编写一个接口 UserService ，以及该接口的一个实现类 UserServiceImpl： 12345678910111213public interface UserService &#123; public void select(); public void update();&#125;public class UserServiceImpl implements UserService &#123; public void select() &#123; System.out.println("查询 selectById"); &#125; public void update() &#123; System.out.println("更新 update"); &#125;&#125; 我们将通过静态代理对 UserServiceImpl 进行功能增强，在调用 select 和 update 之前记录一些日志。写一个代理类 UserServiceProxy，代理类需要实现 UserService 123456789101112131415161718192021222324public class UserServiceProxy implements UserService &#123; private UserService target; // 被代理的对象 public UserServiceProxy(UserService target) &#123; this.target = target; &#125; public void select() &#123; before(); target.select(); // 这里才实际调用真实主题角色的方法 after(); &#125; public void update() &#123; before(); target.update(); // 这里才实际调用真实主题角色的方法 after(); &#125; private void before() &#123; // 在执行方法之前执行 System.out.printf("log start time [%s] \n", new Date()); &#125; private void after() &#123; // 在执行方法之后执行 System.out.printf("log end time [%s] \n", new Date()); &#125;&#125; 测试; 123456789public class Client1 &#123; public static void main(String[] args) &#123; UserService userServiceImpl = new UserServiceImpl(); UserService proxy = new UserServiceProxy(userServiceImpl); proxy.select(); proxy.update(); &#125;&#125; 输出： 123456log start time [Thu Dec 20 14:13:25 CST 2018] 查询 selectByIdlog end time [Thu Dec 20 14:13:25 CST 2018] log start time [Thu Dec 20 14:13:25 CST 2018] 更新 updatelog end time [Thu Dec 20 14:13:25 CST 2018] 通过静态代理，我们达到了功能增强的目的，而且没有侵入原代码，这是静态代理的一个优点。 静态代理缺点虽然静态代理实现简单，且不侵入原代码，但是，当场景稍微复杂一些的时候，静态代理的缺点也会暴露出来。 1、 当需要代理多个类的时候，由于代理对象要实现与目标对象一致的接口，有两种方式： 只维护一个代理类，由这个代理类实现多个接口，但是这样就导致代理类过于庞大; 新建多个代理类，每个目标对象对应一个代理类，但是这样会产生过多的代理类; 2、 当接口需要增加、删除、修改方法的时候，目标对象与代理类都要同时修改，不易维护。 动态代理生成原理Java虚拟机类加载过程主要分为五个阶段：加载、验证、准备、解析、`初始化。其中加载阶段需要完成以下3件事情： 通过一个类的全限定名来获取定义此类的二进制字节流 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据访问入口 由于虚拟机规范对这3点要求并不具体，所以实际的实现是非常灵活的，关于第1点，获取类的二进制字节流（class字节码）就有很多途径： 从ZIP包获取，这是JAR、EAR、WAR等格式的基础 从网络中获取，典型的应用是 Applet 运行时计算生成，这种场景使用最多的是动态代理技术，在 java.lang.reflect.Proxy 类中，就是用了 ProxyGenerator.generateProxyClass来为特定接口生成形式为 *$Proxy 的代理类的二进制字节流 由其它文件生成，典型应用是JSP，即由JSP文件生成对应的Class类 从数据库中获取等等 所以，动态代理就是想办法，根据接口或目标对象，计算出代理类的字节码，然后再加载到JVM中使用。但是如何计算？如何生成？情况也许比想象的复杂得多，我们需要借助现有的方案。 常见的字节码操作类库 这里有一些介绍：java-source.net/open-source… Apache BCEL (Byte Code Engineering Library)：是Java classworking广泛使用的一种框架，它可以深入到JVM汇编语言进行类操作的细节。 ObjectWeb ASM：是一个Java字节码操作框架。它可以用于直接以二进制形式动态生成stub根类或其他代理类，或者在加载时动态修改类。 CGLIB(Code Generation Library)：是一个功能强大，高性能和高质量的代码生成库，用于扩展JAVA类并在运行时实现接口。 Javassist：是Java的加载时反射系统，它是一个用于在Java中编辑字节码的类库; 它使Java程序能够在运行时定义新类，并在JVM加载之前修改类文件。 为了让生成的代理类与目标对象（真实主题角色）保持一致性，从现在开始将介绍以下两种最常见的方式： 通过实现接口的方式 -&gt; JDK动态代理通过继承类的方式 -&gt; CGLIB动态代理 注：使用ASM对使用者要求比较高，使用Javassist会比较麻烦 JDK动态代理JDK动态代理主要涉及两个类：java.lang.reflect.Proxy 和java.lang.reflect.InvocationHandler，我们仍然通过案例来学习. 编写一个调用逻辑处理器 LogHandler 类，提供日志增强功能，并实现 InvocationHandler 接口；在 LogHandler 中维护一个目标对象，这个对象是被代理的对象（真实主题角色）；在 invoke 方法中编写方法调用的逻辑处理 123456789101112131415161718192021222324252627282930import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.util.Date;public class LogHandler implements InvocationHandler &#123; Object target; // 被代理的对象，实际的方法执行者 public LogHandler(Object target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); // 调用 target 的 method 方法 Object result = method.invoke(target, args); after(); return result; // 返回方法的执行结果 &#125; // 调用invoke方法之前执行 private void before() &#123; System.out.printf("log start time [%s] \n", new Date()); &#125; // 调用invoke方法之后执行 private void after() &#123; System.out.printf("log end time [%s] \n", new Date()); &#125;&#125; 编写客户端，获取动态生成的代理类的对象须借助 Proxy 类的 newProxyInstance 方法: 123456789101112131415161718192021222324252627282930313233343536import proxy.UserService;import proxy.UserServiceImpl;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class UserClientJdkDynamic &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException &#123; // 设置变量可以保存动态代理类，默认名称以 $Proxy0 格式命名 // System.getProperties().setProperty("sun.misc.ProxyGenerator.saveGeneratedFiles", "true"); // 1. 创建被代理的对象，UserService接口的实现类 UserServiceImpl userServiceImpl = new UserServiceImpl(); // 2. 获取对应的 ClassLoader ClassLoader classLoader = userServiceImpl.getClass().getClassLoader(); // 3. 获取所有接口的Class，这里的UserServiceImpl只实现了一个接口UserService， Class[] interfaces = userServiceImpl.getClass().getInterfaces(); // 4. 创建一个将传给代理类的调用请求处理器，处理所有的代理对象上的方法调用 // 这里创建的是一个自定义的日志处理器，须传入实际的执行对象 userServiceImpl InvocationHandler logHandler = new LogHandler(userServiceImpl); /* 5.根据上面提供的信息，创建代理对象 在这个过程中， a.JDK会通过根据传入的参数信息动态地在内存中创建和.class 文件等同的字节码 b.然后根据相应的字节码转换成对应的class， c.然后调用newInstance()创建代理实例 */ UserService proxy = (UserService) Proxy.newProxyInstance(classLoader, interfaces, logHandler); // 调用代理的方法 proxy.select(); proxy.update(); // 6. 保存JDK动态代理生成的代理类，类名保存为 UserServiceProxy ProxyUtils.generateClassFile(userServiceImpl.getClass(), "UserServiceProxy2"); &#125;&#125; 运行结果 123456log start time [Thu Dec 20 16:55:19 CST 2018] 查询 selectByIdlog end time [Thu Dec 20 16:55:19 CST 2018] log start time [Thu Dec 20 16:55:19 CST 2018] 更新 updatelog end time [Thu Dec 20 16:55:19 CST 2018] InvocationHandler和Proxy核心方法介绍java.lang.reflect.InvocationHandlerObject invoke(Object proxy, Method method, Object[] args) 定义了代理对象调用方法时希望执行的动作，用于集中处理在动态代理类对象上的方法调用 java.lang.reflect.Proxystatic InvocationHandler getInvocationHandler(Object proxy) 用于获取指定代理对象所关联的调用处理器 static Class&lt;?&gt; getProxyClass(ClassLoader loader, Class&lt;?&gt;... interfaces) 返回指定接口的代理类 static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 构造实现指定接口的代理类的一个新实例，所有方法会调用给定处理器对象的 invoke 方法 static boolean isProxyClass(Class&lt;?&gt; cl) 返回 cl 是否为一个代理类 代理类JDK自动生成的代理类到底长什么样子呢？借助上面第6步操作，可以把代理类保存下来一探究竟， target 的类路径下找到 UserServiceProxy2.class，双击后IDEA的反编译插件会将该二进制class文件转变成java文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import proxy.UserService;public final class UserServiceProxy2 extends Proxy implements UserService &#123; private static Method m1; private static Method m2; private static Method m4; private static Method m0; private static Method m3; public UserServiceProxy2(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; // 省略... &#125; public final String toString() throws &#123; // 省略... &#125; public final void select() throws &#123; try &#123; super.h.invoke(this, m4, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; // 省略... &#125; public final void update() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName("java.lang.Object").getMethod("equals", Class.forName("java.lang.Object")); m2 = Class.forName("java.lang.Object").getMethod("toString"); m4 = Class.forName("proxy.UserService").getMethod("select"); m0 = Class.forName("java.lang.Object").getMethod("hashCode"); m3 = Class.forName("proxy.UserService").getMethod("update"); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 从 UserServiceProxy 的代码中我们可以发现： UserServiceProxy2 继承了 Proxy 类，并且实现了被代理的所有接口，以及equals、hashCode、toString等方法 由于UserServiceProxy2继承了 Proxy 类，所以每个代理类都会关联一个 InvocationHandler 方法调用处理器 类和所有方法都被 public final 修饰，所以代理类只可被使用，不可以再被继承 每个方法都有一个 Method 对象来描述，Method 对象在static静态代码块中创建，以 m+ 数字 的格式命名 调用方法的时候通过 super.h.invoke(this, m1, (Object[])null) 调用，其中的 super.h.invoke 实际上是在创建代理的时候传递给 Proxy.newProxyInstance 的 LogHandler 对象，它继承 InvocationHandler 类，负责实际的调用处理逻辑 而 LogHandler 的 invoke 方法接收到 method、args 等参数后，进行一些处理，然后通过反射让被代理的对象 target 执行方法 12345678@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); // 调用 target 的 method 方法 Object result = method.invoke(target, args); after(); return result; // 返回方法的执行结果&#125; JDK动态代理执行方法调用的过程简图如下： CGLIB动态代理maven引入CGLIB包，然后编写一个UserDao类，它没有接口，只有两个方法，select() 和 update() 12345678public class UserDao &#123; public void select() &#123; System.out.println("UserDao 查询 selectById"); &#125; public void update() &#123; System.out.println("UserDao 更新 update"); &#125;&#125; 编写一个 LogInterceptor ，继承了 MethodInterceptor，用于方法的拦截回调 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class LogInterceptor implements MethodInterceptor &#123; private Object target; //目标类 public LogInterceptor(Object target) &#123; this.target = target; &#125; /** * 返回代理对象 * 具体实现，暂时先不追究。 */ public Object createProxy() &#123; Enhancer enhancer = new Enhancer(); enhancer.setCallback(this); // 回调函数 拦截器 // 设置代理对象的父类,可以看到代理对象是目标对象的子类。所以这个接口类就可以省略了 enhancer.setSuperclass(this.target.getClass()); return enhancer.create(); &#125; /** * @param object 表示要进行增强的对象 * @param method 表示拦截的方法 * @param objects 数组表示参数列表，基本数据类型需要传入其包装类型，如int--&gt;Integer、long-Long、double--&gt;Double * @param methodProxy 表示对方法的代理，invokeSuper方法表示对被代理对象方法的调用 * * @return 执行结果 * * @throws Throwable */ public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; before(); // 注意这里是调用 invokeSuper 而不是 invoke，否则死循环， // methodProxy.invokesuper执行的是原始类的方法，method.invoke执行的是子类的方法 Object result = methodProxy.invokeSuper(object, objects); after(); return result; &#125; private void before() &#123; System.out.println(String.format("log start time [%s] ", new Date())); &#125; private void after() &#123; System.out.println(String.format("log end time [%s] ", new Date())); &#125;&#125; 测试： 12345678910111213public class CglibClient &#123; public static void main(String[] args)&#123; //目标对象 UserDao userDao = new UserDao(); LogInterceptor interceptor = new LogInterceptor(userDao); // 代理对象，调用cglib系统方法自动生成 // 注意：代理类是目标类的子类。 UserDao proxy = (UserDao) interceptor.createProxy(); proxy.select(); proxy.update(); &#125;&#125; 结果： 123456log start time [Fri Nov 29 10:51:17 CST 2019] UserDao 查询 selectByIdlog end time [Fri Nov 29 10:51:17 CST 2019] log start time [Fri Nov 29 10:51:17 CST 2019] UserDao 更新 updatelog end time [Fri Nov 29 10:51:17 CST 2019] CGLIB 创建动态代理类的模式是： 查找目标类上的所有非final 的public类型的方法定义； 将这些方法的定义转换成字节码； 将组成的字节码转换成相应的代理的class对象； 实现 MethodInterceptor 接口，用来处理对代理类上所有方法的请求 JDK与CGLIB动态代理对比 JDK动态代理：基于Java反射机制实现，必须要实现了接口的业务类才能用这种办法生成代理对象。 Cglib动态代理：基于ASM机制实现，通过生成业务类的子类作为代理类。 JDK动态代理优势 最小化依赖关系，减少依赖意味着简化开发和维护，JDK 本身的支持，可能比 cglib 更加可靠。 平滑进行 JDK 版本升级，而字节码类库通常需要进行更新以保证在新版 Java 上能够使用。 代码实现简单。 基于类似 cglib 框架的优势 无需实现接口，达到代理类无侵入 只操作我们关心的类，而不必为其他相关类增加工作量。 高性能 参考[1] Java 动态代理详解 [2] Java动态代理 [3] Java反射机制详解 [4] 从代理模式再出发！Proxy.newProxyInstance的秘密 [5] JDK动态代理实现原理(jdk8) 附件文中代码： https://github.com/austin-brant/dynamic-proxy-demo]]></content>
      <categories>
        <category>Java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤器(Bloom Filter)]]></title>
    <url>%2F2019%2F11%2F27%2F%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8-Bloom-Filter%2F</url>
    <content type="text"><![CDATA[概念布隆过滤器（英语：Bloom Filter）是1970年由一个叫布隆的小伙子提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。 优点： 空间效率和查询时间都远远超过一般的算法 缺点： 有一定的误识别率和删除困难。 原理布隆过滤器的原理是，当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了： 如果这些点有任何一个0，则被检元素一定不在； 如果都是1，则被检元素很可能在 这就是布隆过滤器的基本思想。 Bloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。 缓存穿透 每次查询都会直接打到DB 简而言之，言而简之就是我们先把我们数据库的数据都加载到我们的过滤器中，比如数据库的id现在有：1、2、3 那就用id：1 为例子, 他在上图中经过三次hash之后，把三次原本值0的地方改为1下次数据进来查询的时候如果id的值是1，那么我就把1拿去三次hash 发现三次hash的值，跟上面的三个位置完全一样，那就能证明过滤器中有1的, 反之如果不一样就说明不存在了 那应用的场景在哪里呢？一般我们都会用来防止缓存击穿 简单来说就是你数据库的id都是1开始然后自增的，那我知道你接口是通过id查询的，我就拿负数去查询，这个时候，会发现缓存里面没这个数据，我又去数据库查也没有，一个请求这样，100个，1000个，10000个呢？你的DB基本上就扛不住了，如果在缓存里面加上这个，是不是就不存在了，你判断没这个数据就不去查了，直接return一个数据为空不就好了嘛。 Bloom Filter缺点bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性 存在误判，可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。如果bloom filter中存储的是黑名单，那么可以通过建立一个白名单来存储可能会误判的元素。 删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。可以采用Counting Bloom Filter Guava本地实现布隆过滤器有许多实现与优化，Guava中就提供了一种Bloom Filter的实现。 在使用bloom filter时，绕不过的两点是预估数据量n 以及 期望的误判率fpp， 在实现bloom filter时，绕不过的两点就是hash函数的选取 以及 bit数组的大小。 对于一个确定的场景，我们预估要存的数据量为n，期望的误判率为fpp，然后需要计算我们需要的Bit数组的大小m，以及hash函数的个数k，并选择hash函数 Bit数组大小选择根据预估数据量n以及误判率fpp，bit数组大小的m的计算方式： 哈希函数选择​由预估数据量n以及bit数组长度m，可以得到一个hash函数的个数k：​​​​哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。 哈希函数个数k、位数组大小m、加入的字符串数量n的关系可以参考Bloom Filters - the math，Bloom_filter-wikipedia 要使用BloomFilter，需要引入guava包： 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;23.0&lt;/version&gt;&lt;/dependency&gt; 测试分两步： 1、往过滤器中放一百万个数，然后去验证这一百万个数是否能通过过滤器 2、另外找一万个数，去检验漏网之鱼的数量 1234567891011121314151617181920212223242526272829303132333435/** * 测试布隆过滤器(可用于redis缓存穿透) * * @author 敖丙 */public class TestBloomFilter &#123; private static int total = 1000000; private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total);// private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.001); public static void main(String[] args) &#123; // 初始化1000000条数据到过滤器中 for (int i = 0; i &lt; total; i++) &#123; bf.put(i); &#125; // 匹配已在过滤器中的值，是否有匹配不上的 for (int i = 0; i &lt; total; i++) &#123; if (!bf.mightContain(i)) &#123; System.out.println("有坏人逃脱了~~~"); &#125; &#125; // 匹配不在过滤器中的10000个值，有多少匹配出来 int count = 0; for (int i = total; i &lt; total + 10000; i++) &#123; if (bf.mightContain(i)) &#123; count++; &#125; &#125; System.out.println("误伤的数量：" + count); &#125;&#125; 运行结果： 运行结果表示，遍历这一百万个在过滤器中的数时，都被识别出来了。一万个不在过滤器中的数，误伤了320个，错误率是0.03左右。 看下BloomFilter的源码： 1234567891011121314151617public static &lt;T&gt; BloomFilter&lt;T&gt; create(Funnel&lt;? super T&gt; funnel, int expectedInsertions) &#123; return create(funnel, (long) expectedInsertions);&#125; public static &lt;T&gt; BloomFilter&lt;T&gt; create(Funnel&lt;? super T&gt; funnel, long expectedInsertions) &#123; return create(funnel, expectedInsertions, 0.03); // FYI, for 3%, we always get 5 hash functions&#125;public static &lt;T&gt; BloomFilter&lt;T&gt; create( Funnel&lt;? super T&gt; funnel, long expectedInsertions, double fpp) &#123; return create(funnel, expectedInsertions, fpp, BloomFilterStrategies.MURMUR128_MITZ_64);&#125;static &lt;T&gt; BloomFilter&lt;T&gt; create( Funnel&lt;? super T&gt; funnel, long expectedInsertions, double fpp, Strategy strategy) &#123; ......&#125; BloomFilter一共四个create方法，不过最终都是走向第四个。看一下每个参数的含义： funnel：数据类型(一般是调用Funnels工具类中的) expectedInsertions：期望插入的值的个数 fpp 错误率(默认值为0.03) strategy 哈希算法(我也不懂啥意思)Bloom Filter的应用 在最后一个create方法中，设置一个断点： 上面的numBits，表示存一百万个int类型数字，需要的位数为7298440，700多万位。理论上存一百万个数，一个int是4字节32位，需要481000000=3200万位。如果使用HashMap去存，按HashMap50%的存储效率，需要6400万位。可以看出BloomFilter的存储空间很小，只有HashMap的1/10左右 上面的numHashFunctions，表示需要5个函数去存这些数字 使用第三个create方法，我们设置下错误率： 1private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.0003); 再运行看看： 此时误伤的数量为4，错误率为0.04%左右。 当错误率设为0.0003时，所需要的位数为16883499，1600万位，需要12个函数和上面对比可以看出，错误率越大，所需空间和时间越小，错误率越小，所需空间和时间越大。 Redis实现RedisBloom实现Redis的布隆过滤器不是原生自带的，而是要通过module加载进去。Redis在4.0的版本中加入了module功能。 RedisBloom github 主页地址： https://github.com/RedisBloom/RedisBloom RedisBloom客户端 主页地址： https://github.com/RedisBloom/JRedisBloom 上面有docker一键启动命令，可以很方便地实验。也有几种主流语言的客户端库的链接，比如Java语言的JReBloom。 RedisBloom模块还实现了布谷鸟过滤器，它算是对布隆过滤器的增强版。解决了布隆过滤器的一些比较明显的缺点，比如：不能删除元素，不能计数等。除此之外，布谷鸟过滤器不用使用多个hash函数，所以查询性能更高。除此之外，在相同的误判率下，布谷鸟过滤器的空间利用率要明显高于布隆，空间上大概能节省40%多。 安装Rebloom插件 1 下载并编译 123$ git clone git://github.com/RedisLabsModules/rebloom$ cd rebloom$ make 将Rebloom加载到Redis中，在redis.conf里面添加 1loadmodule /path/to/rebloom.so 命令操作 123BF.ADD bloom redisBF.EXISTS bloom redisBF.EXISTS bloom nonxist 命令行加载rebloom插件,并且设定每个bloomfilter key的容量和错误率： 123cd /usr/redis-4.0.11# 容量100万, 容错率万分之一./src/redis-server redis.conf --loadmodule /usr/rebloom/rebloom.so INITIAL_SIZE 1000000 ERROR_RATE 0.0001 java-lua版操作(java代码不提供了，自己把脚本执行就行) bloomFilterAdd.lua 123456local bloomName = KEYS[1]local value = KEYS[2]-- bloomFilterlocal result_1 = redis.call('BF.ADD', bloomName, value)return result_1 bloomFilterExist.lua 123456local bloomName = KEYS[1]local value = KEYS[2]-- bloomFilterlocal result_1 = redis.call('BF.EXISTS', bloomName, value)return result_1 Bitmap简单实现-原理版Bitmap不是一个确切的数据类型，而是基于String类型定义的一系列面向位操作的方法。因为String是二进制安全的并且它们的最大长度是512MB， 所以String类型很合适去作为一个2^32 长度的位数组。 位操作方法可以被分为两组：一、对单一位的操作，比如设置某一位为1或0，或者得到这一位的值；二、对一组位的操作，比方说计算一定范围内的1的个数（比如计数） bitmap一个最大的优势是它通常能在存储信息的时候节省大量空间。比方说一个用增量ID来辨别用户的系统，可以用仅仅512MB的空间来标识40亿个用户是否想要接受通知。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class BloomFilter &#123; public static double size = Math.pow(2, 32); //第一次加载的时候将数据加载到redis中 public static void saveDataToRedis(Jedis jedis) &#123; List&lt;String&gt; baseList = new ArrayList&lt;String&gt;(); baseList.add("000"); baseList.add("111"); baseList.add("222"); for (int i = 0; i &lt; baseList.size(); i++) &#123; // 只做一次hash，实际中会计算多个hash位置，降低误差率 long index = hashIndex(baseList.get(i)); jedis.setbit("orderId", index, true); &#125; &#125; private static long hashIndex(String target) &#123; return Math.abs((long) (target.hashCode() % size)); &#125; //判断传入的数据是否在redis中 public static boolean checkEleIsContainBloomFilter(String target, Jedis jedis) &#123; long index = hashIndex(target); System.out.println("index: " + index + " size: " + size); boolean checkResult = jedis.getbit("orderId", index); return checkResult; &#125; public static void main(String[] args) &#123; //获取redis链接 Jedis jedis = new Jedis("xxxx", 6379); jedis.auth("xxxxx"); //第一次运行的时候调用，只运行一次 saveDataToRedis(jedis); //获取比较后的值 System.out.println(checkEleIsContainBloomFilter("000", jedis)); //释放redis链接 jedis.close(); &#125;&#125; 常见应用场景 cerberus在收集监控数据的时候, 有的系统的监控项量会很大, 需要检查一个监控项的名字是否已经被记录到db过了, 如果没有的话就需要写入db. 爬虫过滤已抓到的url就不再抓，可用bloom filter过滤 垃圾邮件过滤。如果用哈希表，每存储一亿个 email地址，就需要 1.6GB的内存（用哈希表实现的具体办法是将每一个 email地址对应成一个八字节的信息指纹，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email地址需要占用十六个字节。一亿个地址大约要 1.6GB，即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB的内存。而Bloom Filter只需要哈希表 1/8到 1/4 的大小就能解决同样的问题。 参考[1] Redis-避免缓存穿透的利器之BloomFilter [2] Java redis 模拟布隆过滤器 [3] redis-分布式布隆过滤器（Bloom Filter）详解（初版）]]></content>
      <categories>
        <category>基础知识</category>
        <category>Bloom</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>布隆过滤器</tag>
        <tag>Bloom</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis过期键处理策略]]></title>
    <url>%2F2019%2F11%2F22%2FRedis%E8%BF%87%E6%9C%9F%E9%94%AE%E5%A4%84%E7%90%86%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[Redis Expire Key基础redis数据库在数据库服务器中使用了 redisDb 数据结构，结构如下： 12345678910typedef struct redisDb &#123; dict *dict; /* 键空间 key space */ dict *expires; /* 过期字典 */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP) */ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ struct evictionPoolEntry *eviction_pool; /* Eviction pool of keys */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */&#125; redisDb; 其中: 键空间(key space):dict字典用来保存数据库中的所有键值对 过期字典(expires):保存数据库中所有键的过期时间，过期时间用UNIX时间戳表示，且值为long long整数 设置过期时间命令 EXPIRE &lt;key&gt; &lt;ttl&gt; 命令用于将键key的过期时间设置为ttl秒之后 PEXPIRE &lt;key&gt; &lt;ttl&gt; 命令用于将键key的过期时间设置为ttl毫秒之后 EXPIREAT &lt;key&gt; &lt;timesramp&gt; 命令用于将key的过期时间设置为timrestamp所指定的秒数时间戳 PEXPIREAT &lt;key&gt; &lt;timesramp&gt; 命令用于将key的过期时间设置为timrestamp所指定的毫秒数时间戳 设置过期时间： 1234redis&gt; set Ccww 5 2 0 ok redis&gt; expire Ccww 5 ok 使用redisDb结构存储数据图表示： 过期时间保存以及判定过期键的判定，其实通过过期字典进行判定，步骤： 检查给定键是否存在于过期字典，如果存在，取出键的过期时间 通过判断当前UNIX时间戳是否大于键的过期时间，是的话，键已过期，相反则键未过期。 过期键删除策略定时删除在设置键的过期时间的同时，创建一个定时任务，当键达到过期时间时，立即执行对键的删除操作. 优点对内存友好，定时删除策略可以保证过期键会尽可能快地被删除，并释放国期间所占用的内存 缺点对cpu时间不友好，在过期键比较多时，删除任务会占用很大一部分cpu时间，在内存不紧张但cpu时间紧张的情况下，将cpu时间用在删除和当前任务无关的过期键上，影响服务器的响应时间和吞吐量 惰性删除放任键过期不管，但在每次从键空间获取键时，都检查取得的键是否过期，如果过期的话，就删除该键，如果没有过期，就返回该键 优点对cpu时间友好，在每次从键空间获取键时进行过期键检查并是否删除，删除目标也仅限当前处理的键，这个策略不会在其他无关的删除任务上花费任何cpu时间。 缺点对内存不友好，过期键过期也可能不会被删除，导致所占的内存也不会释放。甚至可能会出现内存泄露的现象，当存在很多过期键，而这些过期键又没有被访问到，这会可能导致它们会一直保存在内存中，造成内存泄露。 定期删除由于定时删除会占用太多cpu时间，影响服务器的响应时间和吞吐量, 而惰性删除浪费太多内存，有内存泄露的危险，所以出现一种整合和折中这两种策略的定期删除策略: 定期删除策略每隔一段时间执行一次删除过期键操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响; 至于要删除多少过期键，以及要检查多少个数据库，则由算法决定； 定时删除策略有效地减少了因为过期键带来的内存浪费; 定时删除策略难点就是确定删除操作执行的时长和频率： 删除操作执行得太频繁。或者执行时间太长，定期删除策略就会退化成为定时删除策略，以至于将cpu时间过多地消耗在删除过期键上。 相反，则与惰性删除策略一样，出现浪费内存的情况。 所以使用定期删除策略，需要根据服务器的情况合理地设置删除操作的执行时长和执行频率。 过期键删除策略实现 Redis服务器结合惰性删除和定期删除两种策略一起使用，通过这两种策略之间的配合使用，使得服务器可以在合理使用CPU时间和浪费内存空间取得平衡点。 惰性删除策略的实现 Redis在执行任何读写命令时都会先找到这个key，惰性删除就作为一个切入点放在查找key之前，如果key过期了就删除这个key。 12345678910robj *lookupKeyRead(redisDb *db, robj *key) &#123; robj *val; expireIfNeeded(db,key); // 切入点 val = lookupKey(db,key); if (val == NULL) server.stat_keyspace_misses++; else server.stat_keyspace_hits++; return val;&#125; 通过expireIfNeeded函数对输入键进行检查是否删除: 123456789101112131415161718192021222324252627282930int expireIfNeeded(redisDb *db, robj *key) &#123; /* 取出键的过期时间 */ mstime_t when = getExpire(db,key); mstime_t now; /* 没有过期时间返回0*/ if (when &lt; 0) return 0; /* No expire for this key */ /* 服务器loading时*/ if (server.loading) return 0; /* 根据一定规则获取当前时间*/ now = server.lua_caller ? server.lua_time_start : mstime(); /* 如果当前的是从(Slave)服务器 * 0 认为key为无效 * 1 if we think the key is expired at this time. * */ if (server.masterhost != NULL) return now &gt; when; /* key未过期，返回 0 */ if (now &lt;= when) return 0; /* 删除键 */ server.stat_expiredkeys++; propagateExpire(db,key,server.lazyfree_lazy_expire); notifyKeyspaceEvent(NOTIFY_EXPIRED, "expired",key,db-&gt;id); return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) : dbSyncDelete(db,key);&#125; 定期删除策略的实现 key的定期删除会在Redis的周期性执行任务（serverCron，默认每100ms执行一次）中进行，而且是发生Redis的master节点，因为slave节点会通过主节点的DEL命令同步过来达到删除key的目的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465for (j = 0; j &lt; dbs_per_call; j++) &#123; int expired; redisDb *db = server.db+(current_db % server.dbnum); current_db++; /* 超过25％的key已过期，则继续. */ do &#123; unsigned long num, slots; long long now, ttl_sum; int ttl_samples; /* 如果该db没有设置过期key，则继续看下个db*/ if ((num = dictSize(db-&gt;expires)) == 0) &#123; db-&gt;avg_ttl = 0; break; &#125; slots = dictSlots(db-&gt;expires); now = mstime(); /*但少于1%时，需要调整字典大小*/ if (num &amp;&amp; slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (num*100/slots &lt; 1)) break; expired = 0; ttl_sum = 0; ttl_samples = 0; if (num &gt; ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP) num = ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP;// 20 while (num--) &#123; dictEntry *de; long long ttl; if ((de = dictGetRandomKey(db-&gt;expires)) == NULL) break; ttl = dictGetSignedIntegerVal(de)-now; if (activeExpireCycleTryExpire(db,de,now)) expired++; if (ttl &gt; 0) &#123; /* We want the average TTL of keys yet not expired. */ ttl_sum += ttl; ttl_samples++; &#125; &#125; /* Update the average TTL stats for this database. */ if (ttl_samples) &#123; long long avg_ttl = ttl_sum/ttl_samples; /样本获取移动平均值 */ if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl; db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50); &#125; iteration++; if ((iteration &amp; 0xf) == 0) &#123; /* 每迭代16次检查一次 */ long long elapsed = ustime()-start; latencyAddSampleIfNeeded("expire-cycle",elapsed/1000); if (elapsed &gt; timelimit) timelimit_exit = 1; &#125; /* 超过时间限制则退出*/ if (timelimit_exit) return; /* 在当前db中，如果少于25%的key过期，则停止继续删除过期key */ &#125; while (expired &gt; ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP/4);&#125; 依次遍历每个db（默认配置数是16），针对每个db，每次循环随机选择20个（ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP）key判断是否过期，如果一轮所选的key少于25%过期，则终止迭代，此外在迭代过程中如果超过了一定的时间限制则终止过期删除这一过程。 Redis采用的过期策略redis 过期策略是：定期删除 + 惰性删除 假设 redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。所以，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就需要结合惰性删除。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 redis 内存块耗尽了，咋整？ 答案是：走内存淘汰机制 内存淘汰机制redis 内存淘汰机制有以下几个： noeviction 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了 allkeys-lru 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）； allkeys-random 当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊； volatile-lru 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）； volatile-random 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key； volatile-ttl 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除； 设置方式： 1config set maxmemory-policy volatile-lru AOF、RDB和复制功能对过期键的处理RDB 生成RDB文件程序会数据库中的键进行检查，已过期的键不会保存到新创建的RDB文件中 载入RDB文件 主服务载入RDB文件，会对文件中保存的键进行检查会忽略过期键加载未过期键 从服务器载入RDB文件，会加载文件所保存的所有键（过期和未过期的），但从主服务器同步数据同时会清空从服务器的数据库。 AOF AOF文件写入当过期键被删除后，会在AOF文件增加一条DEL命令，来显式地记录该键已被删除。 AOF重写已过期的键不会保存到重写的AOF文件中 复制 当服务器运行在复制模式下时，从服务器的过期键删除动作由主服务器控制的，这样的好处主要为了保持主从服务器数据一致性： 主服务器在删除一个过期键之后，会显式地向所有的从服务器发送一个DEL命令，告知从服务器删除这个过期键； 从服务器在执行客户端发送的读取命令时，即使碰到过期键也不会将过期键删除，不作任何处理。只有接收到主服务器 DEL命令后，从服务器进行删除处理。 参考文档[1] 当遇到美女面试官之如何理解Redis的Expire Key(过期键) [2] Redis的过期策略及内存淘汰机制]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>入门</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty性能优化]]></title>
    <url>%2F2019%2F11%2F20%2FNetty%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[共享Handler代码：https://github.com/austin-brant/netty-im 在使用 Netty 完成了一个 IM 系统的核心功能之后，我们再来仔细看一下服务端 NettyServer.java 1234567891011121314151617serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new AuthHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new CreateGroupRequestHandler()); ch.pipeline().addLast(new JoinGroupRequestHandler()); ch.pipeline().addLast(new QuitGroupRequestHandler()); ch.pipeline().addLast(new ListGroupMembersRequestHandler()); ch.pipeline().addLast(new GroupMessageRequestHandler()); ch.pipeline().addLast(new LogoutRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125; &#125;); 我们看到，服务端的 pipeline 链里面已经有 12 个 handler，其中，与指令相关的 handler 有 9 个。 Netty 在这里的逻辑是：每次有新连接到来的时候，都会调用 ChannelInitializer 的 initChannel() 方法，然后这里 9 个指令相关的 handler 都会被 new 一次。 其实这里的每一个指令 handler，他们内部都是没有成员变量的，也就是说是无状态的，我们完全可以使用单例模式，即调用 pipeline().addLast() 方法的时候，都直接使用单例，不需要每次都 new，提高效率，也避免了创建很多小的对象。 比如，我们拿 LoginRequestHandler 举例，来看一下如何改造 LoginRequestHandler.java 1234567891011// 1. 加上注解标识，表明该 handler 是可以多个 channel 共享的@ChannelHandler.Sharablepublic class LoginRequestHandler extends SimpleChannelInboundHandler&lt;LoginRequestPacket&gt; &#123; // 2. 构造单例 public static final LoginRequestHandler INSTANCE = new LoginRequestHandler(); protected LoginRequestHandler() &#123; &#125;&#125; 首先，非常重要的一点，如果一个 handler 要被多个 channel 进行共享，必须要加上 @ChannelHandler.Sharable 显示地告诉 Netty，这个 handler 是支持多个 channel 共享的，否则会报错，读者可以自行尝试一下。 然后，我们仿照 Netty 源码里面单例模式的写法，构造一个单例模式的类。 接着，我们在服务端的代理里面就可以这么写 NettyServer.java 12345678serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; // ...单例模式，多个 channel 共享同一个 handler ch.pipeline().addLast(LoginRequestHandler.INSTANCE); // ... &#125; &#125;); 这样的话，每来一次新的连接，添加 handler 的时候就不需要每次都 new 了。 压缩 handler - 合并编解码器当我们改造完了之后，我们再来看一下服务端代码 NettyServer.java 1234567891011121314151617serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(MessageRequestHandler.INSTANCE); ch.pipeline().addLast(CreateGroupRequestHandler.INSTANCE); ch.pipeline().addLast(JoinGroupRequestHandler.INSTANCE); ch.pipeline().addLast(QuitGroupRequestHandler.INSTANCE); ch.pipeline().addLast(ListGroupMembersRequestHandler.INSTANCE); ch.pipeline().addLast(GroupMessageRequestHandler.INSTANCE); ch.pipeline().addLast(LogoutRequestHandler.INSTANCE); ch.pipeline().addLast(new PacketEncoder()); &#125; &#125;); pipeline 中第一个 handler - Spliter，我们是无法改动它的，因为他内部实现是与每个 channel 有关，每个 Spliter 需要维持每个 channel 当前读到的数据，也就是说他是有状态的。 而 PacketDecoder 与 PacketEncoder 我们是可以继续改造的，Netty 内部提供了一个类，叫做 MessageToMessageCodec，使用它可以让我们的编解码操作放到一个类里面去实现，首先我们定义一个 PacketCodecHandler: PacketCodecHandler.java 1234567891011121314151617181920@ChannelHandler.Sharablepublic class PacketCodecHandler extends MessageToMessageCodec&lt;ByteBuf, Packet&gt; &#123; public static final PacketCodecHandler INSTANCE = new PacketCodecHandler(); private PacketCodecHandler() &#123; &#125; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf byteBuf, List&lt;Object&gt; out) &#123; out.add(PacketCodec.INSTANCE.decode(byteBuf)); &#125; @Override protected void encode(ChannelHandlerContext ctx, Packet packet, List&lt;Object&gt; out) &#123; ByteBuf byteBuf = ctx.channel().alloc().ioBuffer(); PacketCodec.INSTANCE.encode(byteBuf, packet); out.add(byteBuf); &#125;&#125; 首先，这里 PacketCodecHandler，他是一个无状态的 handler，因此，同样可以使用单例模式来实现。 需要实现 decode() 和 encode() 方法，decode 是将二进制数据 ByteBuf 转换为 java 对象 Packet，而 encode 操作是一个相反的过程，在 encode() 方法里面，我们调用了 channel 的 内存分配器手工分配了 ByteBuf。 接着，PacketDecoder 和 PacketEncoder都可以删掉，我们的 server 端代码就成了如下的样子 12345678910111213141516serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(PacketCodecHandler.INSTANCE); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(MessageRequestHandler.INSTANCE); ch.pipeline().addLast(CreateGroupRequestHandler.INSTANCE); ch.pipeline().addLast(JoinGroupRequestHandler.INSTANCE); ch.pipeline().addLast(QuitGroupRequestHandler.INSTANCE); ch.pipeline().addLast(ListGroupMembersRequestHandler.INSTANCE); ch.pipeline().addLast(GroupMessageRequestHandler.INSTANCE); ch.pipeline().addLast(LogoutRequestHandler.INSTANCE); &#125; &#125;); 可以看到，除了拆包器，所有的 handler 都写成了单例，当然，如果你的 handler 里有与 channel 相关成员变量，那就不要写成单例的，不过，其实所有的状态都可以绑定在 channel 的属性上，依然是可以改造成单例模式。 缩短事件传播路径如果我们再仔细观察我们的服务端代码，发现，我们的 pipeline 链中，绝大部分都是与指令相关的 handler，我们把这些 handler 编排在一起，是为了逻辑简洁，但是随着指令相关的 handler 越来越多，handler 链越来越长，在事件传播过程中性能损耗会被逐渐放大，因为解码器解出来的每个 Packet 对象都要在每个 handler 上经过一遍，我们接下来来看一下如何缩短这个事件传播的路径。 压缩handler - 合并平行handler对我们这个应用程序来说，每次 decode 出来一个指令对象之后，其实只会在一个指令 handler 上进行处理，因此，我们其实可以把这么多的指令 handler 压缩为一个 handler，我们来看一下如何实现 我们定义一个 IMHandler，实现如下： IMHandler.java 1234567891011121314151617181920212223@ChannelHandler.Sharablepublic class IMHandler extends SimpleChannelInboundHandler&lt;Packet&gt; &#123; public static final IMHandler INSTANCE = new IMHandler(); private Map&lt;Byte, SimpleChannelInboundHandler&lt;? extends Packet&gt;&gt; handlerMap; private IMHandler() &#123; handlerMap = new HashMap&lt;&gt;(); handlerMap.put(MESSAGE_REQUEST, MessageRequestHandler.INSTANCE); handlerMap.put(CREATE_GROUP_REQUEST, CreateGroupRequestHandler.INSTANCE); handlerMap.put(JOIN_GROUP_REQUEST, JoinGroupRequestHandler.INSTANCE); handlerMap.put(QUIT_GROUP_REQUEST, QuitGroupRequestHandler.INSTANCE); handlerMap.put(LIST_GROUP_MEMBERS_REQUEST, ListGroupMembersRequestHandler.INSTANCE); handlerMap.put(GROUP_MESSAGE_REQUEST, GroupMessageRequestHandler.INSTANCE); handlerMap.put(LOGOUT_REQUEST, LogoutRequestHandler.INSTANCE); &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, Packet packet) throws Exception &#123; handlerMap.get(packet.getCommand()).channelRead(ctx, packet); &#125;&#125; 首先，IMHandler 是无状态的，依然是可以写成一个单例模式的类。 我们定义一个 map，存放指令到各个指令处理器的映射。 每次回调到 IMHandler 的 channelRead0() 方法的时候，我们通过指令找到具体的 handler，然后调用指令 handler 的 channelRead，他内部会做指令类型转换，最终调用到每个指令 handler 的 channelRead0() 方法。 接下来，我们来看一下，如此压缩之后，我们的服务端代码 NettyServer.java 12345678910serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(PacketCodecHandler.INSTANCE); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(IMHandler.INSTANCE); &#125; &#125;); 可以看到，现在，我们服务端的代码已经变得很清爽了，所有的平行指令处理 handler，我们都压缩到了一个 IMHandler，并且 IMHandler 和指令 handler 均为单例模式，在单机十几万甚至几十万的连接情况下，性能能得到一定程度的提升，创建的对象也大大减少了。 当然，如果你对性能要求没这么高，大可不必搞得这么复杂，还是按照我们前面小节的方式来实现即可，比如，我们的客户端多数情况下是单连接的，其实并不需要搞得如此复杂，还是保持原样即可。 更改事件传播源另外，关于缩短事件传播路径，除了压缩 handler，还有一个就是，如果你的 outBound 类型的 handler 较多，在写数据的时候能用 ctx.writeAndFlush() 就用这个方法。 ctx.writeAndFlush() 事件传播路径 ctx.writeAndFlush() 是从 pipeline 链中的 当前节点开始往前找到第一个 outBound 类型的 handler 把对象往前进行传播，如果这个对象确认不需要经过其他 outBound 类型的 handler 处理，就使用这个方法。 如上图，在某个 inBound 类型的 handler 处理完逻辑之后，调用 ctx.writeAndFlush() 可以直接一口气把对象送到 codec 中编码，然后写出去。 ctx.channel().writeAndFlush() 事件传播路径 ctx.channel().writeAndFlush() 是 从pipeline链中的最后一个outBound类型的 handler开始，把对象往前进行传播，如果你确认当前创建的对象需要经过后面的 outBound 类型的 handler，那么就调用此方法。 如上图，在某个 inBound 类型的 handler 处理完逻辑之后，调用 ctx.channel().writeAndFlush()，对象会从最后一个 outBound 类型的 handler 开始，逐个往前进行传播，路径是要比 ctx.writeAndFlush() 要长的。 由此可见，在我们的应用程序中，当我们没有改造编解码之前，我们必须调用 ctx.channel().writeAndFlush(), 而经过改造之后，我们的编码器（既属于 inBound, 又属于 outBound 类型的 handler）已处于 pipeline 的最前面，因此，可以大胆使用 ctx.writeAndFlush()。 减少阻塞主线程的操作通常我们的应用程序会涉及到数据库或者网络，比如以下这个例子 123456protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush() // 4. balabala 其他的逻辑&#125; 我们看到，在 channelRead0() 这个方法里面，第二个过程中，我们有一些耗时的操作，这个时候，我们万万不能将这个操作直接就在这个方法中处理了，为什么？ 默认情况下，Netty 在启动的时候会开启 2 倍的 cpu 核数个 NIO 线程，而通常情况下我们单机会有几万或者十几万的连接，因此，一条 NIO 线程会管理着几千或几万个连接，在传播事件的过程中，单条 NIO 线程的处理逻辑可以抽象成以下一个步骤，我们就拿 channelRead0() 举例 单个 NIO 线程执行的抽象逻辑 123456List&lt;Channel&gt; channelList = 已有数据可读的 channelfor (Channel channel in channelist) &#123; for (ChannelHandler handler in channel.pipeline()) &#123; handler.channelRead0(); &#125; &#125; 从上面的抽象逻辑中可以看到，其中只要有一个 channel 的一个 handler 中的 channelRead0() 方法阻塞了 NIO 线程，最终都会拖慢绑定在该 NIO 线程上的其他所有的 channel，当然，这里抽象的逻辑已经做了简化，想了解细节可以参考我关于 Netty 中 NIO 线程（即 reactor 线程）文章的分析， 「netty 源码分析之揭开 reactor 线程的面纱（一）」， 「netty 源码分析之揭开 reactor 线程的面纱（二）」， 「netty 源码分析之揭开 reactor 线程的面纱（三）」。 而我们需要怎么做？对于耗时的操作，我们需要把这些耗时的操作丢到我们的业务线程池中去处理，下面是解决方案的伪代码 12345678910ThreadPool threadPool = xxx;protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; threadPool.submit(new Runnable() &#123; // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush() // 4. balabala 其他的逻辑 &#125;)&#125; 这样，就可以避免一些耗时的操作影响 Netty 的 NIO 线程，从而影响其他的 channel。 如何准确统计处理时长通常，应用程序都有统计某个操作响应时间的需求，比如，基于我们上面的栗子，我们会这么做 12345678910protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; threadPool.submit(new Runnable() &#123; long begin = System.currentTimeMillis(); // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush() // 4. balabala 其他的逻辑 long time = System.currentTimeMillis() - begin; &#125;)&#125; 这种做法其实是不推荐的，为什么？ 因为 writeAndFlush() 这个方法如果在非NIO线程（这里，我们其实是在业务线程中调用了该方法）中执行，它是一个异步的操作，调用之后，其实是会立即返回的，剩下的所有的操作，都是 Netty 内部有一个任务队列异步执行的，想了解底层细节的可以阅读一下我的这篇文章 「netty 源码分析之 writeAndFlush 全解析」. 因此，这里的 writeAndFlush() 执行完毕之后，并不能代表相关的逻辑，比如事件传播、编码等逻辑执行完毕，只是表示 Netty 接收了这个任务，那么如何才能判断 writeAndFlush() 执行完毕呢？我们可以这么做 123456789101112131415protected void channelRead0(ChannelHandlerContext ctx, T packet) &#123; threadPool.submit(new Runnable() &#123; long begin = System.currentTimeMillis(); // 1. balabala 一些逻辑 // 2. 数据库或者网络等一些耗时的操作 // 3. writeAndFlush xxx.writeAndFlush().addListener(future -&gt; &#123; if (future.isDone()) &#123; // 4. balabala 其他的逻辑 long time = System.currentTimeMillis() - begin; &#125; &#125;); &#125;)&#125; writeAndFlush() 方法会返回一个 ChannelFuture 对象，我们给这个对象添加一个监听器，然后在回调方法里面，我们可以监听这个方法执行的结果，进而再执行其他逻辑，最后统计耗时，这样统计出来的耗时才是最准确的。 最后，需要提出的一点就是，Netty 里面很多方法都是异步的操作，在业务线程中如果要统计这部分操作的时间，都需要使用监听器回调的方式来统计耗时，如果在 NIO 线程中调用，就不需要这么干。 参考摘自：Netty 入门与实战：仿写微信 IM 即时通讯系统 参考文章：[1] netty 源码分析之揭开 reactor 线程的面纱（一）[2] netty 源码分析之揭开 reactor 线程的面纱（二）[3] netty 源码分析之揭开 reactor 线程的面纱（三）[4] netty 源码分析之 writeAndFlush 全解析]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty心跳与空闲检测]]></title>
    <url>%2F2019%2F11%2F15%2FNetty%E5%BF%83%E8%B7%B3%E4%B8%8E%E7%A9%BA%E9%97%B2%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[代码：https://github.com/austin-brant/netty-im 网络问题下图是网络应用程序普遍会遇到的一个问题：连接假死 连接假死的现象是： 在某一端（服务端或者客户端）看来，底层的 TCP 连接已经断开了，但是应用程序并没有捕获到，因此会认为这条连接仍然是存在的，从 TCP 层面来说，只有收到四次握手数据包或者一个 RST 数据包，连接的状态才表示已断开。 连接假死会带来以下两大问题: 对于服务端来说，因为每条连接都会耗费 cpu 和内存资源，大量假死的连接会逐渐耗光服务器的资源，最终导致性能逐渐下降，程序奔溃。 对于客户端来说，连接假死会造成发送数据超时，影响用户体验。 通常，连接假死由以下几个原因造成的 应用程序出现线程堵塞，无法进行数据的读写。 客户端或者服务端网络相关的设备出现故障，比如网卡，机房故障。 公网丢包。公网环境相对内网而言，非常容易出现丢包，网络抖动等现象，如果在一段时间内用户接入的网络连续出现丢包现象，那么对客户端来说数据一直发送不出去，而服务端也是一直收不到客户端来的数据，连接就一直耗着。 如果我们的应用是面向用户的，那么公网丢包这个问题出现的概率是非常大的。对于内网来说，内网丢包，抖动也是会有一定的概率发生。一旦出现此类问题，客户端和服务端都会受到影响，接下来，我们分别从服务端和客户端的角度来解决连接假死的问题。 服务端空闲检测对于服务端来说，客户端的连接如果出现假死，那么服务端将无法收到客户端的数据，也就是说，如果能一直收到客户端发来的数据，那么可以说明这条连接还是活的，因此，服务端对于连接假死的应对策略就是空闲检测。 何为空闲检测？ 空闲检测指的是每隔一段时间，检测这段时间内是否有数据读写，简化一下，我们的服务端只需要检测一段时间内，是否收到过客户端发来的数据即可，Netty 自带的 IdleStateHandler 就可以实现这个功能。 接下来，我们写一个类继承自 IdleStateHandler，来定义检测到假死连接之后的逻辑。 IMIdleStateHandler.java 1234567891011121314public class IMIdleStateHandler extends IdleStateHandler &#123; private static final int READER_IDLE_TIME = 15; public IMIdleStateHandler() &#123; super(READER_IDLE_TIME, 0, 0, TimeUnit.SECONDS); &#125; @Override protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) &#123; System.out.println(READER_IDLE_TIME + "秒内未读到数据，关闭连接"); ctx.channel().close(); &#125;&#125; 首先，我们观察一下 IMIdleStateHandler 的构造函数，他调用父类 IdleStateHandler的构造函数，有四个参数，其中: 第一个表示读空闲时间，指的是在这段时间内如果没有数据读到，就表示连接假死； 第二个是写空闲时间，指的是 在这段时间如果没有写数据，就表示连接假死； 第三个参数是读写空闲时间，表示在这段时间内如果没有产生数据读或者写，就表示连接假死。写空闲和读写空闲为0，表示我们不关心者两类条件； 最后一个参数表示时间单位。在我们的例子中，表示的是：如果 15 秒内没有读到数据，就表示连接假死。 连接假死之后会回调 channelIdle() 方法，我们这个方法里面打印消息，并手动关闭连接。 接下来，我们把这个 handler 插入到服务端 pipeline 的最前面 NettyServer.java 123456789serverBootstrap .childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; // 空闲检测 ch.pipeline().addLast(new IMIdleStateHandler()); ch.pipeline().addLast(new Spliter()); // ... &#125; &#125;); 为什么要插入到最前面？ 因为如果插入到最后面的话，如果这条连接读到了数据，但是在 inBound 传播的过程中出错了或者数据处理完完毕就不往后传递了（我们的应用程序属于这类），那么最终 IMIdleStateHandler 就不会读到数据，最终导致误判。 服务端的空闲检测时间完毕之后，接下来我们再思考一下，在一段时间之内没有读到客户端的数据，是否一定能判断连接假死呢？并不能，如果在这段时间之内客户端确实是没有发送数据过来，但是连接是 ok 的，那么这个时候服务端也是不能关闭这条连接的，为了防止服务端误判，我们还需要在客户端做点什么。 客户端定时发送心跳服务端在一段时间内没有收到客户端的数据，这个现象产生的原因可以分为以下两种： 连接假死。 非假死状态下确实没有发送数据。 我们只需要排除掉第二种可能性，那么连接自然就是假死的。要排查第二种情况，我们可以在客户端定期发送数据到服务端，通常这个数据包称为心跳数据包，接下来，我们定义一个 handler，定期发送心跳给服务端 HeartBeatTimerHandler.java 12345678910111213141516171819public class HeartBeatTimerHandler extends ChannelInboundHandlerAdapter &#123; private static final int HEARTBEAT_INTERVAL = 5; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; scheduleSendHeartBeat(ctx); super.channelActive(ctx); &#125; private void scheduleSendHeartBeat(ChannelHandlerContext ctx) &#123; ctx.executor().scheduleAtFixedRate(() -&gt; &#123; if (ctx.channel().isActive()) &#123; System.out.println("发送心跳信息 : " + new Date()); ctx.writeAndFlush(new HeartBeatRequestPacket()); &#125; &#125;, 0, HEARTBEAT_INTERVAL, TimeUnit.SECONDS); &#125;&#125; ctx.executor() 返回的是当前的 channel 绑定的 NIO 线程，不理解没关系，只要记住就行，然后，NIO线程有一个方法，schedule()，类似 jdk 的延时任务机制，可以隔一段时间之后执行一个任务，而我们这边是实现了每隔 5 秒，向服务端发送一个心跳数据包，这个时间段通常要比服务端的空闲检测时间的一半要短一些，我们这里直接定义为空闲检测时间的三分之一，主要是为了排除公网偶发的秒级抖动。 实际在生产环境中，我们的发送心跳间隔时间和空闲检测时间可以略长一些，可以设置为几分钟级别，具体应用可以具体对待，没有强制的规定。 我们上面其实解决了服务端的空闲检测问题，服务端这个时候是能够在一定时间段之内关掉假死的连接，释放连接的资源了，但是对于客户端来说，我们也需要检测到假死的连接。 服务端回复心跳与客户端空闲检测客户端的空闲检测其实和服务端一样，依旧是在客户端 pipeline 的最前方插入 IMIdleStateHandler NettyClient.java 12345678bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; public void initChannel(SocketChannel ch) &#123; // 空闲检测 ch.pipeline().addLast(new IMIdleStateHandler()); ch.pipeline().addLast(new Spliter()); // ... &#125; &#125; 然后为了排除是否是因为服务端在非假死状态下确实没有发送数据，服务端也要定期发送心跳给客户端。 而其实在前面我们已经实现了客户端向服务端定期发送心跳，服务端这边其实只要在收到心跳之后回复客户端，给客户端发送一个心跳响应包即可。如果在一段时间之内客户端没有收到服务端发来的数据，也可以判定这条连接为假死状态。 因此，服务端的 pipeline 中需要再加上如下一个 handler - HeartBeatRequestHandler，由于这个 handler 的处理其实是无需登录的，所以，我们将该 handler 放置在 AuthHandler 前面 NettyServer.java 1234567891011serverBootstrap ch.pipeline().addLast(new IMIdleStateHandler()); ch.pipeline().addLast(new Spliter()); ch.pipeline().addLast(PacketCodecHandler.INSTANCE); ch.pipeline().addLast(LoginRequestHandler.INSTANCE); // 加在这里 ch.pipeline().addLast(HeartBeatRequestHandler.INSTANCE); ch.pipeline().addLast(AuthHandler.INSTANCE); ch.pipeline().addLast(IMHandler.INSTANCE); &#125; &#125;); HeartBeatRequestHandler 相应的实现为 1234567891011@ChannelHandler.Sharablepublic class HeartBeatRequestHandler extends SimpleChannelInboundHandler&lt;HeartBeatRequestPacket&gt; &#123; public static final HeartBeatRequestHandler INSTANCE = new HeartBeatRequestHandler(); private HeartBeatRequestHandler() &#123;&#125; @Override protected void channelRead0(ChannelHandlerContext ctx, HeartBeatRequestPacket requestPacket) &#123; ctx.writeAndFlush(new HeartBeatResponsePacket()); &#125;&#125; 实现非常简单，只是简单地回复一个 HeartBeatResponsePacket 数据包。客户端在检测到假死连接之后，断开连接，然后可以有一定的策略去重连，重新登录等等。 总结 首先讨论了连接假死相关的现象以及产生的原因 要处理假死问题首先我们要实现客户端与服务端定期发送心跳，在这里，其实服务端只需要对客户端的定时心跳包进行回复 客户端与服务端如果都需要检测假死，那么直接在 pipeline 的最前方插入一个自定义 IdleStateHandler，在 channelIdle() 方法里面自定义连接假死之后的逻辑 通常空闲检测时间要比发送心跳的时间的两倍要长一些，这也是为了排除偶发的公网抖动，防止误判 参考摘自：Netty 入门与实战：仿写微信 IM 即时通讯系统]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty ChannelHandler生命周期]]></title>
    <url>%2F2019%2F11%2F14%2FNetty-ChannelHandler%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[ChannelHandler有很多回调方法，这些回调方法的执行是有顺序的，而这个执行顺序可以称为 ChannelHandler 的生命周期。 代码：https://github.com/austin-brant/netty-im 生命周期详解对于服务端应用程序来说，我们这里讨论 ChannelHandler 更多的指的是 ChannelInboundHandler，在本小节，我们基于 ChannelInboundHandlerAdapter，自定义了一个 handler: LifeCyCleTestHandler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class LifeCyCleTestHandler extends ChannelInboundHandlerAdapter &#123; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("逻辑处理器被添加：handlerAdded()"); super.handlerAdded(ctx); &#125; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 绑定到线程(NioEventLoop)：channelRegistered()"); super.channelRegistered(ctx); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 准备就绪：channelActive()"); super.channelActive(ctx); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println("channel 有数据可读：channelRead()"); super.channelRead(ctx, msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 某次数据读完：channelReadComplete()"); super.channelReadComplete(ctx); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 被关闭：channelInactive()"); super.channelInactive(ctx); &#125; @Override public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channel 取消线程(NioEventLoop) 的绑定: channelUnregistered()"); super.channelUnregistered(ctx); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("逻辑处理器被移除：handlerRemoved()"); super.handlerRemoved(ctx); &#125;&#125; 上面的代码可以看到，我们在每个方法被调用的时候都会打印一段文字，然后把这个事件继续往下传播。最后，我们把这个 handler 添加到构建的 pipeline 中. 1234567891011// 前面代码略.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; // 添加到第一个 ch.pipeline().addLast(new LifeCyCleTestHandler()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125;&#125;); 接着，我们先运行 NettyServer.java，然后再运行 NettyClient.java，这个时候，Server 端 控制台的输出为 可以看到 ChannelHandler 回调方法的执行顺序为 handlerAdded() -&gt; channelRegistered() -&gt; channelActive() -&gt; channelRead() -&gt; channelReadComplete() 下面，我们来逐个解释一下每个回调方法的含义 handlerAdded()指的是当检测到新连接之后，调用 ch.pipeline().addLast(new LifeCyCleTestHandler()); 之后的回调，表示在当前的 channel 中，已经成功添加了一个 handler 处理器。 channelRegistered()这个回调方法，表示当前的 channel 的所有的逻辑处理已经和某个 NIO 线程建立了绑定关系，类似我们在Netty 是什么？这小节中 BIO 编程中，accept 到新的连接，然后创建一个线程来处理这条连接的读写，只不过 Netty 里面是使用了线程池的方式，只需要从线程池里面去抓一个线程绑定在这个 channel 上即可，这里的 NIO 线程通常指的是 NioEventLoop,不理解没关系，后面我们还会讲到。 channelActive()当 channel 的所有的业务逻辑链准备完毕（也就是说 channel 的 pipeline 中已经添加完所有的 handler）以及绑定好一个 NIO 线程之后，这条连接算是真正激活了，接下来就会回调到此方法。 channelRead()客户端向服务端发来数据，每次都会回调此方法，表示有数据可读。 channelReadComplete()服务端每次读完一次完整的数据之后，回调该方法，表示数据读取完毕。 接下来，我们再把客户端关闭，这个时候对于服务端来说，其实就是 channel 被关闭， ChannelHandler 回调方法的执行顺序为 channelInactive() -&gt; channelUnregistered() -&gt; handlerRemoved() 这里的回调方法的执行顺序是新连接建立时候的逆操作，下面我们还是来解释一下每个方法的含义: channelInactive(): 表面这条连接已经被关闭了，这条连接在 TCP 层面已经不再是 ESTABLISH 状态了 channelUnregistered(): 既然连接已经被关闭，那么与这条连接绑定的线程就不需要对这条连接负责了，这个回调就表明与这条连接对应的 NIO 线程移除掉对这条连接的处理 handlerRemoved()：最后，我们给这条连接上添加的所有的业务逻辑处理器都给移除掉。 最后，我们用一幅图来标识 ChannelHandler 的生命周期 ChannelHandler 生命周期各回调方法用法举例Netty 对于一条连接的在各个不同状态下回调方法的定义还是蛮细致的，这个好处就在于我们能够基于这个机制写出扩展性较好的应用程序。 ChannelInitializer 的实现原理仔细翻看一下我们的服务端启动代码，我们在给新连接定义 handler 的时候，其实只是通过 childHandler() 方法给新连接设置了一个 handler，这个 handler 就是 ChannelInitializer，而在 ChannelInitializer 的 initChannel() 方法里面，我们通过拿到 channel 对应的 pipeline，然后往里面塞 handler NettyServer.java 123456789.childHandler(new ChannelInitializer&lt;NioSocketChannel&gt;() &#123; protected void initChannel(NioSocketChannel ch) &#123; ch.pipeline().addLast(new LifeCyCleTestHandler()); ch.pipeline().addLast(new PacketDecoder()); ch.pipeline().addLast(new LoginRequestHandler()); ch.pipeline().addLast(new MessageRequestHandler()); ch.pipeline().addLast(new PacketEncoder()); &#125;&#125;); 这里的 ChannelInitializer 其实就利用了 Netty 的 handler 生命周期中 channelRegistered() 与 handlerAdded() 两个特性，我们简单翻一翻 ChannelInitializer 这个类的源代码： ChannelInitializer.java 123456789101112131415161718192021222324protected abstract void initChannel(C ch) throws Exception;public final void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; // ... initChannel(ctx); // ...&#125;public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; // ... if (ctx.channel().isRegistered()) &#123; initChannel(ctx); &#125; // ...&#125;private boolean initChannel(ChannelHandlerContext ctx) throws Exception &#123; if (initMap.putIfAbsent(ctx, Boolean.TRUE) == null) &#123; initChannel((C) ctx.channel()); // ... return true; &#125; return false;&#125; 这里，我把非重点代码略去，逻辑会更加清晰一些 ChannelInitializer 定义了一个抽象的方法 initChannel()，这个抽象方法由我们自行实现，我们在服务端启动的流程里面的实现逻辑就是往 pipeline 里面塞我们的 handler 链 handlerAdded() 和 channelRegistered() 方法，都会尝试去调用 initChannel() 方法，initChannel() 使用 putIfAbsent() 来防止 initChannel() 被调用多次 如果你 debug 了 ChannelInitializer 的上述两个方法，你会发现，在 handlerAdded() 方法被调用的时候，channel 其实已经和某个线程绑定上了，所以，就我们的应用程序来说，这里的 channelRegistered() 其实是多余的，那为什么这里还要尝试调用一次呢？ 猜测应该是担心我们自己写了个类继承自 ChannelInitializer，然后覆盖掉了 handlerAdded() 方法，这样即使覆盖掉，在 channelRegistered() 方法里面还有机会再调一次 initChannel()，把我们自定义的 handler 都添加到 pipeline 中去。 handlerAdded() 与 handlerRemoved()这两个方法通常可以用在一些资源的申请和释放 channelActive() 与 channelInActive()对我们的应用程序来说，这两个方法表明的含义是 TCP 连接的建立与释放，通常我们在这两个回调里面统计单机的连接数，channelActive() 被调用，连接数加一，channelInActive() 被调用，连接数减一 另外，我们也可以在 channelActive() 方法中，实现对客户端连接 ip 黑白名单的过滤，具体这里就不展开了 channelRead()我们在前面小节讲拆包粘包原理，服务端根据自定义协议来进行拆包，其实就是在这个方法里面，每次读到一定的数据，都会累加到一个容器里面，然后判断是否能够拆出来一个完整的数据包，如果够的话就拆了之后，往下进行传递，这里就不过多展开，感兴趣的同学可以阅读一下: netty源码分析之拆包器的奥秘 channelReadComplete()每次向客户端写数据的时候，都通过 writeAndFlush() 的方法写并刷新到底层，其实这种方式不是特别高效，我们可以在之前调用 writeAndFlush() 的地方都调用 write() 方法，然后在这个方面里面调用 ctx.channel().flush() 方法，相当于一个批量刷新的机制，当然，如果你对性能要求没那么高，writeAndFlush() 足矣。 参考摘自：Netty 入门与实战：仿写微信 IM 即时通讯系统 参考：netty源码分析之拆包器的奥秘]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty数据传输载体ByteBuf简介]]></title>
    <url>%2F2019%2F11%2F12%2FNetty%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E8%BD%BD%E4%BD%93ByteBuf%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[代码：https://github.com/austin-brant/netty-im ByteBuf结构首先，我们先来了解一下 ByteBuf 的结构 以上就是一个 ByteBuf 的结构图，从上面这幅图可以看到 ByteBuf 是一个字节容器，容器里面的的数据分为三个部分: 第一个部分是已经丢弃的字节，这部分数据是无效的； 第二部分是可读字节，这部分数据是 ByteBuf 的主体数据， 从 ByteBuf 里面读取的数据都来自这一部分;最后一部分的数据是可写字节，所有写到 ByteBuf 的数据都会写到这一段。 第三部分虚线表示的是该 ByteBuf 最多还能扩容多少容量 以上三段内容是被两个指针给划分出来的，从左到右，依次是读指针（readerIndex）、写指针（writerIndex），然后还有一个变量 capacity，表示 ByteBuf 底层内存的总容量; 从 ByteBuf 中每读取一个字节，readerIndex 自增1，ByteBuf 里面总共有 writerIndex-readerIndex 个字节可读, 由此可以推论出当 readerIndex 与 writerIndex 相等的时候，ByteBuf 不可读 写数据是从 writerIndex 指向的部分开始写，每写一个字节，writerIndex 自增1，直到增到 capacity，这个时候，表示 ByteBuf 已经不可写了 ByteBuf 里面其实还有一个参数 maxCapacity，当向 ByteBuf 写数据的时候，如果容量不足，那么这个时候可以进行扩容，直到 capacity 扩容到 maxCapacity，超过 maxCapacity 就会报错 Netty使用ByteBuf这个数据结构可以有效地区分可读数据和可写数据，读写之间相互没有冲突，当然，ByteBuf只是对二进制数据的抽象, Netty关于数据读写只认ByteBuf，下面，我们就来学习一下 ByteBuf 常用的 API. 常用API容量 APIcapacity()表示 ByteBuf 底层占用了多少字节的内存（包括丢弃的字节、可读字节、可写字节），不同的底层实现机制有不同的计算方式，后面我们讲 ByteBuf 的分类的时候会讲到 maxCapacity()表示 ByteBuf 底层最大能够占用多少字节的内存，当向 ByteBuf 中写数据的时候，如果发现容量不足，则进行扩容，直到扩容到 maxCapacity，超过这个数，就抛异常 readableBytes() 与 isReadable()readableBytes() 表示 ByteBuf 当前可读的字节数，它的值等于 writerIndex-readerIndex，如果两者相等，则不可读，isReadable() 方法返回 false writableBytes()、 isWritable() 与 maxWritableBytes()writableBytes() 表示 ByteBuf 当前可写的字节数，它的值等于 capacity - writerIndex，如果两者相等，则表示不可写，isWritable() 返回 false，但是这个时候，并不代表不能往 ByteBuf 中写数据了， 如果发现往 ByteBuf 中写数据写不进去的话，Netty 会自动扩容 ByteBuf，直到扩容到底层的内存大小为 maxCapacity，而 maxWritableBytes() 就表示可写的最大字节数，它的值等于 maxCapacity - writerIndex。 读写指针相关的APIreaderIndex() 与 readerIndex(int)前者表示返回当前的读指针 readerIndex, 后者表示设置读指针 writeIndex() 与 writeIndex(int)前者表示返回当前的写指针 writerIndex, 后者表示设置写指针 markReaderIndex() 与 resetReaderIndex()前者表示把当前的读指针保存起来，后者表示把当前的读指针恢复到之前保存的值，下面两段代码是等价的 12345678910// 代码片段1int readerIndex = buffer.readerIndex();// .. 其他操作buffer.readerIndex(readerIndex);// 代码片段二buffer.markReaderIndex();// .. 其他操作buffer.resetReaderIndex(); 希望大家多多使用代码片段二这种方式，不需要自己定义变量，无论 buffer 当作参数传递到哪里，调用 resetReaderIndex() 都可以恢复到之前的状态，在解析自定义协议的数据包的时候非常常见，推荐大家使用这一对API. markWriterIndex() 与 resetWriterIndex()这一对 API 的作用与上述一对 API 类似，这里不再赘述. 读写API本质上，关于ByteBuf的读写都可以看作从指针开始的地方开始读写数据 writeBytes(byte[] src) 与 buffer.readBytes(byte[] dst)writeBytes() 表示把字节数组 src 里面的数据全部写到 ByteBuf，而 readBytes() 指的是把 ByteBuf 里面的数据全部读取到 dst，这里 dst 字节数组的大小通常等于 readableBytes()，而 src 字节数组大小的长度通常小于等于 writableBytes() writeByte(byte b) 与 buffer.readByte()writeByte() 表示往 ByteBuf 中写一个字节，而 buffer.readByte() 表示从 ByteBuf 中读取一个字节，类似的 API 还有 writeBoolean()、writeChar()、writeShort()、writeInt()、writeLong()、writeFloat()、writeDouble() 与 readBoolean()、readChar()、readShort()、readInt()、readLong()、readFloat()、readDouble() 这里就不一一赘述. 与读写 API 类似的 API 还有 getBytes、getByte() 与 setBytes()、setByte() 系列，唯一的区别就是 get/set 不会改变读写指针，而 read/write 会改变读写指针，这点在解析数据的时候千万要注意 release() 与 retain()由于 Netty 使用了堆外内存，而堆外内存是不被 jvm 直接管理的，也就是说申请到的内存无法被垃圾回收器直接回收，所以需要我们手动回收。有点类似于c语言里面，申请到的内存必须手工释放，否则会造成内存泄漏。 Netty 的 ByteBuf 是通过 引用计数 的方式管理的，如果一个 ByteBuf 没有地方被引用到，需要回收底层内存。默认情况下，当创建完一个 ByteBuf，它的引用为1，然后每次调用 retain() 方法， 它的引用就加一， release() 方法原理是将引用计数减一，减完之后如果发现引用计数为0，则直接回收 ByteBuf 底层的内存。 slice()、duplicate()、copy()这三个方法通常情况会放到一起比较，这三者的返回值都是一个新的 ByteBuf 对象 slice() 方法从原始 ByteBuf 中截取一段，这段数据是从 readerIndex 到 writeIndex，同时，返回的新的 ByteBuf 的最大容量 maxCapacity 为原始 ByteBuf 的 readableBytes() duplicate() 方法把整个 ByteBuf 都截取出来，包括所有的数据，指针信息 slice() 方法与 duplicate() 方法比较： 相同点： 底层内存以及引用计数与原始的 ByteBuf 共享，也就是说经过 slice() 或者 duplicate() 返回的 ByteBuf 调用 write 系列方法都会影响到 原始的 ByteBuf，但是它们都维持着与原始 ByteBuf 相同的内存引用计数和不同的读写指针 不同点：slice() 只截取从 readerIndex 到 writerIndex 之间的数据，它返回的 ByteBuf 的最大容量被限制到 原始 ByteBuf 的 readableBytes(), 而 duplicate() 是把整个 ByteBuf 都与原始的 ByteBuf 共享 slice() 方法与 duplicate() 方法不会拷贝数据，它们只是通过改变读写指针来改变读写的行为，而 copy() 会直接从原始的 ByteBuf 中拷贝所有的信息，包括读写指针以及底层对应的数据，因此， copy() 返回的 ByteBuf 中写数据不会影响到原始的 ByteBuf slice() 和 duplicate() 不会改变 ByteBuf 的引用计数，所以原始的 ByteBuf 调用 release() 之后发现引用计数为零，就开始释放内存，调用这两个方法返回的 ByteBuf 也会被释放，这个时候如果再对它们进行读写，就会报错。因此，我们可以通过调用一次 retain() 方法 来增加引用，表示它们对应的底层的内存多了一次引用，引用计数为2，在释放内存的时候，需要调用两次 release() 方法，将引用计数降到零，才会释放内存 这三个方法均维护着自己的读写指针，与原始的 ByteBuf 的读写指针无关，相互之间不受影响 retainedSlice() 与 retainedDuplicate() 它们的作用是在截取内存片段的同时，增加内存的引用计数，分别与下面两段代码等价 12345// retainedSlice 等价于slice().retain();// retainedDuplicate() 等价于duplicate().retain() 使用到 slice 和 duplicate 方法的时候，千万要理清 内存共享，引用计数共享，读写指针不共享 几个概念，下面举两个常见的易犯错的例子 多次释放 12345678910111213141516171819202122Buffer buffer = xxx;doWith(buffer);// 一次释放buffer.release();public void doWith(Bytebuf buffer) &#123; // ... // 没有增加引用计数 Buffer slice = buffer.slice(); foo(slice);&#125;public void foo(ByteBuf buffer) &#123; // read from buffer // 重复释放 buffer.release();&#125; 这里的 doWith 有的时候是用户自定义的方法，有的时候是 Netty 的回调方法，比如 channelRead() 等等 不释放造成内存泄漏 1234567891011121314151617Buffer buffer = xxx;doWith(buffer);// 引用计数为2，调用 release 方法之后，引用计数为1，无法释放内存 buffer.release();public void doWith(Bytebuf buffer) &#123; // ... // 增加引用计数 Buffer slice = buffer.retainedSlice(); foo(slice); // 没有调用 release&#125;public void foo(ByteBuf buffer) &#123; // read from buffer&#125; 想要避免以上两种情况发生，大家只需要记得一点，在一个函数体里面，只要增加了引用计数（包括 ByteBuf 的创建和手动调用 retain() 方法），就必须调用 release() 方法. 实战了解了以上 API 之后，最后我们使用上述 API 来 写一个简单的 demo。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ByteBufTest &#123; public static void main(String[] args) &#123; ByteBuf buffer = ByteBufAllocator.DEFAULT.buffer(9, 100); print("allocate ByteBuf(9, 100)", buffer); // write 方法改变写指针，写完之后写指针未到 capacity 的时候，buffer 仍然可写 buffer.writeBytes(new byte[]&#123;1, 2, 3, 4&#125;); print("writeBytes(1,2,3,4)", buffer); // write 方法改变写指针，写完之后写指针未到 capacity 的时候，buffer 仍然可写, 写完 int 类型之后，写指针增加4 buffer.writeInt(12); print("writeInt(12)", buffer); // write 方法改变写指针, 写完之后写指针等于 capacity 的时候，buffer 不可写 buffer.writeBytes(new byte[]&#123;5&#125;); print("writeBytes(5)", buffer); // write 方法改变写指针，写的时候发现 buffer 不可写则开始扩容，扩容之后 capacity 随即改变 buffer.writeBytes(new byte[]&#123;6&#125;); print("writeBytes(6)", buffer); // get 方法不改变读写指针 System.out.println("getByte(3) return: " + buffer.getByte(3)); System.out.println("getShort(3) return: " + buffer.getShort(3)); System.out.println("getInt(3) return: " + buffer.getInt(3)); print("getByte()", buffer); // set 方法不改变读写指针 buffer.setByte(buffer.readableBytes() + 1, 0); print("setByte()", buffer); // read 方法改变读指针 byte[] dst = new byte[buffer.readableBytes()]; buffer.readBytes(dst); print("readBytes(" + dst.length + ")", buffer); &#125; private static void print(String action, ByteBuf buffer) &#123; System.out.println("after ===========" + action + "============"); System.out.println("capacity(): " + buffer.capacity()); System.out.println("maxCapacity(): " + buffer.maxCapacity()); System.out.println("readerIndex(): " + buffer.readerIndex()); System.out.println("readableBytes(): " + buffer.readableBytes()); System.out.println("isReadable(): " + buffer.isReadable()); System.out.println("writerIndex(): " + buffer.writerIndex()); System.out.println("writableBytes(): " + buffer.writableBytes()); System.out.println("isWritable(): " + buffer.isWritable()); System.out.println("maxWritableBytes(): " + buffer.maxWritableBytes()); System.out.println(); &#125;&#125; 控制台输出 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990after ===========allocate ByteBuf(9, 100)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 0isReadable(): falsewriterIndex(): 0writableBytes(): 9isWritable(): truemaxWritableBytes(): 100after ===========writeBytes(1,2,3,4)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 4isReadable(): truewriterIndex(): 4writableBytes(): 5isWritable(): truemaxWritableBytes(): 96after ===========writeInt(12)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 8isReadable(): truewriterIndex(): 8writableBytes(): 1isWritable(): truemaxWritableBytes(): 92after ===========writeBytes(5)============capacity(): 9maxCapacity(): 100readerIndex(): 0readableBytes(): 9isReadable(): truewriterIndex(): 9writableBytes(): 0isWritable(): falsemaxWritableBytes(): 91after ===========writeBytes(6)============capacity(): 64maxCapacity(): 100readerIndex(): 0readableBytes(): 10isReadable(): truewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90getByte(3) return: 4getShort(3) return: 1024getInt(3) return: 67108864after ===========getByte()============capacity(): 64maxCapacity(): 100readerIndex(): 0readableBytes(): 10isReadable(): truewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90after ===========setByte()============capacity(): 64maxCapacity(): 100readerIndex(): 0readableBytes(): 10isReadable(): truewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90after ===========readBytes(10)============capacity(): 64maxCapacity(): 100readerIndex(): 10readableBytes(): 0isReadable(): falsewriterIndex(): 10writableBytes(): 54isWritable(): truemaxWritableBytes(): 90 总结 Netty 对二进制数据的抽象 ByteBuf 的结构，本质原理就是，它引用了一段内存，这段内存可以是堆内也可以是堆外的，然后用引用计数来控制这段内存是否需要被释放，使用读写指针来控制对 ByteBuf 的读写，可以理解为是外观模式的一种使用 基于读写指针和容量、最大可扩容容量，衍生出一系列的读写方法，要注意 read/write 与 get/set 的区别 多个 ByteBuf 可以引用同一段内存，通过引用计数来控制内存的释放，遵循谁 retain() 谁 release() 的原则 参考[转载] 数据传输载体 ByteBuf 介绍]]></content>
      <categories>
        <category>Netty</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Netty</tag>
        <tag>网络IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql性能分析工具]]></title>
    <url>%2F2019%2F10%2F09%2FMysql%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Explain简介MySQL 提供了一个 EXPLAIN 命令, 它可以对 SELECT 语句进行分析, 并输出 SELECT 执行的详细信息, 以供开发人员针对性优化. EXPLAIN 命令用法十分简单, 在 SELECT 语句前加上 Explain 就可以了, 例如: 1EXPLAIN SELECT * from user_info WHERE id &lt; 300; 数据准备12345678910111213141516171819202122232425262728293031323334353637CREATE TABLE `user_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `name` varchar(50) NOT NULL DEFAULT &apos;&apos;, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `name_index` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;INSERT INTO user_info (name, age) VALUES (&apos;xys&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;a&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;b&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;c&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;d&apos;, 15);INSERT INTO user_info (name, age) VALUES (&apos;e&apos;, 20);INSERT INTO user_info (name, age) VALUES (&apos;f&apos;, 21);INSERT INTO user_info (name, age) VALUES (&apos;g&apos;, 23);INSERT INTO user_info (name, age) VALUES (&apos;h&apos;, 50);INSERT INTO user_info (name, age) VALUES (&apos;i&apos;, 15);CREATE TABLE `order_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `user_id` bigint(20) DEFAULT NULL, `product_name` varchar(50) NOT NULL DEFAULT &apos;&apos;, `productor` varchar(30) DEFAULT NULL, PRIMARY KEY (`id`), KEY `user_product_detail_index` (`user_id`,`product_name`,`productor`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p2&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (1, &apos;p1&apos;, &apos;DX&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (2, &apos;p5&apos;, &apos;WL&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (3, &apos;p3&apos;, &apos;MA&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (4, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (6, &apos;p1&apos;, &apos;WHH&apos;);INSERT INTO order_info (user_id, product_name, productor) VALUES (9, &apos;p8&apos;, &apos;TE&apos;); EXPLAIN 输出格式1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) 各列的含义如下: id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type: SELECT 查询的类型. table: 表示查询涉及的表或衍生表 partitions: 匹配的分区 type: join 类型 possible_keys: 表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. key: 此次查询中真正使用到的索引. ref: 哪个字段或常数与 key 一起被使用 rows: MySQL查询优化器根据统计信息, 估算SQL要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. filtered: 表示此查询条件所过滤的数据的百分比 extra: 额外的信息 参数说明select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE, 表示此查询不包含 UNION 查询或子查询 PRIMARY, 表示此查询是最外层的查询 UNION, 表示此查询是 UNION 的第二个或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY, 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. 最常见的查询类别应该是 SIMPLE 了, 比如当我们的查询没有子查询, 也没有 UNION 查询时, 那么通常就是 SIMPLE 类型, 例如: 1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) 如果我们使用了 UNION 查询, 那么 EXPLAIN 输出 的结果类似如下: 123456789mysql&gt; EXPLAIN (SELECT * FROM user_info WHERE id IN (1, 2, 3)) UNION (SELECT * FROM user_info WHERE id IN (3, 4, 5));+----+--------------+------------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+-------+---------------+---------+---------+------+------+-------------+| 1 | PRIMARY | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || 2 | UNION | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 3 | Using where || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | |+----+--------------+------------+-------+---------------+---------+---------+------+------+-------------+3 rows in set (0.00 sec) typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. type 常用类型取值有: system: 表中只有一条数据. 这个类型是特殊的 const 类型. const: 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可.例如下面的这个查询, 它使用了主键索引, 因此 type 就是 const 类型的. 1234567mysql&gt; explain select * from user_info where id = 2;+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | |+----+-------------+-----------+-------+---------------+---------+---------+-------+------+-------+1 row in set (0.00 sec) eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. 例如: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id;+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+-------------+| 1 | SIMPLE | order_info | index | user_product_detail_index | user_product_detail_index | 254 | NULL | 9 | Using index || 1 | SIMPLE | user_info | eq_ref | PRIMARY | PRIMARY | 8 | test.order_info.user_id | 1 | |+----+-------------+------------+--------+---------------------------+---------------------------+---------+-------------------------+------+-------------+2 rows in set (0.00 sec) ref: 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询.例如下面这个例子中, 就使用到了 ref 类型的查询: 12345678mysql&gt; EXPLAIN SELECT * FROM user_info, order_info WHERE user_info.id = order_info.user_id AND order_info.user_id = 5; +----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+--------------------------+| 1 | SIMPLE | user_info | const | PRIMARY | PRIMARY | 8 | const | 1 | || 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 9 | const | 1 | Using where; Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+-------+------+--------------------------+2 rows in set (0.01 sec) range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中. 当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. 123456 mysql&gt; EXPLAIN SELECT * FROM user_info WHERE id BETWEEN 2 AND 8;+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | user_info | range | PRIMARY | PRIMARY | 8 | NULL | 7 | Using where |+----+-------------+-----------+-------+---------------+---------+---------+------+------+-------------+ index: 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据. index 类型通常出现在: 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index.例如: 123456 mysql&gt; EXPLAIN SELECT name FROM user_info;+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+| 1 | SIMPLE | user_info | index | NULL | name_index | 152 | NULL | 10 | Using index |+----+-------------+-----------+-------+---------------+------------+---------+------+------+-------------+ 上面的例子中, 我们查询的 name 字段恰好是一个索引, 因此我们直接从索引中获取数据就可以满足查询的需求了, 而不需要查询表中的数据. 因此这样的情况下, type 的值是 index, 并且 Extra 的值是 Using index. ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. 下面是一个全表扫描的例子, 可以看到, 在全表扫描时, possible_keys 和 key 字段都是 NULL, 表示没有使用到索引, 并且 rows 十分巨大, 因此整个查询效率是十分低下的. 123456 mysql&gt; EXPLAIN SELECT age FROM user_info WHERE age = 20;+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+| 1 | SIMPLE | user_info | ALL | NULL | NULL | NULL | NULL | 10 | Using where |+----+-------------+-----------+------+---------------+------+---------+------+------+-------------+ type 类型的性能比较 通常来说, 不同的 type 类型的性能关系如下: ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system ALL 类型因为是全表扫描, 因此在相同的查询条件下, 它是速度最慢的. 而 index 类型的查询虽然不是全表扫描, 但是它扫描了所有的索引, 因此比 ALL 类型的稍快. 后面的几种类型都是利用了索引来查询数据, 因此可以过滤部分或大部分数据, 因此查询效率就比较高了. key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到.key_len 的计算规则如下: 字符串 char(n): n字节长度 varchar(n): 如果是 utf8 编码, 则是 (3n + 2) 字节; 如果是 utf8mb4 编码, 则是 (4n + 2） 字节. 数值类型: TINYINT: 1字节 SMALLINT: 2字节 MEDIUMINT: 3字节 INT: 4字节 BIGINT: 8字节 时间类型 DATE: 3字节 TIMESTAMP: 4字节 DATETIME: 8字节 字段属性: NULL 属性 占用一个字节. 如果一个字段是 NOT NULL 的, 则没有此属性. 我们来举两个简单的例子: 123456mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id &lt; 3 AND product_name = 'p1' AND productor = 'WHH';+----+-------------+------------+-------+---------------------------+---------------------------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------------------+---------------------------+---------+------+------+--------------------------+| 1 | SIMPLE | order_info | range | user_product_detail_index | user_product_detail_index | 9 | NULL | 4 | Using where; Using index |+----+-------------+------------+-------+---------------------------+---------------------------+---------+------+------+--------------------------+ 上面的例子是从表 order_info 中查询指定的内容, 而我们从此表的建表语句中可以知道, 表 order_info 有一个联合索引: KEY user_product_detail_index (user_id, product_name, productor) 不过此查询语句 WHERE user_id &lt; 3 AND product_name = &#39;p1&#39; AND productor = &#39;WHH&#39; 中, 因为先进行 user_id 的范围查询, 而根据 最左前缀匹配 原则, 当遇到范围查询时, 就停止索引的匹配, 因此实际上我们使用到的索引的字段只有 user_id, 因此在 EXPLAIN 中, 显示的 key_len 为 9. 因为 user_id 字段是 BIGINT, 占用 8 字节, 而 NULL 属性占用一个字节, 因此总共是 9 个字节. 若我们将user_id 字段改为 BIGINT(20) NOT NULL DEFAULT ‘0’, 则 key_length 应该是8. 上面因为 最左前缀匹配 原则, 我们的查询仅仅使用到了联合索引的 user_id 字段, 因此效率不算高. 接下来我们来看一下下一个例子: 123456mysql&gt; EXPLAIN SELECT * FROM order_info WHERE user_id = 1 AND product_name = 'p1';+----+-------------+------------+------+---------------------------+---------------------------+---------+-------------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------------------+---------------------------+---------+-------------+------+--------------------------+| 1 | SIMPLE | order_info | ref | user_product_detail_index | user_product_detail_index | 161 | const,const | 2 | Using where; Using index |+----+-------------+------------+------+---------------------------+---------------------------+---------+-------------+------+--------------------------+ 这次的查询中, 我们没有使用到范围查询, key_len 的值为 161. 为什么呢? 因为我们的查询条件 WHERE user_id = 1 AND product_name = &#39;p1&#39; 中, 仅仅使用到了联合索引中的前两个字段, 因此 keyLen(user_id) + keyLen(product_name) = 9 + 50 * 3 + 2 = 161 ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. 例如下面的例子: 123456 mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY product_name;+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-----------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-----------------------------+| 1 | SIMPLE | order_info | index | NULL | user_product_detail_index | 254 | NULL | 9 | Using index; Using filesort |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-----------------------------+ 我们的索引是 KEY user_product_detail_index (user_id, product_name, productor) 但是上面的查询中根据 product_name 来排序, 因此不能使用索引进行优化, 进而会产生 Using filesort.如果我们将排序依据改为 ORDER BY user_id, product_name, 那么就不会出现 Using filesort 了. 例如: 123456mysql&gt; EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name;+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-------------+| 1 | SIMPLE | order_info | index | NULL | user_product_detail_index | 254 | NULL | 9 | Using index |+----+-------------+------------+-------+---------------+---------------------------+---------+------+------+-------------+ Using index“覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化. Profiling简介当我们要对某一条sql的性能进行分析时，可以使用它。 Profiling是从 mysql5.0.3版本以后才开放的。启动profile之后，所有查询包括错误的语句都会记录在内。关闭会话或者set profiling=0 就关闭了。（如果将profiling_history_size参数设置为0，同样具有关闭MySQL的profiling效果。） 此工具可用来查询SQL执行状态，System lock和Table lock 花多少时间等等， 对定位一条语句的I/O、CPU、IPC，Memory消耗 非常重要。(SQL 语句执行所消耗的最大两部分资源就是IO和CPU) 在mysql5.7之后，profile信息将逐渐被废弃，mysql推荐使用performance schema 用法简易流程大概如下： 12345678910111213set profiling=1; //打开分析，默认值为0（off），可以通过设置profiling为1或ON开启 run your sql1; run your sql2; show profiles; //查看sql1, sql2的语句分析 show profile for query 1; //查看sql1的具体分析 show profile ALL for query 1; //查看sql1相关的所有分析【主要看i/o与cpu,下边分析中有各项意义介绍】set profiling=0; //关闭分析 语法： 123SHOW PROFILE [type [, type] ... ] [FOR QUERY n] [LIMIT row_count [OFFSET offset]] type: ALL | BLOCK IO | CONTEXT SWITCHES | CPU | IPC | MEMORY | PAGE FAULTS | SOURCE | SWAPS SOURCE：显示和Source_function,Source_file,Source_line相关的开销信息 注意：profiling被应用在每一个会话中，当前会话关闭后，profiling统计的信息将丢失。 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155mysql&gt; show variables like '%profil%'; ## 查看mysql profiling相关配置+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | OFF | ## 是否开启| profiling_history_size | 15 | ## 记录历史条数+------------------------+-------+3 rows in set (0.00 sec)mysql&gt; set profiling = ON; ## 开启分析Query OK, 0 rows affected (0.01 sec)mysql&gt; show variables like '%profil%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | ON || profiling_history_size | 15 |+------------------------+-------+3 rows in set (0.00 sec)mysql&gt; show tables; +-------------------------------------+| Tables_in_bigdata_new |+-------------------------------------+| TblDataShoppingRecord || TblDataViewRecord || TblEmailNotifyRecord |+-------------------------------------+3 rows in set (0.00 sec)mysql&gt; select * from TblDataShoppingRecord order by id desc limit 1\G;*************************** 1. row *************************** id: 1263 status: off name: mapdata_meta_27 creator: zhangsan createTime: 2019-10-08 15:20:201 row in set (0.00 sec)ERROR: No query specifiedmysql&gt; select * from TblDataShoppingRecord where creator = 'austin' order by id desc limit 1\G;*************************** 1. row *************************** id: 1285 status: off name: metadata_30 creator: austin createTime: 2019-10-08 15:20:201 row in set (0.01 sec)ERROR: No query specifiedmysql&gt; show profiles;+----------+------------+---------------------------------------------------------------------------------------------+| Query_ID | Duration | Query |+----------+------------+---------------------------------------------------------------------------------------------+| 1 | 0.00045400 | show variables like '%profil%' || 2 | 0.00086200 | show tables || 3 | 0.00034900 | select * from TblDataShoppingRecord order by id desc limit 1 || 4 | 0.00040000 | select * from TblDataShoppingRecord where creator = 'austin' order by id desc limit 1 |+----------+------------+---------------------------------------------------------------------------------------------+4 rows in set (0.00 sec)mysql&gt; show profile for query 4;+--------------------------------+----------+| Status | Duration |+--------------------------------+----------+| starting | 0.000039 || checking query cache for query | 0.000052 || Opening tables | 0.000015 || System lock | 0.000005 || Table lock | 0.000037 || init | 0.000031 || optimizing | 0.000009 || statistics | 0.000073 || preparing | 0.000016 || executing | 0.000005 || Sorting result | 0.000006 || Sending data | 0.000050 || end | 0.000007 || query end | 0.000004 || freeing items | 0.000027 || storing result in query cache | 0.000015 || logging slow query | 0.000004 || cleaning up | 0.000005 |+--------------------------------+----------+18 rows in set (0.00 sec)mysql&gt; show profile cpu for query 4;+--------------------------------+----------+----------+------------+| Status | Duration | CPU_user | CPU_system |+--------------------------------+----------+----------+------------+| starting | 0.000039 | 0.000000 | 0.000000 || checking query cache for query | 0.000052 | 0.000000 | 0.000000 || Opening tables | 0.000015 | 0.000000 | 0.000000 || System lock | 0.000005 | 0.000000 | 0.000000 || Table lock | 0.000037 | 0.000000 | 0.000000 || init | 0.000031 | 0.000000 | 0.000000 || optimizing | 0.000009 | 0.000000 | 0.000000 || statistics | 0.000073 | 0.000000 | 0.000000 || preparing | 0.000016 | 0.000000 | 0.000000 || executing | 0.000005 | 0.000000 | 0.000000 || Sorting result | 0.000006 | 0.000000 | 0.000000 || Sending data | 0.000050 | 0.000000 | 0.000000 || end | 0.000007 | 0.000000 | 0.000000 || query end | 0.000004 | 0.000000 | 0.000000 || freeing items | 0.000027 | 0.000000 | 0.000000 || storing result in query cache | 0.000015 | 0.000000 | 0.000000 || logging slow query | 0.000004 | 0.000000 | 0.000000 || cleaning up | 0.000005 | 0.000000 | 0.000000 |+--------------------------------+----------+----------+------------+18 rows in set (0.00 sec)mysql&gt; show profile BLOCK IO, CPU, IPC, MEMORY, SOURCE, SWAPS for query 4; +--------------------------------+----------+----------+------------+--------------+---------------+---------------+-------------------+-------+------------------+---------------+-------------+| Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out | Messages_sent | Messages_received | Swaps | Source_function | Source_file | Source_line |+--------------------------------+----------+----------+------------+--------------+---------------+---------------+-------------------+-------+------------------+---------------+-------------+| starting | 0.000039 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | NULL | NULL | NULL || checking query cache for query | 0.000052 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_cache.cc | 1523 || Opening tables | 0.000015 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_base.cc | 4618 || System lock | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | lock.cc | 260 || Table lock | 0.000037 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | lock.cc | 271 || init | 0.000031 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2528 || optimizing | 0.000009 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 833 || statistics | 0.000073 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 1024 || preparing | 0.000016 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 1046 || executing | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 1780 || Sorting result | 0.000006 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2205 || Sending data | 0.000050 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2338 || end | 0.000007 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_select.cc | 2574 || query end | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 5118 || freeing items | 0.000027 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 6142 || storing result in query cache | 0.000015 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_cache.cc | 985 || logging slow query | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 1735 || cleaning up | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | 0 | 0 | 0 | unknown function | sql_parse.cc | 1703 |+--------------------------------+----------+----------+------------+--------------+---------------+---------------+-------------------+-------+------------------+---------------+-------------+18 rows in set (0.00 sec)mysql&gt; set profiling=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like '%profil%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | OFF || profiling_history_size | 15 |+------------------------+-------+3 rows in set (0.00 sec) 结果参数说明其中标题含义： 12345678910111213141516&quot;Status&quot;: &quot;query end&quot;, 状态&quot;Duration&quot;: &quot;1.751142&quot;, 持续时间&quot;CPU_user&quot;: &quot;0.008999&quot;, cpu用户&quot;CPU_system&quot;: &quot;0.003999&quot;, cpu系统&quot;Context_voluntary&quot;: &quot;98&quot;, 上下文主动切换&quot;Context_involuntary&quot;: &quot;0&quot;, 上下文被动切换&quot;Block_ops_in&quot;: &quot;8&quot;, 阻塞的输入操作&quot;Block_ops_out&quot;: &quot;32&quot;, 阻塞的输出操作&quot;Messages_sent&quot;: &quot;0&quot;, 消息发出&quot;Messages_received&quot;: &quot;0&quot;, 消息接受&quot;Page_faults_major&quot;: &quot;0&quot;, 主分页错误&quot;Page_faults_minor&quot;: &quot;0&quot;, 次分页错误&quot;Swaps&quot;: &quot;0&quot;, 交换次数&quot;Source_function&quot;: &quot;mysql_execute_command&quot;, 源功能&quot;Source_file&quot;: &quot;sql_parse.cc&quot;, 源文件&quot;Source_line&quot;: &quot;4465&quot; 源代码行 不同阶段： 12345678910111213141516starting：开始checking permissions：检查权限Opening tables：打开表init ： 初始化System lock ：系统锁optimizing ： 优化statistics ： 统计preparing ：准备executing ：执行Sending data ：发送数据Sorting result ：排序end ：结束query end ：查询 结束closing tables ： 关闭表 ／去除TMP 表freeing items ： 释放物品cleaning up ：清理 配置显示的记录数由变量“profiling_history_size”控制,默认15条，最大值为100，可以手动设置该参数值。 12345678910111213mysql&gt; set profiling_history_size = 30; Query OK, 0 rows affected (0.01 sec)mysql&gt; show variables like '%profil%';+------------------------+-------+| Variable_name | Value |+------------------------+-------+| have_profiling | YES || profiling | OFF || profiling_history_size | 30 |+------------------------+-------+3 rows in set (0.00 sec) 参考博客[1] MySQL 性能优化神器 Explain 使用分析[2] Mysql分析-profile详解[3] MySQL性能分析工具profiling]]></content>
      <categories>
        <category>Mysql</category>
        <category>性能</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql索引底层原理]]></title>
    <url>%2F2019%2F09%2F28%2FMysql%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[常用引擎InnoDB（聚集索引） InnoDB的存储文件有两个，后缀名分别是.frm 和.idb，其中.frm是表的定义文件，而.idb是索引和数据文件。 InnoDB 中存在表锁和行锁，不过行锁是在命中索引的情况下才会起作用。 InnoDB 支持事务，且支持四种隔离级别（读未提交、读已提交、可重复读、串行化），默认的为可重复读；而在 Oracle 数据库中，只支持串行化级别和读已提交这两种级别，其中默认的为读已提交级别。 MyISAM（非聚集索引） Myisam 的存储文件有三个，后缀名分别是.frm、.MYD、MYI，其中.frm是表的定义文件，.MYD是数据文件，.MYI是索引文件。 Myisam只支持表锁，且不支持事务。Myisam 由于有单独的索引文件，在读取数据方面的性能很高 。 存储结构可以用来优化查询的数据结构有哈希表，完全平衡二叉树，B树，B+树。我们使用最多的是B+树，InnoDB和Myisam都是用 B+Tree 来存储数据的。 数据结构可视化网站： https://www.cs.usfca.edu/~galles/visualization/Algorithms.html Hash为什么很少使用hash? 优点：直接计算下标，查询单一数据非常快。 缺点：如果是进行的范围查询的话，哈希索引就必须全表遍历，获得age数据，然后再依次进行比较，也就是相当于没有索引了。这样就不能优化查询效率了。 B树 d 为大于1的一个正整数，称为B-Tree的度，表示节点的数据存储个数； h 为一个正整数，称为B-Tree的高度； 每个非叶子节点由n-1个key和n个指针组成，其中 d &lt;= n &lt;= 2d； 每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null； 所有叶节点具有相同的深度，等于树高h; key和指针互相间隔，节点两端是指针; 一个节点中的key从左到右非递减排列; 所有节点组成树结构; 每个指针要么为null，要么指向另外一个节点; 关于B-Tree有一系列有趣的性质，例如一个度为 d 的B-Tree，设其索引 N 个key，则其树高h的上限为 logd((N+1)/2) ，检索一个key，其查找节点个数的线性复杂度为 O(logdN) 。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 B+树B-Tree有许多变种，其中最常见的是B+Tree。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1; 非叶子节点不存储data，只存储key，可节省空间，增大 度; 叶子节点不存储指针; 带有顺序访问指针，提高了区间访问性能; 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。 为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的最小逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 mysql页文件配置查看mysql页文件大小 SHOW GLOBAL STATUS like ‘Innodb_page_size’; 为什么Mysql页文件默认16kb就够了呢 假设我们一行数据大小为1K,那么一页就能存16条数据，也就是一个叶子节点能存16条数据; 再看非叶子节点，假设主键ID为bigint类型, 那么长度为8B，指针大小在Innodb源码中为6B，-共就是14B,那么一页里就可以存储16K/14=1170个(主键+指针)，那么： 一颗高度 为2的B+树能存储的数据为: 1170 * 16 = 18720条 一 颗高度为3的B+树能存储的数据为: 1170 * 1170 * 16 = 21902400 (千万级条)。 所以在InnoDB中B+树高度一般为1-3层， 它就能满足千万级的数据存储。在查找数据时一次页的查找代表一次I/O, 所以通过主键索引查询通常只需要1-3次IO操作即可查找到数据。所以也就回答了我们的问题，1 页=16k这么设置是比较合适的，是适用大多数的企业的，当然这个值是可以修改的，所以也能根据业务的时间情况进行调整。 B-/+Tree索引的性能分析一般以使用磁盘I/O次数评价索引结构的优劣。 先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。 为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。B+Tree之所以更适合外存索引，原因和内节点度d有关。从上面分析可以看到，d越大索引的性能越好，而度的上限取决于节点内key和data的大小： 1dmax = floor(pagesize/(keysize+datasize+pointsize)) floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的度，拥有更好的性能。 MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的。 MyISAM索引实现 索引文件仅仅保存数据记录的地址，索引文件和数据文件是分离的； 主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复； 索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 数据文件本身就是索引文件，都存储在后缀为.idb的文件中； 叶节点data域保存了完整的数据记录, 而不是行地址； 因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形； 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录； InnodeDB主键选择与插入优化基于以上特点就很容易理解为什么不建议使用过长的字段作为主键？而且推荐使用整形自增主键？ 所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大； InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择； 联合索引有如下数据表： 1234567CREATE TABLE People ( last_name varchar(50) not null, first_name varchar(50) not null, dob date not null, gender enum('m','f') not null, key(last_name,first_name,dob)) 这个建表语句在last_name、first_name、dob列上建立了一个联合索引，下图展示了该索引的存储结构。 可以看到，联合索引中的索引项会先根据第一个索引列进行排序，第一个索引列相同的情况下，会再按照第二个索引列进行排序，依次类推。根据这种存储特点，B-Tree索引对如下类型的查找有效： 全值匹配：查找条件和索引中的所有列相匹配 匹配最左前缀：查找条件只有索引中的第一列 匹配列前缀：只匹配某一列值的开头部分。这里并不一定只能匹配第一个索引列的前缀。例如在确定第一个索引列的值时，也可以在第二个索引列上匹配列前缀。在上面例子中，对于查找姓为Allen，名为J开头的人，也可以应用到索引。 匹配范围值，或者精确匹配某一列并范围匹配另外一列：例如查找姓在Allen和Barrymore之间的人，或者查找姓为Allen，名字在某一个范围内的人。 只访问索引的查询，即要查询的值在索引中都包含，只需要访问索引就行了，无需访问数据行。这种索引被称作覆盖索引。 对于上面列出的查询类型，索引除了可以用来查询外，还可以用来排序。 下面是B-Tree索引的一些限制： 如果不是从索引的最左列开始查找，则无法使用索引。例如直接查找名字为Bill的人，或查找某个生日的人都无法应用到上面的索引，因为都跳过了索引的第一个列。此外查找姓以某个字母结尾的人，也无法使用到上面的索引。 不能在中间跳过索引中的某个列，例如不能查找姓为Smith，生日为某个特定日期的类。这样的查询只能使用到索引的第一列。 如果查询中有某个列的范围查询，则该列右边的所有列都无法使用索引优化查找。例如有查询WHERE last_name=’Smith’ AND first_name LIKE ‘J%’ AND dob=’1976-12-23’，这个查询只能使用到索引的前两列，而不能使用整个索引。 通过上面列出的这些条件，可见对于一个B-TREE联合索引，索引列的顺序非常重要。 InnoDB中有一个功能叫“自适应哈希索引”，当InnoDB注意到某些索引值使用的非常频繁时，会在B-Tree索引之上再建立一层哈希索引，以加速查找效率。这是完全自动的内部行为，用户无法干预。 索引查询多列索引当出现对多个索引列做相交(AND)操作的查询时，代表需要一个包含所有相关列的联合索引，而不是多个独立的单列索引。 在MySql官方提供的示例数据库sakila中，表film_actor在字段film_id和actor_id上各有一个单列索引，对于下面这条查询语句，这两个单列索引都不是很好的选择： 1SELECT film_id,actor_id FROM film_actor WHERE actor_id=1 OR film_id=1; 在老的MySql版本中，这个查询会使用全表扫描。但在MySql5.0之后，查询能够同时使用这两个单列索引进行扫描，然后将结果合并，相当于转换成下面这条查询： 123SELECT film_id,actor_id FROM film_actor WHERE actor_id=1 UNIONSELECT film_id,actor_id FROM film_actor WHERE film_id=1; 在MySql5.7中，执行上面查询的执行计划如下图所示： 从执行计划的type字段可以看到，MySql同时使用了两个索引，并将各自的查询结果合并。并且Extra字段描述了使用索引的详细信息。 虽然MySql在背后对查询进行了优化，使其可以同时利用两个单列索引。但是这需要耗费大量的CPU和内存资源，所以直接将查询改写成UNION的方式会更好。像这种两个列上都有索引的情况，用union代替or会得到更好的效果(注意要求两个列上都建有索引，如果没有索引，用union代替or反而会降低效率)。 如果在EXPLAIN中看到有索引合并，那就应该好好检查一下查询和表的结构，看看是不是已经是最优的。 覆盖索引如果一个索引包含所有需要查询的字段，就称之为“覆盖索引”。由于在索引的叶子节点中已经包含了要查询的全部数据，所以就可以从索引中直接获取查询结果，而没必要再回表查询。 索引一般远远小于数据行的大小，如果只需要访问索引，就会极大减少数据访问量。而且索引是按照顺序存储，所以在进行范围查询时会比随机从磁盘读取每一条数据的I/O要少的多。由此看出，覆盖索引能够极大的提高查询性能。 sakila数据库中包含了由store_id和film_id组成的一个联合索引，如下图所示： 如果只查询store_id和film_id这两列，就可以使用这个索引做覆盖索引。 EXPLAIN的Extra列如果是Using index，则代表这个查询使用到了覆盖索引。注意type字段和是否为覆盖索引毫无关系。 参考博客[1] MySQL索引背后的数据结构及算法原理 [2] MySql索引]]></content>
      <categories>
        <category>Mysql</category>
        <category>索引</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>索引</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka快速入门]]></title>
    <url>%2F2019%2F09%2F23%2FKafka%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[介绍Apache Kafka® 是 一个分布式流处理平台.我们知道流处理平台有以下三种特性: 可以让你发布和订阅流式的记录。这一方面与消息队列或者企业消息系统类似。 可以储存流式的记录，并且有较好的容错性。 可以在流式记录产生时就进行处理。 Kafka适合什么样的场景? 它可以用于两大类别的应用: 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue) 构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化) 首先是一些概念: Kafka作为一个集群，运行在一台或者多台服务器上. Kafka 通过 topic 对存储的流数据进行分类。 每条记录中包含一个key，一个value和一个timestamp（时间戳）。 Topics和日志让我们首先深入了解下Kafka的核心概念:提供一串流式的记录— topic 。 Topic 就是数据主题，是数据记录发布的地方,可以用来区分业务系统。Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。 对于每一个topic， Kafka集群都会维持一个分区日志，如下所示： 每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的commit log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，offset用来唯一的标识分区中每一条记录。 Kafka 集群保留所有发布的记录—无论他们是否已被消费—并通过一个可配置的参数——保留期限来控制. 举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。Kafka的性能和数据大小无关，所以长时间存储数据没有什么问题. 日志中的 partition（分区）有以下几个用途。 第一，当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。 第二，可以作为并行的单元集 分布式日志的分区partition （分布）在Kafka集群的服务器上。每个服务器在处理数据和请求时，共享这些分区。每一个分区都会在已配置的服务器上进行备份，确保容错性. 每个分区都有一台 server 作为 “leader”，零台或者多台server作为 follwers 。leader server 处理一切对 partition （分区）的读写请求，而follwers只需被动的同步leader上的数据。当leader宕机了，followers 中的一台服务器会自动成为新的 leader。每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的。 消费者如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例. 如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程. 在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。 Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。 Kafka的 topic 被分割成了一组完全有序的 partition，其中每一个 partition 在任意给定的时间内只能被每个订阅了这个 topic 的 consumer 组中的一个 consumer 消费。这意味着 partition 中 每一个 consumer 的位置仅仅是一个数字，即下一条要消费的消息的offset。这使得被消费的消息的状态信息相当少，每个 partition 只需要一个数字。这个状态信息还可以作为周期性的 checkpoint。这以非常低的代价实现了和消息确认机制等同的效果。 这种方式还有一个附加的好处。consumer 可以回退到之前的 offset 来再次消费之前的数据，这个操作违反了队列的基本原则，但事实证明对大多数 consumer 来说这是一个必不可少的特性。 例如，如果 consumer 的代码有 bug，并且在 bug 被发现前已经有一部分数据被消费了， 那么 consumer 可以在 bug 修复后通过回退到之前的 offset 来再次消费这些数据。 持久化Kafka 对消息的存储和缓存严重依赖于文件系统。现代操作系统提供了 read-ahead 和 write-behind 技术，read-ahead 是以大的 data block 为单位预先读取数据，而 write-behind 是将多个小型的逻辑写合并成一次大型的物理磁盘写入。关于该问题的进一步讨论可以参考 ACM Queue article，他们发现实际上顺序磁盘访问在某些情况下比随机内存访问还要快！ 这里给出了一个非常简单的设计：相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将数据 flush 到文件系统，我们把这个过程倒过来。所有数据一开始就被写入到文件系统的持久化日志中，而不用在 cache 空间不足的时候 flush 到磁盘。实际上，这表明数据被转移到了内核的 pagecache 中。 使用文件系统和 pagecache 显得更有优势–我们可以通过自动访问所有空闲内存将可用缓存的容量至少翻倍，并且通过存储紧凑的字节结构而不是独立的对象，有望将缓存容量再翻一番。 这样使得32GB的机器缓存容量可以达到28-30GB,并且不会产生额外的 GC 负担。此外，即使服务重新启动，缓存依旧可用，而 in-process cache 则需要在内存中重建(重建一个10GB的缓存可能需要10分钟)，否则进程就要从 cold cache 的状态开始(这意味着进程最初的性能表现十分糟糕)。 这同时也极大的简化了代码，因为所有保持 cache 和文件系统之间一致性的逻辑现在都被放到了 OS 中，这样做比一次性的进程内缓存更准确、更高效。如果你的磁盘使用更倾向于顺序读取，那么 read-ahead 可以有效的使用每次从磁盘中读取到的有用数据预先填充 cache。 持久化队列可以建立在简单的读取和向文件后追加两种操作之上，这和日志解决方案相同。这种架构的优点在于所有的操作复杂度都是O(1)，而且读操作不会阻塞写操作，读操作之间也不会互相影响。这有着明显的性能优势，在不产生任何性能损失的情况下能够访问几乎无限的硬盘空间，这意味着我们可以提供一些其它消息系统不常见的特性。例如：在 Kafka 中，我们可以让消息保留相对较长的一段时间(比如一周)，而不是试图在被消费后立即删除。正如我们后面将要提到的，这给消费者带来了很大的灵活性。 优化 减少数据拷贝使用 sendfile 方法，可以允许操作系统将数据从 pagecache 直接发送到网络，这样避免重新复制数据。所以这种优化方式，只需要最后一步的copy操作，将数据复制到 NIC 缓冲区。pagecache 和 sendfile 的组合使用意味着，在一个kafka集群中，大多数 consumer 消费时，您将看不到磁盘上的读取活动，因为数据将完全由缓存提供。 端到端批量压缩Kafka 以高效的批处理格式支持一批消息可以压缩在一起发送到服务器。这批消息将以压缩格式写入，并且在日志中保持压缩，只会在 consumer 消费时解压缩。 生产者负载均衡生产者直接发送数据到主分区的服务器上，不需要经过任何中间路由。为了让生产者实现这个功能，所有的 kafka 服务器节点都能响应这样的元数据请求： 哪些服务器是活着的，主题的哪些分区是主分区，分配在哪个服务器上，这样生产者就能适当地直接发送它的请求到服务器上。 客户端控制消息发送数据到哪个分区，这个可以实现随机的负载均衡方式, 或者使用一些特定语义的分区函数。 我们有提供特定分区的接口让用于根据指定的键值进行hash分区(当然也有选项可以重写分区函数)，例如，如果使用用户ID作为key，则用户相关的所有数据都会被分发到同一个分区上。 异步发送批处理是提升性能的一个主要驱动，为了允许批量处理，kafka 生产者会尝试在内存中汇总数据，并用一次请求批次提交信息。 批处理，不仅仅可以配置指定的消息数量，也可以指定等待特定的延迟时间(如64k 或10ms)，这允许汇总更多的数据后再发送，在服务器端也会减少更多的IO操作。 该缓冲是可配置的，并给出了一个机制，通过权衡少量额外的延迟时间获取更好的吞吐量。 消息交互语义Kafka可以提供的消息交付语义保证有多种： At most once——消息可能会丢失但绝不重传。 At least once——消息可以重传但绝不丢失。在 0.11.0.0 之前的版本中, 如果 producer 没有收到表明消息已经被提交的响应, 那么 producer 除了将消息重传之外别无选择。 这里提供的是 at-least-once 的消息交付语义，因为如果最初的请求事实上执行成功了，那么重传过程中该消息就会被再次写入到 log 当中。 Exactly once——这正是人们想要的, 每一条消息只被传递一次. 从 0.11.0.0 版本开始，Kafka producer新增了幂等性的传递选项，该选项保证重传不会在 log 中产生重复条目。 为实现这个目的, broker 给每个 producer 都分配了一个 ID ，并且 producer 给每条被发送的消息分配了一个序列号来避免产生重复的消息。 同样也是从 0.11.0.0 版本开始, producer 新增了使用类似事务性的语义将消息发送到多个 topic partition 的功能： 也就是说，要么所有的消息都被成功的写入到了 log，要么一个都没写进去。这种语义的主要应用场景就是 Kafka topic 之间的 exactly-once 的数据传递。 并非所有使用场景都需要这么强的保证。对于延迟敏感的应用场景，我们允许生产者指定它需要的持久性级别。如果 producer 指定了它想要等待消息被提交，则可以使用10ms的量级。然而， producer 也可以指定它想要完全异步地执行发送，或者它只想等待直到 leader 节点拥有该消息（follower 节点有没有无所谓）。 现在让我们从 consumer 的视角来描述语义。 假设 consumer 要读取一些消息——它有几个处理消息和更新位置的选项。 Consumer 可以先读取消息，然后将它的位置保存到 log 中，最后再对消息进行处理。在这种情况下，消费者进程可能会在保存其位置之后，带还没有保存消息处理的输出之前发生崩溃。而在这种情况下，即使在此位置之前的一些消息没有被处理，接管处理的进程将从保存的位置开始。在 consumer 发生故障的情况下，这对应于“at-most-once”的语义，可能会有消息得不到处理。 Consumer 可以先读取消息，然后处理消息，最后再保存它的位置。在这种情况下，消费者进程可能会在处理了消息之后，但还没有保存位置之前发生崩溃。而在这种情况下，当新的进程接管后，它最初收到的一部分消息都已经被处理过了。在 consumer 发生故障的情况下，这对应于“at-least-once”的语义。 在许多应用场景中，消息都设有一个主键，所以更新操作是幂等的（相同的消息接收两次时，第二次写入会覆盖掉第一次写入的记录）。 多副本创建副本的单位是 topic 的 partition ，正常情况下， 每个分区都有一个 leader 和零或多个 followers 。 总的副本数是包含 leader 的总和。 所有的读写操作都由 leader 处理，一般 partition 的数量都比 broker 的数量多的多，各分区的 leader 均 匀的分布在brokers 中。所有的 followers 节点都同步 leader 节点的日志，日志中的消息和偏移量都和 leader 中的一致。（当然, 在任何给定时间, leader 节点的日志末尾时可能有几个消息尚未被备份完成）。 Followers 节点就像普通的 consumer 那样从 leader 节点那里拉取消息并保存在自己的日志文件中。Followers 节点可以从 leader 节点那里批量拉取消息日志到自己的日志文件中。 与大多数分布式系统一样，自动处理故障需要精确定义节点 “alive” 的概念。Kafka 判断节点是否存活有两种方式。 节点必须可以维护和 ZooKeeper 的连接，Zookeeper 通过心跳机制检查每个节点的连接。 如果节点是个 follower ，它必须能及时的同步 leader 的写操作，并且延时不能太久。 我们认为满足这两个条件的节点处于 “in sync” 状态，区别于 “alive” 和 “failed” 。 Leader会追踪所有 “in sync” 的节点。如果有节点挂掉了, 或是写超时, 或是心跳超时, leader 就会把它从同步副本列表中移除。 同步超时和写超时的时间由 replica.lag.time.max.ms 配置确定。 在所有时间里，Kafka 保证只要有至少一个同步中的节点存活，提交的消息就不会丢失。 Kafka分配Replica的算法如下： 将所有Broker（假设共n个Broker）和待分配的Partition排序 将第i个Partition分配到第（i mod n）个Broker上 将第i个Partition的第j个Replica分配到第（(i + j) mod n）个Broker上 可用性和持久性保证向 Kafka 写数据时，producers 设置 ack 是否提交完成， 0：不等待broker返回确认消息, 1: leader保存成功返回或, -1(all): 所有备份都保存成功返回. 请注意. 设置 “ack = all” 并不能保证所有的副本都写入了消息。默认情况下，当 acks = all 时，只要 ISR 副本同步完成，就会返回消息已经写入。例如，一个 topic 仅仅设置了两个副本，那么只有一个 ISR 副本，那么当设置acks = all时返回写入成功时，剩下了的那个副本数据也可能数据没有写入。 尽管这确保了分区的最大可用性，但是对于偏好数据持久性而不是可用性的一些用户，可能不想用这种策略，因此，我们提供了两个topic 配置，可用于优先配置消息数据持久性： 禁用 unclean leader 选举机制 - 如果所有的备份节点都挂了,分区数据就会不可用，直到最近的 leader 恢复正常。这种策略优先于数据丢失的风险， 参看上一节的 unclean leader 选举机制。 指定最小的 ISR 集合大小，只有当 ISR 的大小大于最小值，分区才能接受写入操作，以防止仅写入单个备份的消息丢失造成消息不可用的情况，这个设置只有在生产者使用 acks = all 的情况下才会生效，这至少保证消息被 ISR 副本写入。此设置是一致性和可用性 之间的折衷，对于设置更大的最小ISR大小保证了更好的一致性，因为它保证将消息被写入了更多的备份，减少了消息丢失的可能性。但是，这会降低可用性，因为如果 ISR 副本的数量低于最小阈值，那么分区将无法写入。 高可用ISRKafka 动态维护了一个同步状态的备份的集合 （a set of in-sync replicas）， 简称 ISR ，在这个集合中的节点都是和 leader 保持高度一致的，只有这个集合的成员才 有资格被选举为 leader，一条消息必须被这个集合 所有 节点读取并追加到日志中了，这条消息才能视为提交。这个 ISR 集合发生变化会在 ZooKeeper 持久化，正因为如此，这个集合中的任何一个节点都有资格被选为 leader 。因为 ISR 模型和 f+1 副本，一个 Kafka topic 冗余 f 个节点故障而不会丢失任何已经提交的消息。 Kafka 对于数据不会丢失的保证，是基于至少一个节点在保持同步状态，一旦分区上的所有备份节点都挂了，就无法保证了。但是，实际在运行的系统需要去考虑假设一旦所有的备份都挂了，怎么去保证数据不会丢失，这里有两种实现的方法 等待一个 ISR 的副本重新恢复正常服务，并选择这个副本作为领 leader （它有极大可能拥有全部数据）。 选择第一个重新恢复正常服务的副本（不一定是 ISR 中的）作为leader。 kafka 默认选择第二种策略，当所有的 ISR 副本都挂掉时，会选择一个可能不同步的备份作为 leader ，可以配置属性 unclean.leader.election.enable 禁用此策略，那么就会使用第 一种策略即停机时间优于不同步。 如何选举Leader 最简单最直观的方案是，所有Follower都在Zookeeper上设置一个Watch，一旦Leader宕机，其对应的ephemeral znode会自动删除，此时所有Follower都尝试创建该节点，而创建成功者（Zookeeper保证只有一个能创建成功）即是新的Leader，其它Replica即为Follower。 但是该方法会有3个问题： split-brain 这是由Zookeeper的特性引起的，虽然Zookeeper能保证所有Watch按顺序触发，但并不能保证同一时刻所有Replica“看”到的状态是一样的，这就可能造成不同Replica的响应不一致 herd effect 如果宕机的那个Broker上的Partition比较多，会造成多个Watch被触发，造成集群内大量的调整 Zookeeper负载过重 每个Replica都要为此在Zookeeper上注册一个Watch，当集群规模增加到几千个Partition时Zookeeper负载会过重。 Kafka 0.8.* 的Leader Election方案解决了上述问题，它 在所有broker中选出一个controller，所有Partition的Leader选举都由controller决定。controller会将Leader的改变直接通过RPC的方式（比Zookeeper Queue的方式更高效）通知需为此作出响应的Broker。同时controller也负责增删Topic以及Replica的重新分配。如果 controller 节点挂了，其他 存活的 broker 都可能成为新的 controller 节点。 【更详细的分析可看】Kafka设计解析（二）- Kafka High Availability （上） Custom RebalanceConsumer Rebalance 的算法如下： 将目标 Topic 下的所有 Partirtion 排序，存于 PT 对某 Consumer Group 下所有 Consumer 排序，存于 CG，第 i 个 Consumer 记为 Ci N=size(PT)/size(CG)，向上取整 解除 Ci 对原来分配的 Partition 的消费权（i 从 0 开始） 将第 i * N 到（i+1）* N−1 个 Partition 分配给 Ci 根据 Kafka 社区 wiki，Kafka 作者正在考虑在还未发布的 0.9.x 版本中使用中心协调器 (Coordinator) 。大体思想是为所有 Consumer Group 的子集选举出一个 Broker 作为 Coordinator，由它 Watch Zookeeper，从而判断是否有 Partition 或者 Consumer 的增减，然后生成 Rebalance 命令，并检查是否这些 Rebalance 在所有相关的 Consumer 中被执行成功，如果不成功则重试，若成功则认为此次 Rebalance 成功（这个过程跟 Replication Controller 非常类似）: Consumer 1) Consumer 启动时，先向 Broker 列表中的任意一个 Broker 发送 ConsumerMetadataRequest，并通过 ConsumerMetadataResponse 获取它所在 Group 的 Coordinator 信息。 2）Consumer 连接到 Coordinator 并发送 HeartbeatRequest: 如果返回的 HeartbeatResponse 没有任何错误码，Consumer 继续 fetch 数据。 若其中包含 IllegalGeneration 错误码，即说明 Coordinator 已经发起了 Rebalance 操作，此时 Consumer 停止 fetch 数据，commit offset，并发送 JoinGroupRequest 给它的 Coordinator，并在 JoinGroupResponse 中获得它应该拥有的所有 Partition 列表和它所属的 Group 的新的 Generation ID。此时 Rebalance 完成，Consumer 开始 fetch 数据。 故障检测机制Consumer 成功加入 Group 后，Consumer 和相应的 Coordinator 同时开始故障探测程序。 Consumer 向 Coordinator 发起周期性的 Heartbeat（HeartbeatRequest）并等待响应，该周期为 session.timeout.ms/heartbeat.frequency。 若 Consumer 在 session.timeout.ms 内未收到 HeartbeatResponse，或者发现相应的 Socket channel 断开，它即认为 Coordinator 已宕机并启动 Coordinator 探测程序。 若 Coordinator 在 session.timeout.ms 内没有收到一次 HeartbeatRequest，则它将该 Consumer 标记为宕机状态并为其所在 Group 触发一次 Rebalance 操作。 Coordinator Failover 过程中，Consumer 可能会在新的 Coordinator 完成 Failover 过程之前或之后发现新的 Coordinator 并向其发送 HeatbeatRequest。 对于后者，新的 Cooodinator 可能拒绝该请求，致使该 Consumer 重新探测 Coordinator 并发起新的连接请求。 如果该 Consumer 向新的 Coordinator 发送连接请求太晚，新的 Coordinator 可能已经在此之前将其标记为宕机状态而将之视为新加入的 Consumer 并触发一次 Rebalance 操作。 Coordinator1）稳定状态下，Coordinator 通过上述故障探测机制跟踪其所管理的每个 Group 下的每个 Consumer 的健康状态。 2）刚启动时或选举完成后，Coordinator 从 Zookeeper 读取它所管理的 Group 列表及这些 Group 的成员列表。如果没有获取到 Group 成员信息，它不会做任何事情直到某个 Group 中有成员注册进来。 3）在 Coordinator 完成加载其管理的 Group 列表及其相应的成员信息之前，它将为 HeartbeatRequest，OffsetCommitRequest 和 JoinGroupRequests 返回 CoordinatorStartupNotComplete 错误码。此时，Consumer 会重新发送请求。 4）Coordinator 会跟踪被其所管理的任何 Consumer Group 注册的 Topic 的 Partition 的变化，并为该变化触发 Rebalance 操作。创建新的 Topic 也可能触发 Rebalance，因为 Consumer 可以在 Topic 被创建之前就已经订阅它了。 Coordinator 发起 Rebalance 操作流程如下所示。 Coordinator Failover如前文所述，Rebalance 操作需要经历如下几个阶段 1）Topic/Partition 的改变或者新 Consumer 的加入或者已有 Consumer 停止，触发 Coordinator 注册在 Zookeeper 上的 watch，Coordinator 收到通知准备发起 Rebalance 操作。 2）Coordinator 通过在 HeartbeatResponse 中返回 IllegalGeneration 错误码发起 Rebalance 操作。 3）Consumer 发送 JoinGroupRequest 4）Coordinator 在 Zookeeper 中增加 Group 的 Generation ID 并将新的 Partition 分配情况写入 Zookeeper 5）Coordinator 发送 JoinGroupResponse 【摘自】Kafka 设计解析（四）：Kafka Consumer 解析]]></content>
      <categories>
        <category>中间件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql入门]]></title>
    <url>%2F2019%2F09%2F22%2FMysql%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[重识Count()COUNT(列名)、COUNT(常量)和COUNT(*)之间区别前面我们提到过 COUNT(expr) 用于做行数统计，统计的是expr不为NULL的行数，那么 COUNT(列名)、 COUNT(常量) 和 COUNT(*) 这三种语法中，expr分别是列名、 常量 和 *。 那么列名、 常量 和 * 这三个条件中，常量 是一个固定值，肯定不为NULL。* 可以理解为查询整行，所以肯定也不为NULL，那么就只有列名的查询结果有可能是NULL了。 所以， COUNT(常量) 和 COUNT(*)表示的是直接查询符合条件的数据库表的行数。而COUNT(列名)表示的是查询符合条件的列的值不为NULL的行数。 除了查询得到结果集有区别之外，COUNT(*)相比COUNT(常量) 和 COUNT(列名)来讲，COUNT(*)是SQL92定义的标准统计行数的语法，因为他是标准语法，所以MySQL数据库对他进行过很多优化。 COUNT(*)的优化MyISAM和InnoDB有很多区别，其中有一个关键的区别和我们接下来要介绍的COUNT(*)有关，那就是: MyISAM不支持事务，MyISAM中的锁是表级锁； 而InnoDB支持事务，并且支持行级锁; 因为MyISAM的锁是表级锁，同一张表上面的操作需要串行进行，所以，MyISAM做了一个简单的优化: 它可以把表的总行数单独记录下来，如果从一张表中使用COUNT(*)进行查询的时候，可以直接返回这个记录下来的数值就可以了，当然，前提是不能有where条件。 MyISAM之所以可以把表中的总行数记录下来供COUNT(*)查询使用，那是因为MyISAM数据库是表级锁，不会有并发的数据库行数修改，所以查询得到的行数是准确的。 但是，对于InnoDB来说，就不能做这种缓存操作了，因为InnoDB支持事务，其中大部分操作都是行级锁，所以可能表的行数可能会被并发修改，那么缓存记录下来的总行数就不准确了。 在InnoDB中，使用COUNT(*)查询行数的时候，不可避免的要进行扫表了，那么，就可以在扫表过程中下功夫来优化效率了。从MySQL 8.0.13开始，针对InnoDB的 SELECT COUNT(*) FROM tbl_name 语句，确实在扫表的过程中做了一些优化。前提是查询语句中不包含WHERE或GROUP BY等条件。 COUNT(*)的目的只是为了统计总行数，所以，他根本不关心自己查到的具体值，所以，他如果能够在扫表的过程中，选择一个成本较低的索引进行的话，那就可以大大节省时间。 我们知道，InnoDB中索引分为聚簇索引（主键索引）和非聚簇索引（非主键索引），聚簇索引的叶子节点中保存的是整行记录，而非聚簇索引的叶子节点中保存的是该行记录的主键的值。 相比之下，非聚簇索引要比聚簇索引小很多，所以MySQL会优先选择最小的非聚簇索引来扫表。所以，当我们建表的时候，除了主键索引以外，创建一个非主键索引还是有必要的。 至此，我们介绍完了MySQL数据库对于COUNT(*)的优化，这些优化的前提都是查询语句中不包含WHERE以及GROUP BY条件。 COUNT(*)和COUNT(1)看下MySQL官方文档是怎么说的： InnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference. 画重点：same way , no performance difference。所以，对于COUNT(1)和COUNT(*)，MySQL的优化是完全一样的，根本不存在谁比谁快！ 建议使用COUNT(*)！因为这个是SQL92定义的标准统计行数的语法。 COUNT(字段)最后，就是我们一直还没提到的COUNT(字段)，他的查询就比较简单粗暴了，就是进行全表扫描，然后判断指定字段的值是不是为NULL，不为NULL则累加。相比COUNT(*)，COUNT(字段)多了一个步骤就是判断所查询的字段是否为NULL，所以他的性能要比COUNT(*)慢。 事务及原理Mysql事务及其原理 事务的 ACID 属性 原子性（Atomicity）：作为逻辑工作单元，一个事务里的所有操作的执行，要么全部成功，要么全部失败。 一致性（Consistency）：数据库从一个一致性状态变换到另外一个一致性状态，数据库的完整性不会受到破坏。 隔离性（Isolation）：通常来说，一个事务所做的修改在最终提交前，对其他事务是不可见的。为什么是通常来说，为了提高事务的并发引出不同的隔离级别，具体参考下一章节。 持久性（Durability）：一旦事务提交，则其所做的修改就会永久保存到数据库中，即使系统故障，修改的数据也不会丢失。 事务的隔离级别为了尽可能的高并发，事务的隔离性被分为四个级别：读未提交、读已提交、可重复读和串行化。用户可以根据需要选择不同的级别。 未提交读（READ UNCOMMITTED）：一个事务还未提交，它的变更就能被别的事务看到。 例：事务 A 可以读到事务 B 修改的但还未提交的数据，会导致脏读（可能事务 B 在提交后失败了，事务 A 读到的数据是脏的）。 提交读（READ COMMITTED）：一个事务提交后，它的变更才能被其他事务看到。大多数据库系统的默认级别，但 Mysql 不是。 例：事务 A 只能读到事务 B 修改并提交后的数据，会导致不可重复读（事务 A 中执行两次查询，一次在事务 B 提交过程中，一次在事务 B 提交之后，会导致两次读取的结果不一致）。 可重复读（REPEATABLE READ）：未提交的事务的变更不能被其他事务看到，同时一次事务过程中多次读取同样记录的结果是一致的。 例：事务 A 在执行过程中多次获取某范围内的记录，事务 B 提交后在此范围内插入或者删除 N条记录，事务 A 执行过程中多次范围读会存在不一致，即幻读（Mysql 的默认级别，InnoDB 通过 MVVC 解决了幻读的问题）。 可串行化（SERIALIZABLE）：当两个事务间存在读写冲突时，数据库通过加锁强制事务串行执行，解决了前面所说的所有问题（脏读、不可重复读、幻读）。是最高隔离的隔离级别。 用表格可以更清晰的描述四种隔离级别的定义和可能存在的问题： 以上是对四种隔离级别的定义和初步认识，看《十分钟搞懂MySQL四种事务隔离级别》这篇文章可以彻底弄清楚他们之间的区别。 分库分表概念 分表 - 解决单表数据过大比如你单表都几千万数据了，你确定你能扛住么？绝对不行，单表数据量太大，会极大影响你的 sql 执行的性能，到了后面你的 sql 可能就跑的很慢了。一般来说，就以我的经验来看，单表到几百万的时候，性能就会相对差一些了，你就得分表了。 分表就是把一个表的数据放到多个表中，然后查询的时候你就查一个表。比如按照用户 id 来分表，将一个用户的数据就放在一个表中。然后操作的时候你对一个用户就操作那个表就好了。这样可以控制每个表的数据量在可控的范围内，比如每个表就固定在 200 万以内。 分库 - 解决单库并发压力太大一个库一般我们经验而言，最多支撑到并发 2000，一定要扩容了，而且一个健康的单库并发值你最好保持在每秒 1000 左右，不要太大。那么你可以将一个库的数据拆分到多个库中，访问的时候就访问一个库好了。 分库分表前 分库分表后 并发支撑情况 MySQL 单机部署，扛不住高并发 MySQL从单机到多机，能承受的并发增加了多倍 磁盘使用情况 MySQL 单机磁盘容量几乎撑满 拆分为多个库，数据库服务器磁盘使用率大大降低 SQL 执行性能 单表数据量太大，SQL 越跑越慢 单表数据量减少，SQL 执行效率明显提升 水平拆分水平拆分的意思，就是把一个表的数据给弄到多个库的多个表里去，但是每个库的表结构都一样，只不过每个库表放的数据是不同的，所有库表的数据加起来就是全部数据。水平拆分的意义，就是将数据均匀放更多的库里，然后用多个库来扛更高的并发，还有就是用多个库的存储容量来进行扩容。 垂直拆分垂直拆分的意思，就是把一个有很多字段的表给拆分成多个表，或者是多个库上去。每个库表的结构都不一样，每个库表都包含部分字段。一般来说，会将较少的访问频率很高的字段放到一个表里去，然后将较多的访问频率很低的字段放到另外一个表里去。因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。 这个其实挺常见的，不一定我说，大家很多同学可能自己都做过，把一个大表拆开，订单表、订单支付表、订单商品表。 拆表不拆库还有表层面的拆分，就是分表，将一个表变成 N 个表，就是让每个表的数据量控制在一定范围内，保证 SQL 的性能。否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多，但是也得看具体你怎么操作，也可能是 500 万，或者是 100 万。你的SQL越复杂，就最好让单表行数越少。 分库分表策略 垂直拆分，你可以在表层面来做，对一些字段特别多的表做一下拆分； 水平拆分，你可以说是并发承载不了，或者是数据量太大，容量承载不了，你给拆了，按什么字段来拆，你自己想好； 分表，你考虑一下，你如果哪怕是拆到每个库里去，并发和容量都 ok 了，但是每个库的表还是太大了，那么你就分表不分库，将这个表分开，保证每个表的数据量并不是很大。 而且这儿还有两种分库分表的方式： 按照 range 来分，就是每个库一段连续的数据，这个一般是按比如时间范围来的，但是这种一般较少用。 优点： 扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了 缺点：很容易产生热点问题，大量的流量都打在最新的数据上。实际生产用 range，要看场景； 按照某个字段 hash 一下均匀分散，这个较为常用。 优点：可以平均分配每个库的数据量和请求压力； 缺点：扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。 分库分表中间件比较常见的包括： Cobar阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 Cobar 集群，Cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。 TDDL淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。 Atlas360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。 Sharding-jdbc当当开源的，属于 client 层方案，目前已经更名为 ShardingSphere（后文所提到的 Sharding-jdbc，等同于 ShardingSphere）。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且截至 2019.4，已经推出到了 4.0.0-RC1 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。 Mycat基于 Cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 Sharding jdbc 来说，年轻一些，经历的锤炼少一些。 总结综上，现在其实建议考量的，就是 Sharding-jdbc 和 Mycat，这两个都可以去考虑使用。 Sharding-jdbc 优点： 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，建议中小型公司选用; 缺点： 如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 Sharding-jdbc 的依赖； Mycat 优点： 对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞定就行； 优点： 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，最好是专门弄个人来研究和维护 Mycat，然后大量项目直接透明使用即; Mysql主从复制读写分离其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 主从复制原理主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行 SQL 的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以 MySQL 实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 半同步复制也叫 semi-sync 复制，指的就是主库写入 binlog 日志之后，强制立即将数据同步到从库，从库将日志写入自己本地的 relay log 之后，接着会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为写操作完成了； 并行复制从库开启多个线程，并行读取 relay log 中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 主从同步延时问题一般来说，如果主从延迟较为严重，有以下解决方案： 分库，将一个主库拆分为多个主库，每个主库的写并发就减少了几倍，此时主从延迟可以忽略不计； 打开MySQL支持的并行复制，多个库并行复制。如果说某个库的写入并发就是特别高，单库写并发达到了 2000/s，并行复制还是没意义； 重写代码，写代码的同学，要慎重，插入数据时立马查询可能查不到; 如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，如果这么做，读写分离的意义就丧失了。]]></content>
      <categories>
        <category>Mysql</category>
        <category>入门</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis入门教程]]></title>
    <url>%2F2019%2F09%2F19%2FRedis%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[redis 的线程模型redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，根据 socket 的事件类型交给对应的事件处理器进行处理。 来看客户端与 redis 的一次通信过程： 要明白，通信是通过 socket 来完成的，不懂的同学可以先去看一看 socket 网络编程。 首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 这样便完成了一次通信。关于 Redis 的一次通信过程，推荐读者阅读《Redis 设计与实现——黄健宏》进行系统学习。 redis单线程模型为什么快 1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路I/O复用模型，非阻塞IO； 5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 6、C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。 以上几点都比较好理解，下边我们针对多路 I/O 复用模型进行简单的探讨： （1）多路 I/O 复用模型 多路I/O复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。 这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。 redis持久化持久化的两种方式 RDB：内存快照，全量；是对 redis 中的数据执行周期性的快照。 AOF：对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。类似于mysql的binlog, 存储紧张时会进行压缩，指令合并 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点优点 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，比如说 Amazon 的 S3 云服务上去，在国内可以是阿里云的 ODPS 分布式存储上，以预定好的备份策略来定期备份 redis 中的数据。 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 缺点 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF 优缺点优点 AOF可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF日志文件以append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。 缺点 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大。 AOF开启后，支持的写QPS会比RDB支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。（如果实时写入，那么 QPS 会大降，redis 性能会大大降低） 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的 基于命令日志 merge 回放 的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 持久化方案 不要仅仅使用 RDB，因为那样会导致你丢失很多数据； 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug； redis支持 同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 redis高并发和高可用redis 实现高并发主要依靠主从架构，一主多从，一般来说，很多项目其实就足够了，单主用来写入数据，单机几万 QPS，从用来查询数据，多个从实例可以提供每秒 10w 的 QPS。 如果想要在实现高并发的同时，容纳大量的数据，那么就需要 redis 集群，使用 redis 集群之后，可以提供每秒几十万的读写并发。 redis高可用，如果是做主从架构部署，那么加上哨兵就可以了，就可以实现，任何一个实例宕机，可以进行主备切换。 redis主从架构单机的 redis，能够承载的 QPS 大概就在上万到几万不等。对于缓存来说，一般都是用来支撑读高并发的。因此架构做成主从(master-slave)架构，一主多从，主负责写，并且将数据复制到其它的 slave 节点，从节点负责读。所有的读请求全部走从节点。这样也可以很轻松实现水平扩容，支撑读高并发。 redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发 redis replication 的核心机制 redis 采用异步方式（master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node）复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量； 一个 master node 是可以配置多个 slave node 的； slave node 也可以连接其他的 slave node； slave node 做复制的时候，不会 block master node 的正常工作； slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了； slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。 注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。 另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。 redis 主从复制的核心原理当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。 如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制： （1）master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端 client 新收到的所有写命令缓存在内存中； （2）RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave； （3）slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中； （4）接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。 slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。 主从复制的断点续传从 redis2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份。 master node会 在内存中维护一个 backlog，master 和 slave 都会保存一个 replica offset 还有一个 master run id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次 replica offset 开始继续复制，如果没有找到对应的 offset，那么就会执行一次 resynchronization。 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分。 无磁盘化复制master 在内存中直接创建 RDB，然后发送给 slave，不会在自己本地落地磁盘了。只需要在配置文件中开启 repl-diskless-sync yes 即可。 1234repl-diskless-sync yes# 等待 5s 后再开始复制，因为要等更多 slave 重新连接过来repl-diskless-sync-delay 5 复制的完整流程slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的host和ip，但是复制流程没开始。 slave node 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，master node 持续将写命令，异步复制给 slave node。 全量复制 master 执行 bgsave ，在本地生成一份 rdb 快照文件。 master node 将 rdb 快照文件发送给 slave node，如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s) master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。 1client-output-buffer-limit slave 256MB 64MB 60 slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务。 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。 增量复制 如果全量复制过程中，master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。 master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB。 master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。 heartbeat主从节点互相都会发送 heartbeat 信息。 master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。 redis基于哨兵实现高可用哨兵的介绍sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很很不合理了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 哨兵集群必须部署 2 个以上节点, 经典的 3 节点哨兵集群是这样的 123456789 +----+ | M1 | | S1 | +----+ |+----+ | +----+| R2 |----+----| R3 || S2 | | S3 |+----+ +----+ 配置 quorum=2，如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机了，然后选举出一个来执行故障转移，同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移. redis 哨兵主备切换的数据丢失问题 异步复制导致的数据丢失 因为 master -&gt; slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这部分数据就丢失了。 脑裂导致的数据丢失脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。 此时虽然某个 slave 被切换成了 master，但是可能 client 还没来得及切换到新的 master，还继续向旧 master 写数据。因此旧 master 再次恢复的时候，会被作为一个 slave 挂到新的 master 上去，自己的数据会清空，重新从新的 master 复制数据。而新的 master 并没有后来 client 写入的数据，因此，这部分数据也就丢失了。 数据丢失问题的解决方案进行如下配置： 12min-slaves-to-write 1 ## 表示要求至少有1个slavemin-slaves-max-lag 10 ## 表示数据复制和同步的延迟不能超过10秒 减少异步复制数据的丢失有了 min-slaves-max-lag 这个配置，就可以确保说，一旦 slave 复制数据和 ack 延时太长，超过10s，就认为可能 master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 master 宕机时由于部分数据未同步到 slave 导致的数据丢失降低的可控范围内。 减少脑裂的数据丢失如果一个 master 出现了脑裂，跟其他 slave 丢了连接，那么上面两个配置可以确保说，如果不能继续给指定数量的 slave 发送数据，而且 slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。 sdown 和 odown 转换机制 sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 （1）sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds 指定的毫秒数之后，就主观认为 master 宕机了； （2）如果一个哨兵在指定时间内，收到了 quorum 数量的其它哨兵也认为那个 master 是 sdown 的，那么就认为是 odown 了。 哨兵集群的自动发现机制 哨兵互相之间的发现，是通过 redis 的 pub/sub 系统实现的，每个哨兵都会往 __sentinel__:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在。 每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的__sentinel__:hello channel 里发送一个消息，内容是自己的 host、ip 和 runid 还有对这个 master 的监控配置。 每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在。 每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步。 slave 配置的自动纠正哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 复制现有 master 的数据；如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上。 slave-&gt;master 选举算法如果一个 master 被认为 odown 了，而且 majority 数量的哨兵都允许主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来，会考虑 slave 的一些信息： 跟 master 断开连接的时长 slave 优先级 复制 offset run id 如果一个 slave 跟 master 断开连接的时间已经超过了 down-after-milliseconds 的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master。 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对 slave 进行排序： 按照 slave 优先级进行排序，slave priority 越低，优先级就越高。 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高。 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave。 quorum 和 majority每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还需要得到 majority 哨兵的授权，才能正式执行切换。 如果 quorum &lt; majority，比如 5 个哨兵，majority 就是 3，quorum 设置为 2，那么就 3 个哨兵授权就可以执行切换。 但是如果 quorum &gt;= majority，那么必须 quorum 数量的哨兵都授权，比如 5 个哨兵，quorum 是 5，那么必须 5 个哨兵都同意授权，才能执行切换。 configuration epoch哨兵会对一套 redis master+slaves 进行监控，有相应的监控的配置。 执行切换的那个哨兵，会从要切换到的新 master（salve-&gt;master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。 configuration 传播哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。 这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。 redis集群集群介绍Redis集群是一个提供在多个Redis节点间共享数据的程序集。主要是针对海量数据+高并发+高可用的场景。 Redis集群并不支持处理多个keys的命令, 因为这需要在不同的节点间移动数据, 从而达不到像Redis那样的性能, 在高负载的情况下可能会导致不可预料的错误. Redis 集群通过分区来提供一定程度的可用性,在实际环境中当某个节点宕机或者不可达的情况下继续处理命令. Redis 集群的优势: 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。 cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本原理集群元数据的维护采用Gossip协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。 gossip好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力； 不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后； 10000端口：每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号+10000，比如 7001，那么用于节点间通信的就是 17001 端口。每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其它几个节点接收到 ping 之后返回 pong。 交换的信息：信息包括故障信息，节点的增加和删除，hash slot 信息等等。 gossip 协议gossip 协议包含多种消息，包含 ping,pong,meet,fail 等等。 meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。1redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。 ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。 pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。 fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机了。 ping 消息深入ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，如果调得比较大，那么会降低 ping 的频率。 每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。 Redis 集群的数据分片Redis 集群引入了 哈希槽的概念. Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,举个例子,比如当前集群有3个节点,那么: 123节点 A 包含 0 到 5500号哈希槽.节点 B 包含5501 到 11000 号哈希槽.节点 C 包含11001 到 16384号哈希槽. 这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我想移除节点A,需要将A中的槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态. redis cluster 的高可用与主备切换原理redis cluster 的高可用的原理，几乎跟哨兵是类似的。 判断节点宕机如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。 从节点过滤对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。 从节点选举每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。 从节点执行主备切换，从节点切换为主节点。]]></content>
      <categories>
        <category>中间件</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>入门</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用消息队列简介]]></title>
    <url>%2F2019%2F09%2F05%2F%E5%B8%B8%E7%94%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[消息队列常见场景消息队列常见的使用场景比较核心的有 4 个：解耦、异步、削峰、分布式事务。 解耦 通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 异步看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了。 削峰每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 分布式事务 发送消息开启确认发布机制，MQ收到后会返回回执，超时未收到回执，发送方可重新发送；消费者开启手动ack模式，控制消息的重发、清楚、丢弃，保证数据可靠处理。 队列高可用RabbitMq入门以及使用教程 RabbitMQ 的应用场景以及基本原理介绍 RabbitMQ两种集群模式 普通模式默认的集群模式。 场景1、客户端直接连接队列所在节点如果有一个消息生产者或者消息消费者通过amqp-client的客户端连接至节点1进行消息的发布或者订阅，那么此时的集群中的消息收发只与节点1相关，这个没有任何问题； 场景2、客户端连接的是非队列数据所在节点如果消息生产者所连接的是节点2或者节点3，此时队列1的完整数据不在该两个节点上，那么在发送消息过程中这两个节点主要起了一个路由转发作用，根据这两个节点上的元数据（也就是上文提到的：指向queue的owner node的指针）转发至节点1上，最终发送的消息还是会存储至节点1的队列1上。同样，如果消息消费者所连接的节点2或者节点3，那这两个节点也会作为路由节点起到转发作用，将会从节点1的队列1中拉取消息进行消费。 特点： 非高可用 主要是提高吞吐量的 镜像模式把需要的队列做成镜像队列，存在于多个节点，属于RabbitMQ的HA方案跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。 高可用分布式集群 基于镜像模式，对于消息的生产和消费者可以通过HAProxy的软负载将请求分发至RabbitMQ集群中的Node1～Node7节点，其中Node8～Node10的三个节点作为磁盘节点保存集群元数据和配置信息。 消息不丢失RabbitMQ Kafaka 消费者:关闭自动提交 offset，在处理完之后自己手动提交offset； kafka:设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 生产者:如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 消息顺序性场景描述举个例子，数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql -&gt; mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog 日志，接着这三条 binlog 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。 本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。 先看看顺序会错乱的俩场景： RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。 Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。 解决方案RabbitMQ拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 Kafka 一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。 常见消息队列对比 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。]]></content>
      <categories>
        <category>中间件</category>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot集成Mybatis]]></title>
    <url>%2F2019%2F08%2F29%2FSpring-Boot%E9%9B%86%E6%88%90Mybatis%2F</url>
    <content type="text"><![CDATA[SpringBoot 整合 Mybatis 有两种常用的方式，一种就是我们常见的 xml 的方式 ，还有一种是全注解的方式。我觉得这两者没有谁比谁好，在 SQL 语句不太长的情况下，我觉得全注解的方式一定是比较清晰简洁的。但是，复杂的 SQL 确实不太适合和代码写在一起。 下面记录一下配置过程： 创建工程创建一个spring boot的maven工程, pom核心内容如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;mysql.version&gt;5.1.46&lt;/mysql.version&gt; &lt;mybatis.version&gt;1.3.5&lt;/mybatis.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!--msyql核心驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.10&lt;/version&gt; &lt;/dependency&gt; &lt;!--这个是官方的mybatis依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;mybatis-generator&lt;/id&gt; &lt;phase&gt;deploy&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;!-- Mybatis-Generator 工具配置文件的位置 --&gt; &lt;configurationFile&gt;src/main/resources/mybatis-generator/generatorConfig.xml&lt;/configurationFile&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;!-- 这个是自动生成mapper等的依赖，必须得加--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;!--这个是自动生成mapper等的依赖，必须得加--&gt; &lt;!--https://mvnrepository.com/artifact/org.mybatis.generator/mybatis-generator-core--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;$&#123;mybatis.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置文件application.properties 12345678910111213141516171819## mapper xml 文件地址mybatis.mapper-locations=classpath*:mapper/*Mapper.xml#数据库设置spring.datasource.type=com.alibaba.druid.pool.DruidDataSource##数据库urlspring.datasource.url=jdbc:mysql://127.0.0.1:3306/test?characterEncoding=utf8&amp;useSSL=false##数据库用户名spring.datasource.username=xxxx##数据库密码spring.datasource.password=xxxx##数据库驱动spring.datasource.driver-class-name=com.mysql.jdbc.Driver# Mybatis Generator configuration# dao类和实体类的位置mybatis.project =src/main/java# mapper文件的位置mybatis.resources=src/main/resources 自动生成xml方式generatorConfig配置在src/main/resource下创建mybatis-generator文件夹，在文件夹下创建generatorConfig.xml文件，制定generator生成规则： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;!-- 配置生成器 --&gt;&lt;generatorConfiguration&gt; &lt;!--执行generator插件生成文件的命令： call mvn mybatis-generator:generate -e --&gt; &lt;!-- 引入配置文件 --&gt; &lt;properties resource="application.properties"/&gt; &lt;!--classPathEntry:数据库的JDBC驱动,换成你自己的驱动位置 可选 --&gt; &lt;!--&lt;classPathEntry location="~/Downloads/mysql-connector-java-5.1.24-bin.jar"/&gt;--&gt; &lt;!-- 一个数据库一个context --&gt; &lt;!--defaultModelType="flat" 大数据字段，不分表 --&gt; &lt;context id="MysqlTables" targetRuntime="MyBatis3Simple" defaultModelType="flat"&gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name="autoDelimitKeywords" value="true"/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name="javaFileEncoding" value="utf-8"/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name="beginningDelimiter" value="`"/&gt; &lt;property name="endingDelimiter" value="`"/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name="javaFormatter" value="org.mybatis.generator.api.dom.DefaultJavaFormatter"/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name="xmlFormatter" value="org.mybatis.generator.api.dom.DefaultXmlFormatter"/&gt; &lt;plugin type="org.mybatis.generator.plugins.SerializablePlugin"/&gt; &lt;plugin type="org.mybatis.generator.plugins.ToStringPlugin"/&gt; &lt;!-- 为了防止生成的代码中有很多注释，比较难看，加入下面的配置控制 --&gt; &lt;commentGenerator&gt; &lt;property name="suppressAllComments" value="true"/&gt;&lt;!-- 是否取消注释 --&gt; &lt;property name="suppressDate" value="true"/&gt; &lt;!-- 是否生成注释带时间戳--&gt; &lt;/commentGenerator&gt; &lt;!-- jdbc连接 --&gt; &lt;jdbcConnection driverClass="$&#123;spring.datasource.driver-class-name&#125;" connectionURL="$&#123;spring.datasource.url&#125;" userId="$&#123;spring.datasource.username&#125;" password="$&#123;spring.datasource.password&#125;"/&gt; &lt;!-- 类型转换 --&gt; &lt;javaTypeResolver&gt; &lt;!-- 是否使用bigDecimal， false可自动转化以下类型（Long, Integer, Short, etc.） --&gt; &lt;property name="forceBigDecimals" value="false"/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 生成实体类地址 --&gt; &lt;javaModelGenerator targetPackage="com.austin.entity" targetProject="$&#123;mybatis.project&#125;"&gt; &lt;!-- 是否允许子包，即targetPackage.schemaName.tableName --&gt; &lt;property name="enableSubPackages" value="false"/&gt; &lt;!-- 是否对model添加 构造函数 --&gt; &lt;property name="constructorBased" value="true"/&gt; &lt;!-- 是否对类CHAR类型的列的数据进行trim操作 --&gt; &lt;property name="trimStrings" value="true"/&gt; &lt;!-- 建立的Model对象是否 不可改变 即生成的Model对象不会有 setter方法，只有构造方法 --&gt; &lt;property name="immutable" value="false"/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成maperxml文件, targetPackage表示xml文件存放地址 --&gt; &lt;sqlMapGenerator targetPackage="mapper" targetProject="$&#123;mybatis.resources&#125;"&gt; &lt;property name="enableSubPackages" value="false"/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成mapxml对应client，也就是接口dao --&gt; &lt;javaClientGenerator targetPackage="com.austin.dao" targetProject="$&#123;mybatis.project&#125;" type="XMLMAPPER"&gt; &lt;property name="enableSubPackages" value="false"/&gt; &lt;/javaClientGenerator&gt; &lt;!--table可以有多个,每个数据库中的表都可以写一个table， tableName表示要匹配的数据库表名,也可以在tableName属性中通过使用%通配符来匹配所有数据库表,只有匹配的表才会自动生成文件 domainObjectName是生成的实体类名称,可以不写，默认会用表名的驼峰格式 --&gt; &lt;table tableName="tbl_user" domainObjectName="UserInfo" enableCountByExample="true" enableUpdateByExample="true" enableDeleteByExample="true" enableSelectByExample="true" selectByExampleQueryId="true"&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;!-- 数据库表主键 --&gt; &lt;generatedKey column="id" sqlStatement="Mysql" identity="true"/&gt; &lt;/table&gt; &lt;table tableName="tbl_role" domainObjectName="RoleInfo" enableCountByExample="true" enableUpdateByExample="true" enableDeleteByExample="true" enableSelectByExample="true" selectByExampleQueryId="true"&gt; &lt;property name="useActualColumnNames" value="false"/&gt; &lt;!-- 数据库表主键 --&gt; &lt;generatedKey column="id" sqlStatement="Mysql" identity="true"/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 其中，如果在pom中配置了红框中所示的依赖，则不需要额外指定classPathEntry路径。 工程结构 执行生成执行可以采用两种方式 命令行pom文件所在路径执行： 1mvn mybatis-generator:generate IDE 生成结果 注解方式全注解的方式，这种方式和后面提到的xml的方式的区别仅仅在于 一个将 sql 语句写在 java 代码中，一个写在 xml 配置文件中。 123456789@Mapperpublic interface StudentMapper &#123; @Insert("insert into tbl_student(name, class_name, age) values (#&#123;name&#125;, #&#123;className&#125;, #&#123;age&#125;)") void insert(@Param("name") String name, @Param("className") String className, @Param("age") int age); @Select("select * from tbl_student") List&lt;Student&gt; findAllStudent();&#125; 常见问题（1）mapper文件无法注入出错原因： 启动类上需要通过@MapperScan指定mpper文件路径。 代码路径https://github.com/austin-brant/mybatis-spring-boot-demo]]></content>
      <categories>
        <category>数据库中间件</category>
        <category>Mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[多线程]Volatile和Synchronization详解]]></title>
    <url>%2F2019%2F08%2F14%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B-Volatile%E5%92%8CSynchronization%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在多线程并发编程中synchronized和volatile都扮演着重要的角色，volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。本文将深入分析volatile和synchronize的原理，通过深入分析帮助我们正确地使用volatile和synchronize关键字。首先先了解下并发编程的三大概念。 并发编程的三大概念可见性 可见性的意思是当一个线程修改一个共享变量时，另外一个线程能立马读到这个修改的值。 原子性 原子（atomic）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为“不可被中断的一个或一系列操作”。即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 在Java中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。但在多处理器上实现原子操作就变得有点复杂。 比如 a=0；（a非long和double类型） 这个操作是不可分割的，那么我们说这个操作时原子操作。再比如：a++； 这个操作实际是a = a + 1；是可分割的，所以他不是一个原子操作。非原子操作都会存在线程安全问题，需要我们使用同步技术（sychronized）来让它变成一个原子操作。一个操作是原子操作，那么我们称它具有原子性。java的concurrent包下提供了一些原子类，我们可以通过阅读API来了解这些原子类的用法。比如：AtomicInteger、AtomicLong、AtomicReference等。 有序性 有序性就是程序执行的顺序按照代码的先后顺序执行。 什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。也就是说，要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。在Java里面，可以通过volatile关键字来保证一定的“有序性”。另外可以通过synchronized和Lock来保证有序性，很显然，synchronized和Lock保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。 另外，Java内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从happens-before原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。 Java内存模型 Java内存模型规定了所有的变量都存储在主内存中。为了提高处理速度，处理器不直接和主内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他，俗称工作内存），线程对变量的所有操作（读取，赋值）都必须在工作内存中进行，但操作完不知道何时会写到内存。 不同线程之间也无法直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。 Volatile的定义与实现原理Java语言规范第3版中对volatile的定义如下：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过 排他锁 单独获得这个变量。Java语言提供了volatile，在某些情况下比锁要更加方便。如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的。 让我们在X86处理器下通过工具获取JIT编译器生成的汇编指令来查看对volatile进行写操作时，CPU会做什么事情。 Java代码如下。 1instance = new Singleton(); // instance是volatile变量 转变成汇编代码，如下。 10x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); 有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事情: 1）将当前处理器缓存行的数据写回到系统内存。 2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过 嗅探在总线上传播的数据 来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会 重新从系统内存中把数据读到处理器缓存里 。 具体讲解volatile的两条实现原则。 1）Lock前缀指令会引起处理器缓存回写到内存 Lock前缀指令导致在执行指令期间，声言处理器的LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以独占任何共享内存。但是，在新的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销的比较大，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为 “缓存锁定” ，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。 2）一个处理器的缓存回写到内存会导致其他处理器的缓存无效 IA-32处理器和Intel 64处理器使用 MESI（修改、独占、共享、无效）控制协议 去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能 嗅探其他处理器访问系统内存和它们的内部缓存。 处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。 例如，在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它自己的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。 Synchronized的实现在多线程并发编程中synchronized一直是元老级角色，很多人都会称呼它为重量级锁。但是，随着Java SE 1.6对synchronized进行了各种优化之后，有些情况下它就并不那么重了。下面详细介绍Java SE 1.6中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁，以及锁的存储结构和升级过程。 先来看下利用synchronized实现同步的基础：Java中的每一个对象都可以作为锁。具体表现为以下3种形式。 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象。 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。 从JVM规范中可以看到Synchonized在JVM里的实现原理，JVM基于进入和退出Monitor对象来实现方法同步和代码块同步，但两者的实现细节不一样。代码块同步是使用 monitorenter 和monitorexit 指令实现的，而方法同步是使用另外一种方式实现的，细节在JVM规范里并没有详细说明。但是，方法的同步同样可以使用这两个指令来实现。 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处，JVM要保证每个monitorenter必须有对应的monitorexit与之配对。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor的所有权，即尝试获得对象的锁。 Java对象头synchronized用的锁是存在Java对象头里的。如果对象是数组类型，则虚拟机用3个字宽（Word）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，1字宽等于4字节，即32bit。 Java对象头里的Mark Word里默认存储对象的HashCode、分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如下表所示： 在运行期间，Mark Word里存储的数据会随着锁标志位的变化而变化。Mark Word可能变化为存储以下4种数据，如下表： 在64位虚拟机下，Mark Word是64bit大小的，其存储结构如下表所示： 锁的升级Java SE 1.6为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”，在Java SE 1.6中，锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率，下文会详细分析。 偏向锁HotSpot的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。 如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 (1) 偏向锁的撤销 偏向锁使用了一种 等到竞争出现才释放锁 的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。 偏向锁的撤销，需要等待全局安全点（在这个时间点上没有正在执行的字节码）。 它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着; 如果线程不处于活动状态，则将对象头设置成无锁状态； 如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。 下图中的线程1演示了偏向锁初始化的流程，线程2演示了偏向锁撤销的流程。 (2) 关闭偏向锁 偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟： 1-XX:BiasedLockingStartupDelay=0 如果你确定应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁： 1-XX:-UseBiasedLocking=false 那么程序默认会进入轻量级锁状态。 轻量级锁（1）轻量级锁加锁 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并 将对象头中的Mark Word复制到锁记录中，官方称 为Displaced Mark Word 。然后线程尝试 使用CAS将对象头中的Mark Word替换为指向锁记录的指针。 如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 （2）轻量级锁解锁 轻量级解锁时，会 使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。下图是两个线程同时争夺锁，导致锁膨胀的流程图。 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 锁的对比 锁 优点 缺点 适用场景 无锁 高性能 基本上无并发，不存在线程竞争 偏向锁 加锁解锁不需要额外消耗，和执行非同步方法相比仅存在纳秒级差距 若线程间存在锁竞争，会带来额外的锁撤销消耗 只有一个线程访问的同步块场景 轻量级锁 竞争的线程一直自旋不会阻塞，提高了程序响应速度 未得到锁的线程自旋消耗CPU 追求响应时间，同步块执行速度非常快 重量级锁 线程竞争不使用自旋，不消耗CPU 线程竞争会直接阻塞，进入内核态，响应时间慢 追求吞吐量，同步块执行时间较长 Volatile vs Synchronize volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的; volatile仅能实现变量的修改可见性，并不能保证原子性；synchronized则可以保证变量的修改可见性和原子性; volatile不会造成线程的阻塞; synchronized可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化(禁止指令重排)；synchronized标记的变量可以被编译器优化 volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 本文主要摘抄于 《Java并发编程的艺术》（第二章） 只作为个人读书记录。]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>Synchronization</tag>
        <tag>Volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal源码解析]]></title>
    <url>%2F2019%2F08%2F13%2FThreadLocal%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[ThreadLocal的作用ThreadLocal的作用是提供线程内的局部变量，说白了，就是在各线程内部创建一个变量的副本，相比于使用各种锁机制访问变量，ThreadLocal的思想就是用空间换时间，使各线程都能访问属于自己这一份的变量副本，变量值不互相干扰，减少同一个线程内的多个函数或者组件之间一些公共变量传递的复杂度。 ThreadLocal特性及使用场景 1、方便同一个线程使用某一对象，避免不必要的参数传递； 2、线程间数据隔离（每个线程在自己线程里使用自己的局部变量，各线程间的ThreadLocal对象互不影响）； 3、获取数据库连接、Session、关联ID（比如日志的uniqueID，方便串起多个日志）； ThreadLocal应注意 1、ThreadLocal并未解决多线程访问共享对象的问题； 2、ThreadLocal并不是每个线程拷贝一个对象，而是直接new（新建）一个； 3、如果ThreadLocal.set()的对象是多线程共享的，那么还是涉及并发问题。 图解TreadLocal 每个线程可能有多个ThreadLocal，同一线程的各个ThreadLocal存放于同一个ThreadLocalMap中。 图解ThreadLocal(JDK8).vsdx原图下载地址：https://github.com/zxiaofan/JDK-Study/tree/master/src/java1/lang/threadLocal 内部类ThreadLocalMap1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; /** * ThreadLocalMap的key是ThreadLocal * value是Object（即我们所谓的“线程本地数据”） */ Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; /** * 初始容量，2的幂等次方 */ private static final int INITIAL_CAPACITY = 16; /** * 实际保存数据的数组，超过threshold会2倍扩容 */ private Entry[] table; /** * 实际存储的entry数量 */ private int size = 0; /** * 下次扩容的阈值 */ private int threshold; // Default to 0 /** * Set the resize threshold to maintain at worst a 2/3 load factor. */ private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; /** * 往后移动一位 */ private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0); &#125; /** * 往前移动一位 */ private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1); &#125; /** * Construct a new map initially containing (firstKey, firstValue). * ThreadLocalMaps懒汉模式, 等第一个entry被放入时才初始化. */ ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125; /** * 将父线程的ThreadLocalMaps内容复制过来 * Called only by createInheritedMap. */ private ThreadLocalMap(ThreadLocalMap parentMap) &#123; Entry[] parentTable = parentMap.table; int len = parentTable.length; setThreshold(len); table = new Entry[len]; for (int j = 0; j &lt; len; j++) &#123; Entry e = parentTable[j]; if (e != null) &#123; @SuppressWarnings("unchecked") ThreadLocal&lt;Object&gt; key = (ThreadLocal&lt;Object&gt;) e.get(); if (key != null) &#123; Object value = key.childValue(e.value); Entry c = new Entry(key, value); int h = key.threadLocalHashCode &amp; (len - 1); while (table[h] != null) h = nextIndex(h, len); table[h] = c; size++; &#125; &#125; &#125; &#125;&#125; ThreadLocalMap是定制的hashMap，仅用于维护当前线程的本地变量值。仅ThreadLocal类对其有操作权限，是Thread的私有属性。为避免占用空间较大或生命周期较长的数据常驻于内存引发一系列问题，hash table的key是弱引用WeakReferences。当空间不足时，会清理未被引用的entry。这时Entry里的key为null了，那么直到线程结束前，Entry中的value都是无法回收的，这里可能产生内存泄露。 SuppliedThreadLocal12345678910111213static final class SuppliedThreadLocal&lt;T&gt; extends ThreadLocal&lt;T&gt; &#123; private final Supplier&lt;? extends T&gt; supplier; SuppliedThreadLocal(Supplier&lt;? extends T&gt; supplier) &#123; this.supplier = Objects.requireNonNull(supplier); &#125; @Override protected T initialValue() &#123; return supplier.get(); &#125;&#125; SuppliedThreadLocal是JDK8新增的内部类，只是扩展了ThreadLocal的初始化值的方法而已，允许使用JDK8新增的Lambda表达式赋值。需要注意的是，函数式接口Supplier不允许为null。 初始化123456789101112131415161718192021222324252627282930313233public class ThreadLocal&lt;T&gt; &#123; /** * ThreadLocal初始化时会调用nextHashCode()方法初始化 * threadLocalHashCode，且threadLocalHashCode初始化后不可变。 * threadLocalHashCode可用来标记不同的ThreadLocal实例。 */ private final int threadLocalHashCode = nextHashCode(); private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; protected T initialValue() &#123; return null; &#125; /** * JDK8新增，支持Lambda表达式，和ThreadLocal重写的initialValue()效果一样。 */ public static &lt;S&gt; ThreadLocal&lt;S&gt; withInitial(Supplier&lt;? extends S&gt; supplier) &#123; return new SuppliedThreadLocal&lt;&gt;(supplier); &#125; public ThreadLocal() &#123; &#125;&#125; ThreadLocal类变量有3个，其中2个是静态变量（包括一个常量），实际作为作为ThreadLocal实例的变量只有threadLocalHashCode这1个，而且已经初始化就不可变了。 其中withInitial()方法使用示例： 123456789101112131415161718192021public void jdk8Test()&#123; Supplier&lt;String&gt; supplier =new Supplier&lt;String&gt;()&#123; @Override public String get()&#123; return"supplier_new"; &#125; &#125;; threadLocal= ThreadLocal.withInitial(supplier); System.out.println(threadLocal.get()); // supplier_new // Lambda表达式 threadLocal= ThreadLocal.withInitial(()-&gt;"sup_new_2"); System.out.println(threadLocal.get()); // sup_new_2 ThreadLocal&lt;DateFormat&gt; localDate = ThreadLocal.withInitial(()-&gt;new SimpleDateFormat("yyyy-MM-dd")); System.out.println(localDate.get().format(new Date())); // 2017-01-22 ThreadLocal&lt;String&gt; local =new ThreadLocal&lt;&gt;().withInitial(supplier); System.out.println(local.get()); // supplier_new&#125; 源码分析get方法12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 直接看代码，可以分析主要有以下几步： 获取当前的Thread对象，通过getMap获取Thread内的ThreadLocalMap 如果map已经存在，以当前的ThreadLocal为键，获取Entry对象，并从从Entry中取出值 否则，调用setInitialValue进行初始化。 getMap123ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; getMap很简单，就是返回线程中ThreadLocalMap，跳到Thread源码里看，ThreadLocalMap是这么定义的： 1ThreadLocal.ThreadLocalMap threadLocals = null; 所以ThreadLocalMap还是定义在ThreadLocal里面的，我们前面已经说过ThreadLocalMap中的Entry定义，下面为了先介绍ThreadLocalMap的定义我们把setInitialValue放在前面说。 setInitialValue12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; setInititialValue在Map不存在的时候调用。 首先是调用initialValue生成一个初始的value值，深入initialValue函数，我们可知它就是返回一个null，如果创建ThreadLocal时调用withInitial() 方法指定了初始方法，则返回自定义值； 还是在get()一下Map，如果map存在，则直接map.set(), 这个函数会放在后文说； 如果map不存在，则会调用createMap()创建ThreadLocalMap。 createMap123void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 比较简单，就是调用了ThreadLocalMap内部类的构造函数而已。 map.getEntry12345678private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e);&#125; 首先是计算索引位置i，通过计算key的hash%(table.length-1)得出； 根据获取Entry，如果Entry存在且Entry的key恰巧等于ThreadLocal，那么直接返回Entry对象； 否则，也就是在此位置上找不到对应的Entry，那么就调用getEntryAfterMiss。 getEntryAfterMiss12345678910111213141516private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125; 这个方法我们还得结合上一步看，上一步是因为不满足 1e != null &amp;&amp; e.get() == key 才沦落到调用getEntryAfterMiss的，所以: 首先e如果为null的话，证明不存在value, 那么getEntryAfterMiss还是直接返回null的 如果是不满足e.get() == key，那么进入while循环，这里是不断循环，如果e一直不为空，那么就调用nextIndex，不断递增i，在此过程中一直会做两个判断： 如果 k == key, 那么代表找到了这个所需要的Entry，直接返回； 如果 k == null，那么证明这个Entry中key已经为null, 那么这个Entry就是一个过期对象，这里调用expungeStaleEntry清理该Entry。这里解答了前面留下的一个坑，即ThreadLocal Ref销毁时，ThreadLocal实例由于只有Entry中的一条弱引用指着，那么就会被GC掉，Entry的key没了，value可能会内存泄露的，其实在每一个get，set操作时都会不断清理掉这种key为null的Entry的。 为什么循环查找？ 这里你可以直接跳到下面的set方法，主要是因为处理哈希冲突的方法，我们都知道HashMap采用拉链法处理哈希冲突，即在一个位置已经有元素了，就采用链表把冲突的元素链接在该元素后面，而ThreadLocal采用的是开放地址法，即有冲突后，把要插入的元素放在要插入的位置后面为null的地方 具体关于这两种方法的区别可以参考：解决哈希（HASH）冲突的主要方法。 所以上面的循环就是因为我们在第一次计算出来的i位置不一定存在key与我们想查找的key恰好相等的Entry，所以只能不断在后面循环，来查找是不是被插到后面了，直到找到为null的元素，因为若是插入也是到null为止的。 expungeStaleEntry12345678910111213141516171819202122232425262728293031323334private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; // （1）删掉staleSlot位置value值 tab[staleSlot].value = null; tab[staleSlot] = null; size--; // （2）Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; // 删除元素后，需要重新移动存活的元素，因为查找时遇到null会终止 int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125; 看上面这段代码主要有两部分： (1) 这段主要是将i位置上的Entry的value设为null，Entry的引用也设为null，那么系统GC的时候自然会清理掉这块内存； (2) 这段就是扫描位置staleSlot之后，null之前的Entry数组，清除每一个key为null的Entry，同时若是key不为空，做rehash，调整其位置。 为什么要做rehash呢？ 因为我们在清理的过程中会把某个值设为null，那么这个值后面的区域如果之前是连着前面的，那么下次循环查找时，就会只查到null为止。 举个例子就是： …, &lt;key1(hash1), value1&gt;, &lt;key2(hash1), value2&gt;,… 即key1和key2的hash值相同, 此时，若插入 &lt;key3(hash2), value3&gt; 其hash计算的目标位置被 &lt;key2(hash1), value2&gt; 占了，于是往后寻找可用位置，hash表可能变为： …, &lt;key1(hash1), value1&gt;, &lt;key2(hash1), value2&gt;, &lt;key3(hash2), value3&gt;, … 此时，若 &lt;key2(hash1), value2&gt; 被清理，显然 &lt;key3(hash2), value3&gt;应该往前移(即通过rehash调整位置)，否则若以key3查找hash表，将会找不到key3。 set方法我们在get方法的循环查找那里也大概描述了set方法的思想，即开放地址法,下面看具体代码： 12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125; 首先也是获取当前线程，根据线程获取到ThreadLocalMap，若是有ThreadLocalMap，则调用 1map.set(ThreadLocal&lt;?&gt; key, Object value) 若是没有则调用createMap创建。 map.set1234567891011121314151617181920212223242526private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 看上面这段代码： 首先还是根据key计算出位置i，然后查找i位置上的Entry， 若是Entry已经存在并且key等于传入的key，那么这时候直接给这个Entry赋新的value值。 若是Entry (e != null) 存在，但是key为null，则调用replaceStaleEntry来更换这个key为空的Entry 不断循环检测，直到遇到为null的地方，这时候要是还没在循环过程中return，那么就在这个null的位置新建一个Entry，并且插入，同时size增加1。 最后调用cleanSomeSlots，这个函数就不细说了，你只要知道内部还是调用了上面提到的expungeStaleEntry函数清理key为null的Entry就行了，最后返回是否清理了Entry，接下来再判断 sz&gt;thresgold ,这里就是判断是否达到了rehash的条件，达到的话就会调用rehash函数。 上面这段代码有两个函数还需要分析下，首先是: replaceStaleEntry123456789101112131415161718192021222324252627282930313233343536373839404142434445private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; // 向前找到key为null的位置 int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // staleSlot节点key为空，属于应该清理节点 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; // 更新value值 tab[i] = tab[staleSlot]; // i指向key为空节点 tab[staleSlot] = e; // staleSlot前面全不为空，i节点指向最新key为null位置 if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; // 更新key为空节点位置 if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; &#125; // If key not found, put new entry in stale slot tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); // If there are any other stale entries in run, expunge them if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 首先我们回想上一步是因为这个位置的Entry的key为null才调用replaceStaleEntry。 第1个for循环：我们向前找到key为null的位置，记录为slotToExpunge,这里是为了后面的清理过程，可以不关注了； 第2个for循环：我们从staleSlot起到下一个null为止，若是找到key和传入key相等的Entry，就给这个Entry赋新的value值，并且把它和staleSlot位置的Entry交换，然后调用CleanSomeSlots清理key为null的Entry。 若是一直没有key和传入key相等的Entry，那么就在staleSlot处新建一个Entry。函数最后再清理一遍空key的Entry。 说完replaceStaleEntry，还有个重要的函数是rehash以及rehash的条件： 首先是sz &gt; threshold时调用rehash rehash12345678private void rehash() &#123; // 清理全部空节点 expungeStaleEntries(); // Use lower threshold for doubling to avoid hysteresis if (size &gt;= threshold - threshold / 4) resize();&#125; 清理完空key的Entry后，如果size大于3/4的threshold，则调用resize函数： resize123456789101112131415161718192021222324252627private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; // Help the GC 下次gc会被回收 &#125; else &#123; int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; setThreshold(newLen); size = count; table = newTab;&#125; 由源码我们可知每次扩容大小扩展为原来的2倍，然后再一个for循环里，清除空key的Entry，同时重新计算key不为空的Entry的hash值，把它们放到正确的位置上，再更新ThreadLocalMap的所有属性。 remove最后一个需要探究的就是remove函数，它用于在map中移除一个不用的Entry。也是先计算出hash值，若是第一次没有命中，就循环直到null，在此过程中也会调用expungeStaleEntry清除空key节点。代码如下： 1234567891011121314private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; e.clear(); expungeStaleEntry(i); return; &#125; &#125;&#125; 使用ThreadLocal的最佳实践我们发现无论是set,get还是remove方法，过程中key为null的Entry都会被擦除，那么Entry内的value也就没有强引用链，GC时就会被回收。那么怎么会存在内存泄露呢？但是以上的思路是假设你调用get或者set方法了，很多时候我们都没有调用过，所以最佳实践就是: 使用者需要手动调用remove函数，删除不再使用的ThreadLocal. 尽量将ThreadLocal设置成private static的，这样ThreadLocal会尽量和线程本身一起消亡。 问题与思考（1）如果有多个ThreadLocal都对同一个线程ThreadLocalMap写数据时，可能存在hash位置冲突，导致set()和get()效率显著下降； （2）ThreadLocal不能读取父线程的ThradLocalMap内容，需要使用InheritableThreadLocal；]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>ThreadLocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-入门]]></title>
    <url>%2F2019%2F08%2F12%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[数组数组，将元素存储到内存的连续位置中，是最基本的数据结构。数组数据结构的主要优点是如果知道索引就可以通过 O(l) 进行快速搜索，但是在数组中添加和删除元素的速度会很慢，因为数组一旦被创建，就无法更改其大小。如果需要创建更长或更短的数组，得先创建一个新数组，再把原数组中的所有元素复制到新创建的数组中。 解决数组相关问题的关键是要熟悉数组的数据结构和基本的构造，如循环、递归等等； 给定一个 1-100 的整数数组，请找到其中缺少的数字。解决方法与代码：https://javarevisited.blogspot.com/2014/11/how-to-find-missing-number-on-integer-array-java.html 两种思路：（1）如果只缺少一个数字，n*(n+1)/2 - sum 就是缺失的数字；（2）缺失多个数字或是某些数字重复出现，则只能遍历一遍，记录哪些数字出现过，可用List或BitSet来记录。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 给定一个 1-n 的整数数组，请找到其中缺少的数字 * * @author austin * @since 2019/7/30 14:31 */public class FindMissingNumber &#123; /** * 支持多个重复或是缺失多个情况 * * @param arr 数组 * @param count n */ public static void printMissingNumber(int[] arr, int count) &#123; int missingCount = count - arr.length; BitSet bitSet = new BitSet(count); for (int i : arr) &#123; bitSet.set(i - 1); &#125; int lastIndex = 0; for (int i = 0; i &lt; missingCount; i++) &#123; lastIndex = bitSet.nextClearBit(lastIndex); System.out.println(++lastIndex); &#125; &#125; /** * 只支持缺失1个情况 */ public static void printSingleMissingNumber(int[] arr, int count) &#123; int exceptedSum = count * (count + 1) / 2; int sum = 0; for (int i : arr) &#123; sum += i; &#125; System.out.println(exceptedSum - sum); &#125; public static void main(String[] args) &#123; // 缺失一个数字 printMissingNumber(new int[] &#123;1, 2, 3, 4, 6&#125;, 6); // 缺失3个数字 printMissingNumber(new int[] &#123;1, 2, 3, 4, 6, 9, 8&#125;, 10); // 缺失一个数字 printSingleMissingNumber(new int[] &#123;1, 2, 3, 4, 6&#125;, 6); &#125;&#125; 在给定的成对整数数组中，请找出所有总和等于给定数字的组合。解决方法与代码：http://javarevisited.blogspot.com/2014/08/how-to-find-all-pairs-in-array-of-integers-whose-sum-equal-given-number-java.html 三种思路：（1）两层循环，时间复杂度O(n^2);（2）存储到HashTable里，在HashTable里找（sum - arr[i]）值， 时间复杂度： O(n), 空间复杂度： O(n);（3）先排序O(nlogn), 然后首尾想加，往中间靠； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import java.util.Arrays;import java.util.HashSet;import java.util.Set;/** * 在给定的成对整数数组中，请找出所有总和等于给定数字的组合 * * @author austin * @since 2019/7/30 15:35 */public class ComposeSum &#123; /** * 可输出全部组合 * 两层循环 */ public static void printPairs(int[] array, int sum) &#123; for (int i = 0; i &lt; array.length; i++) &#123; int first = array[i]; for (int j = i + 1; j &lt; array.length; j++) &#123; int second = array[j]; if ((first + second) == sum) &#123; System.out.printf("(%d, %d) %n", first, second); &#125; &#125; &#125; &#125; /** * map方式 */ public static void printPairsUsingSet(int[] numbers, int n) &#123; if (numbers.length &lt; 2) &#123; return; &#125; Set set = new HashSet(numbers.length); for (int value : numbers) &#123; int target = n - value; // if target number is not in set then add if (!set.contains(target)) &#123; set.add(value); &#125; else &#123; System.out.printf("(%d, %d) %n", value, target); &#125; &#125; &#125; /** * 先排序 O(n log(n)) */ public static void printPairsUsingTwoPointers(int[] numbers, int k) &#123; if (numbers.length &lt; 2) &#123; return; &#125; Arrays.sort(numbers); int left = 0; int right = numbers.length - 1; while (left &lt; right) &#123; int sum = numbers[left] + numbers[right]; if (sum == k) &#123; System.out.printf("(%d, %d) %n", numbers[left], numbers[right]); left = left + 1; right = right - 1; &#125; else if (sum &lt; k) &#123; left = left + 1; &#125; else if (sum &gt; k) &#123; right = right - 1; &#125; &#125; &#125; public static void main(String[] args)&#123; printPairs(new int[]&#123; 2, 4, 3, 5, 6, -2, 4, 7, 8, 9 &#125;, 12); System.out.println(); printPairsUsingSet(new int[]&#123; 2, 4, 3, 5, 6, -2, 4, 7, 8, 9 &#125;, 12); System.out.println(); printPairsUsingTwoPointers(new int[]&#123; 2, 4, 3, 5, 6, -2, 4, 7, 8, 9 &#125;, 12); &#125;&#125; 数组中重复的数据给定一个整数数组 a，其中1 ≤ a[i] ≤ n （n为数组长度）, 其中有些元素出现两次而其他元素出现一次。 找到所有出现两次的元素。 你可以不用到任何额外空间并在O(n)时间复杂度内解决这个问题吗？ 示例： 输入: 1[4,3,2,7,8,2,3,1] 输出: 1[2,3] 解题思路：这个题目开头暗示了n的范围，所以可以加以利用，将元素转换成数组的索引并对应的将该处的元素乘以-1； 若数组索引对应元素的位置本身就是负数，则表示已经对应过一次；在结果列表里增加该索引的正数就行； 1234567891011121314151617class Solution &#123; public List&lt;Integer&gt; findDuplicates(int[] nums) &#123; List&lt;Integer&gt; dupliacates = new ArrayList&lt;Integer&gt;(); for(int i : nums)&#123; if(i &lt; 0)&#123; i = -i; &#125; if(nums[i-1] &lt; 0)&#123; // 已经重复过 dupliacates.add(i); continue; &#125; nums[i-1] = -1 * nums[i-1]; &#125; return dupliacates; &#125;&#125; 快速排序快排采用的是分治的思想。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.Arrays;/** * 快速排序 * * @author austin * @since 2019/7/30 20:05 */public class QuickSort &#123; public static void sort(int[] nums) &#123; if (null == nums || nums.length == 0) &#123; return; &#125; quickSort(nums, 0, nums.length - 1); &#125; private static void quickSort(int[] nums, int low, int high) &#123; // 选取中位数 int pivot = nums[low + (high - low) / 2]; int left = low; int right = high; while (left &lt;= right) &#123; while (nums[left] &lt; pivot) &#123; left++; &#125; while (nums[right] &gt; pivot) &#123; right--; &#125; if (left &lt;= right) &#123; int temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; left++; right--; &#125; &#125; if (low &lt; right) &#123; quickSort(nums, low, right); &#125; if (high &gt; left) &#123; quickSort(nums, left, high); &#125; &#125; public static void main(String[] args)&#123; int[] unsorted = &#123;6, 5, 3, 1, 8, 7, 2, 4&#125;; System.out.println("Unsorted array :" + Arrays.toString(unsorted)); sort(unsorted); System.out.println("Sorted array :" + Arrays.toString(unsorted)); &#125;&#125; 前缀和技巧：和为K的子数组个数给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。 示例 1 : 输入:nums = [1,1,1], k = 2输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。 说明 : 数组的长度为 [1, 20,000]。数组中元素的范围是 [-1000, 1000] ，且整数 k 的范围是 [-1e7, 1e7]。 思路很简单，我把所有子数组都穷举出来，算它们的和，看看谁的和等于 k 不就行了。 关键是，如何快速得到某个子数组的和呢，比如说给你一个数组nums，让你实现一个接口sum(i, j)，这个接口要返回nums[i..j]的和，而且会被多次调用，你怎么实现这个接口呢？ 因为接口要被多次调用，显然不能每次都去遍历nums[i..j]，有没有一种快速的方法在 O(1) 时间内算出nums[i..j]呢？这就需要前缀和技巧了。 前缀和前缀和的思路是这样的，对于一个给定的数组nums，我们额外开辟一个前缀和数组进行预处理： 123456int n = nums.length;// 前缀和数组int[] preSum = new int[n + 1];preSum[0] = 0;for (int i = 0; i &lt; n; i++) preSum[i + 1] = preSum[i] + nums[i]; 这个前缀和数组preSum的含义也很好理解，preSum[i]就是nums[0..i-1]的和。那么如果我们想求nums[i..j]的和，只需要一步操作preSum[j+1]-preSum[i]即可，而不需要重新去遍历数组了。 回到这个子数组问题，我们想求有多少个子数组的和为 k，借助前缀和技巧很容易写出一个解法： 1234567891011121314151617181920class Solution &#123; public int subarraySum(int[] nums, int k) &#123; int[] preSum = new int[nums.length + 1]; // 构造前缀和 for(int i = 0; i &lt; nums.length; i++)&#123; preSum[i+1] = preSum[i] + nums[i]; &#125; int count = 0; // 穷举所有子数组 for(int i = 0; i &lt; preSum.length; i++)&#123; for(int j = i + 1; j &lt; preSum.length; j++)&#123; if(preSum[j] - preSum[i] == k)&#123; count++; &#125; &#125; &#125; return count; &#125;&#125; 这个解法的时间复杂度空间复杂度，并不是最优的解法。不过通过这个解法理解了前缀和数组的工作原理之后，可以使用一些巧妙的办法把时间复杂度进一步降低. 优化解优化的思路是：我直接记录下有几个sum[j]和sum[i]-k相等，直接更新结果，就避免了内层的 for 循环。我们可以用哈希表，在记录前缀和的同时记录该前缀和出现的次数。 123456789101112131415161718192021222324class Solution &#123; public int subarraySum(int[] nums, int k) &#123; // &lt;前缀和, 该前缀和出现次数&gt; HashMap&lt;Integer, Integer&gt; preSum = new HashMap&lt;&gt;(); preSum.put(0, 1); int count = 0; int currentSum = 0; for(int num : nums)&#123; currentSum += num; int sum = currentSum - k; // 该数之前已经有符合条件的前缀和 if(preSum.containsKey(sum))&#123; count += preSum.get(sum); &#125; if(!preSum.containsKey(currentSum))&#123; preSum.put(currentSum, 0); &#125; preSum.put(currentSum, preSum.get(currentSum) + 1); &#125; return count; &#125;&#125; 来源：【LeetCode】 560. 和为K的子数组参考：前缀和技巧：解决子数组问题 交换数组两部分比如数组【1,2,3,4,5】，要将【1,2,3】和【4,5】交换，得到【4,5,1,2,3】 先看一道类似的 LeetCode 题目： Given an array, rotate the array to the right by k steps, where k is non-negative. 举个例子： Input: [1,2,3,4,5,6,7] and k = 3Output: [5,6,7,1,2,3,4]Explanation:rotate 1 steps to the right: [7,1,2,3,4,5,6]rotate 2 steps to the right: [6,7,1,2,3,4,5]rotate 3 steps to the right: [5,6,7,1,2,3,4] 题目很好理解，实际上就是交换数组的两部分，只是换成“rotate k steps”的说法而已。等价于“把数组的最后 k 个元素和其他元素交换”。 一个细节区别就是，这里的 k 可以大于数组的长度，但只要让 k 与数组长度求模（余数）就行了，又转换成了“交换数组的两部分”这个问题。 思路一，想办法构造一个环 可以把这个数组转化成一个链表，然后把这个链表首尾相接，形成一个环，然后在适当的地方把这个环重新断开成为一个链表，最后把这个新得到的链表转换成数组，这个数组就是结果了。至于应该在什么地方重新断开这个环，跟 k 有关. 思路二，这个问题具有递归结构首先，如果交换的两部分元素数量一样（k == 数组长度 / 2），那么就比较简单，直接对应元素逐个互换就行了。（递归的 base case） 但是两部分元素如果不一样多，难点就来了，长的那部分交换会有剩余，这个剩余区域还需要和长区域经过交换的那个区域交换，以还原长区域的原始顺序，然后这个交换过程又会产生长短不一的问题，然后请重新阅读这段话。（递归调用） 思路三，终极解法 这个解法简单得让人诧异，只需要进行三次数组反转就行了：首先把数组的这两部分分别反转，然后把整个数组反转，就得到了答案。 其实，“交换数组的两部分”经过简单的变形，就变成了谷歌公司的一道面试题： 给你一个英文句子字符串，对它进行完全倒装（不允许使用额外空间） 比如说给面试者一句话： A cycle of boom and bust. 请你倒装成： bust and boom of cycle A. 原地倒装，不允许使用额外存储空间。 这个面试题问倒了不少面试者，因为正常人的思维总会陷入如何交换不同长度的单词，陷入复杂的条件判定和无限的细节中，这个人类看起来如此简单的问题对计算机来说似乎是个不可能完成的任务。但是你应该很容易想到解法：先把整个句子反转，再将每个单词分别反转，就得到了答案。 接雨水给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例: 输入: [0,1,0,2,1,0,1,3,2,1,2,1]输出: 6 双指针解法 123456789101112131415161718192021222324class Solution &#123; public int trap(int[] height) &#123; if(height.length &lt;= 2)&#123; return 0; &#125; int lMax = height[0]; int rMax = height[height.length - 1]; int count = 0, left = 0, right = height.length - 1; while(left &lt;= right)&#123; lMax = Math.max(lMax, height[left]); rMax = Math.max(rMax, height[right]); if(lMax &lt; rMax)&#123; count += lMax - height[left]; left++; &#125;else&#123; count += rMax - height[right]; right--; &#125; &#125; return count; &#125;&#125; 参考：【LeetCode42】 接雨水 详解一道高频面试题：接雨水 字符串最长回文子串给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： 输入: “babad”输出: “bab”注意: “aba” 也是一个有效答案。 示例 2：输入: “cbbd”输出: “bb” 中心扩展法寻找回文串的问题核心思想是：从中间开始向两边扩散来判断回文串。对于最长回文子串，就是这个意思： 123for 0 &lt;= i &lt; len(s): 找到以 s[i] 为中心的回文串 更新答案 但是呢，我们刚才也说了，回文串的长度可能是奇数也可能是偶数，如果是abba这种情况，没有一个中心字符，上面的算法就没辙了。所以我们可以修改一下： 1234for 0 &lt;= i &lt; len(s): 找到以 s[i] 为中心的回文串 找到以 s[i] 和 s[i+1] 为中心的回文串 更新答案 按照上面的思路，先要实现一个函数来寻找最长回文串: 1234567891011121314151617181920212223class Solution &#123; public String longestPalindrome(String s) &#123; String longest = ""; for(int i = 0; i &lt; s.length(); i++)&#123; // 以s[i]为中心的最长回文子串 String one = palindrome(s, i, i); // 以s[i], s[i+1]为中心的最长回文子串 String two = palindrome(s, i, i+1); longest = longest.length() &gt; one.length() ? longest : one; longest = longest.length() &gt; two.length() ? longest : two; &#125; return longest; &#125; public String palindrome(String s, int left, int right)&#123; while(left &gt;=0 &amp;&amp; right &lt; s.length() &amp;&amp; s.charAt(left) == s.charAt(right))&#123; left--; right++; &#125; return s.substring(left + 1, right); &#125;&#125; 【LeetCode】5. 最长回文子串 最长回文子序列给定一个字符串s，找到其中最长的回文子序列。可以假设s的最大长度为1000。 示例 1:输入:“bbbab”输出:4 一个可能的最长回文子序列为 “bbbb”。 示例 2:输入:“cbbd”输出:2 一个可能的最长回文子序列为 “bb”。 模板解题思路 1、第一种思路模板是一个一维的 dp 数组： 12345678int n = array.length;int[] dp = new int[n];for (int i = 1; i &lt; n; i++) &#123; for (int j = 0; j &lt; i; j++) &#123; dp[i] = 最值(dp[i], dp[j] + ...) &#125;&#125; 举个我们写过的例子最长递增子序列，在这个思路中 dp 数组的定义是： 在子数组 array[0..i] 中，以 array[i] 结尾的目标子序列（最长递增子序列）的长度是dp[i]。 为啥最长递增子序列需要这种思路呢？前文说得很清楚了，因为这样符合归纳法，可以找到状态转移的关系。 2、第二种思路模板是一个二维的 dp 数组： 1234567891011int n = arr.length;int[][] dp = new dp[n][n];for (int i = 0; i &lt; n; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; if (arr[i] == arr[j]) dp[i][j] = dp[i][j] + ... else dp[i][j] = 最值(...) &#125;&#125; 这种思路运用相对更多一些，尤其是涉及两个字符串/数组的子序列。本思路中 dp 数组含义又分为「只涉及一个字符串」和 「涉及两个字符串」 两种情况。 2.1 涉及两个字符串/数组时（比如最长公共子序列），dp 数组的含义如下：在子数组 arr1[0..i] 和子数组 arr2[0..j] 中，我们要求的子序列（最长公共子序列）长度为 dp[i][j]。 2.2 只涉及一个字符串/数组时（比如本文要讲的最长回文子序列），dp 数组的含义如下：在子数组 array[i..j] 中，我们要求的子序列（最长回文子序列）的长度为dp[i][j]。 本题思路dp 数组的定义是：在子串 s[i..j] 中，最长回文子序列的长度为 dp[i][j]。 具体来说，如果我们想求 dp[i][j]，假设你知道了子问题 dp[i+1][j-1] 的结果（s[i+1..j-1]中最长回文子序列的长度），你是否能想办法算出 dp[i][j] 的值（s[i..j] 中，最长回文子序列的长度）呢？ 可以！这取决于s[i]和 s[j] 的字符： 如果它俩相等，那么它俩加上 s[i+1..j-1] 中的最长回文子序列就是 s[i..j] 的最长回文子序列; 如果它俩不相等，说明它俩不可能同时出现在 s[i..j] 的最长回文子序列中，那么把它俩分别加入 s[i+1..j-1] 中，看看哪个子串产生的回文子序列更长即可; 以上两种情况写成代码就是这样： 123456if (s[i] == s[j]) // 它俩一定在最长回文子序列中 dp[i][j] = dp[i + 1][j - 1] + 2;else // s[i+1..j] 和 s[i..j-1] 谁的回文子序列更长？ dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]); 至此，状态转移方程就写出来了，根据 dp 数组的定义，我们要求的就是dp[0][n - 1]，也就是整个s的最长回文子序列的长度。 Java实现首先明确一下 base case，如果只有一个字符，显然最长回文子序列长度是 1，也就是 dp[i][j] = 1,(i == j)。 因为i肯定小于等于j，所以对于那些i &gt; j的位置，根本不存在什么子序列，应该初始化为 0。 另外，看看刚才写的状态转移方程，想求dp[i][j]需要知道dp[i+1][j-1]，dp[i+1][j]，dp[i][j-1]这三个位置；为了保证每次计算 dp[i][j]，左、下、左下三个方向的位置已经被计算出来，只能斜着遍历或者反着遍历： 12345678910111213141516171819202122class Solution &#123; public int longestPalindromeSubseq(String s) &#123; int len = s.length(); int[][] dp = new int[len][len]; // base case dp[i][i] = 1 for(int i = 0; i &lt; len; i++)&#123; dp[i][i] = 1; &#125; // 从左下角向右上角遍历 for(int i = len - 1; i &gt;= 0; i--)&#123; for(int j = i + 1; j &lt; len; j++)&#123; if(s.charAt(i) == s.charAt(j))&#123; dp[i][j] = 2 + dp[i+1][j-1]; &#125; else &#123; dp[i][j] = Math.max(dp[i+1][j], dp[i][j-1]); &#125; &#125; &#125; return dp[0][len-1]; &#125;&#125; 来源：LeetCode 516. 最长回文子序列 参考：子序列解题模板：最长回文子序列 链表链表是另一种常见的数据结构，和数组相似，链表也是线性的数据结构并且以线性方式存储元素。而与数组不同的是，链表不是将元素存储在连续的位置中，而是可以存储在任意位置，彼此之间通过节点相互连接。 链表也可以说就是一个节点列表，每个节点中包含存储的值和下一个节点的地址。也正是因为这种结构，在链表里添加和删除元素很容易，你只需要更改链接而不用创建新的数组。但是搜索会很困难，并且在单链表中找到一个元素就需要 O（n）个时间。 链表有多种形式，如：单链表，允许你在一个方向上进行遍历；双链表，可以在两个方向上进行遍历；循环链表，最后节点的指针指向第一个节点从而形成一个环形的链；因为链表是一种递归数据结构，所以在解决链表问题时，熟练掌握递归算法就显得更加重要了。 判断单链表是否存在环及求环入口点 先判断是否有环设置两个指针(fast, slow)，初始值都指向头，slow每次前进一步，fast每次前进二步，如果链表存在环，则fast必定先进入环，而slow后进入环，两个指针必定相遇。(当然，fast先行头到尾部为NULL，则为无环链表) 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public boolean hasCycle(ListNode head) &#123; ListNode slow = head; ListNode quick = head; while(null != quick)&#123; if(quick.next != null &amp;&amp; null != quick.next.next)&#123; quick = quick.next.next; &#125;else&#123; return false; &#125; slow = slow.next; if(slow == quick)&#123; return true; &#125; &#125; return false; &#125;&#125; 此问题可扩展至： 求循环链表任一节点“对面的”（最远端）的节点 算法同上，当quick到达起始节点或起始节点next时，slow指示的就是最远端的节点。 经过第1步确认存在环后，寻找环入口点： 算法描述： 当quick若与slow相遇时，slow肯定没有走遍历完链表，而quick已经在环内循环了n圈(1&lt;=n)。假设slow走了s步，则fast走了2s步（fast步数还等于s 加上在环上多转的n圈），设环长为r，则： 122s = s + nrs = nr 设整个链表长L，入口环与相遇点距离为x，起点到环入口点的距离为a。 12345a + x = s = nra + x = (n–1)r + r = (n-1)r + L - aa = (n-1)r + (L – a – x) (L – a – x) 为相遇点到环入口点的距离，由此可知，从链表头到环入口点等于(n-1)循环内环+相遇点到环入口点 于是我们从链表头、与相遇点分别设一个指针，每次各走一步，两个指针必定相遇，且相遇第一点为环入口点。 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode hasCycle(ListNode head) &#123; ListNode slow = head; ListNode quick = head.next.next; // 一定有环， 寻找相遇点 while(slow != quick)&#123; slow = slow.next; quick = quick.next.next; &#125; // quick指针重新指向head节点 quick = head; while(slow != quick)&#123; slow = slow.next; quick = quick.next; &#125; return slow; &#125;&#125; 此问题可扩展至： 判断两个单链表是否相交，如果相交，给出相交的第一个点（两个链表都不存在环）。 根据问题描述，两个单链表自相交点起，将合并为一个单链表，这是理解算法的关键。 算法描述： 将其中一个链表首尾相连，检测另外一个链表是否存在环，如果存在，则两个链表相交，而检测出来的依赖环入口即为相交的第一个点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if(null == headA || null == headB)&#123; return null; &#125; // 先将一个链表构成环 ListNode tail = headA; while(tail.next != null)&#123; tail = tail.next; &#125; tail.next = headA; ListNode slow = headB; ListNode quick = headB; while(quick != null)&#123; if(null != quick.next &amp;&amp; null != quick.next.next)&#123; quick = quick.next.next; &#125;else&#123; tail.next = null; return null; &#125; slow = slow.next; if(slow == quick)&#123; break; &#125; &#125; // 能够执行到此处一定是有环 quick = headB; while(slow != quick)&#123; slow = slow.next; quick = quick.next; &#125; tail.next = null; return slow; &#125;&#125; 单链表相交找到两个单链表相交的起始节点。 注意：如果两个链表没有交点，返回 null.在返回结果后，两个链表仍须保持原有的结构。可假定整个链表结构中没有循环。程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。 解题思路：双指针法 创建两个指针 pA 和 pB，分别初始化为链表 A 和 B 的头结点。然后让它们向后逐结点遍历。 当 pA 到达链表的尾部时，将它重定位到链表 B 的头结点 (你没看错，就是链表 B); 类似的，当 pB 到达链表的尾部时，将它重定位到链表 A 的头结点。 若在某一时刻 pA 和 pB 相遇，则 pA/pB 为相交结点。 想弄清楚为什么这样可行, 可以考虑以下两个链表: A={1,3,5,7,9,11} 和 B={2,4,9,11}，相交于结点 9。由于 B.length (=4) &lt; A.length (=6)，pB 比 pA 少经过 2 个结点，会先到达尾部。将 pB 重定向到 A 的头结点，pA 重定向到 B 的头结点后，pB 要比 pA 多走 2 个结点。因此，它们会同时到达交点。如果两个链表存在相交，它们末尾的结点必然相同。因此当 pA/pB 到达链表结尾时，记录下链表 A/B 对应的元素。若最后元素不相同，则两个链表不相交。 复杂度分析 时间复杂度 : O(m+n)O(m+n)。空间复杂度 : O(1)O(1)。 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if(headA == null || headB == null) &#123; return null; &#125; ListNode pA = headA, pB = headB; // 在这里第一轮体现在pA和pB第一次到达尾部会移向另一链表的表头, // 而第二轮体现在如果pA或pB相交就返回交点, 不相交最后就是null==null while(pA != pB)&#123; pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; &#125; return pA; &#125;&#125; 反转单链表两种思路：（1）迭代假设存在链表 1 → 2 → 3 → Ø，我们想要把它改成 Ø ← 1 ← 2 ← 3。 在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！ 12345678910111213141516171819202122/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; ListNode prev = null; ListNode cur = head; ListNode next = null; while(cur != null)&#123; next = cur.next; cur.next = prev; prev = cur; cur = next; &#125; return prev; &#125;&#125; 复杂度分析 时间复杂度：O(n)，假设 n 是列表的长度，时间复杂度是 O(n)。空间复杂度：O(1)。 （2）递归核心思想： 1head.next.next = head 12345678910111213141516171819/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; if(head == null || head.next == null)&#123; return head; &#125; ListNode p = reverseList(head.next); head.next.next = head; head.next = null; return p; &#125;&#125; 复杂度分析 时间复杂度：O(n)，假设 n 是列表的长度，那么时间复杂度为 O(n)。空间复杂度：O(n)，由于使用递归，将会使用隐式栈空间。递归深度可能会达到 n 层。 删除链表的倒数第N个节点双指针法：第一个指针从列表的开头向前移动 n+1n+1 步，而第二个指针将从列表的开头出发。现在，这两个指针被 n 个结点分开。我们通过同时移动两个指针向前来保持这个恒定的间隔，直到第一个指针到达最后一个结点。此时第二个指针将指向从最后一个结点数起的第 n 个结点。我们重新链接第二个指针所引用的结点的 next 指针指向该结点的下下个结点。 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode dummy = new ListNode(0); // 哑指针，防止极端情况 dummy.next = head; ListNode first = dummy; ListNode second = dummy; // Advances first pointer so that the gap between first and second is n nodes apart for (int i = 1; i &lt;= n + 1; i++) &#123; first = first.next; &#125; // Move first to the end, maintaining the gap while (first != null) &#123; first = first.next; second = second.next; &#125; second.next = second.next.next; return dummy.next; &#125;&#125; 二叉树二叉树是一种非常重要的数据结构，很多其它数据结构都是基于二叉树的基础演变而来的。 对于二叉树，有深度遍历和广度遍历，深度遍历有前序、中序以及后序三种遍历方法，广度遍历即我们平常所说的层次遍历。 因为树的定义本身就是递归定义，因此采用递归的方法去实现树的三种遍历不仅容易理解而且代码很简洁，而对于广度遍历来说，需要其他数据结构的支撑，比如堆了。所以，对于一段代码来说，可读性有时候要比代码本身的效率要重要的多。 二叉树深度遍历 递归方式 1234567891011121314151617181920212223242526272829303132/*** 前序遍历*/ public void preOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; System.out.print(root.val+" "); preOrderTraverse1(root.left); preOrderTraverse1(root.right); &#125;&#125;/*** 中序遍历*/ public void inOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; inOrderTraverse1(root.left); System.out.print(root.val+" "); inOrderTraverse1(root.right); &#125;&#125;/*** 后序遍历*/ public void postOrderTraverse1(TreeNode root) &#123; if (root != null) &#123; postOrderTraverse1(root.left); postOrderTraverse1(root.right); System.out.print(root.val+" "); &#125;&#125; 非递归方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/*** 前序遍历*/public void preOrderTraverse(TreeNode root) &#123; LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; System.out.println(cur.val); stack.push(cur); cur = cur.left; &#125; else &#123; cur = stack.pop(); cur = cur.right; &#125; &#125;&#125;/*** 中序遍历*/public void midOrderTraverse(TreeNode root) &#123; LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; else &#123; cur = stack.pop(); System.out.println(cur.val); cur = cur.right; &#125; &#125;&#125;/*** 后序遍历*/public void postOrderTraverse(TreeNode root) &#123; LinkedList&lt;TreeNode&gt; stack = new LinkedList&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; if (cur != null) &#123; stack.push(cur); cur = cur.right; &#125; else &#123; cur = stack.pop(); System.out.println(cur.val); cur = cur.left; &#125; &#125;&#125; 二叉树层遍历层次遍历的代码比较简单，只需要一个队列即可，先在队列中加入根结点。之后对于任意一个结点来说，在其出队列的时候，访问之。同时如果左孩子和右孩子有不为空的，入队列。代码如下： 1234567891011121314151617181920public void levelTraverse(TreeNode root)&#123; if(root == null)&#123; return; &#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); TreeNode cur; while (!queue.isEmpty())&#123; cur = queue.poll(); System.out.print(cur.val); if (cur.left != null)&#123; queue.offer(cur.left); &#125; if (cur.right != null)&#123; queue.offer(cur.right); &#125; &#125;&#125; 堆排序https://www.jianshu.com/p/a20e5ec158b3 回溯算法解决一个回溯问题，实际上就是一个决策树的遍历过程。你只需要思考 3 个问题： 1、路径：也就是已经做出的选择。 2、选择列表：也就是你当前可以做的选择。 3、结束条件：也就是到达决策树底层，无法再做选择的条件。 如果你不理解这三个词语的解释，没关系，我们后面会用「全排列」和「N 皇后问题」这两个经典的回溯算法问题来帮你理解这些词语是什么意思，现在你先留着印象。 代码方面，回溯算法的框架： 12345678910result = []def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) return for 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择 其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」，特别简单。 参考：https://leetcode-cn.com/problems/n-queens/solution/hui-su-suan-fa-xiang-jie-by-labuladong/ 全排列给定一个没有重复数字的序列，返回其所有可能的全排列。 示例: 输入: [1, 2, 3] 输出: [ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 解题思路其实这就是回溯算法，我们高中无师自通就会用，或者有的同学直接画出如下这棵回溯树：只要从根遍历这棵树，记录路径上的数字，其实就是所有的全排列。我们不妨把这棵树称为回溯算法的「决策树」。 为啥说这是决策树呢，因为你在每个节点上其实都在做决策。比如说你站在下图的红色节点上： 你现在就在做决策，可以选择 1 那条树枝，也可以选择 3 那条树枝。为啥只能在 1 和 3 之中选择呢？因为 2 这个树枝在你身后，这个选择你之前做过了，而全排列是不允许重复使用数字的。 现在可以解答开头的几个名词：[2] 就是「路径」，记录你已经做过的选择；[1,3] 就是「选择列表」，表示你当前可以做出的选择；「结束条件」就是遍历到树的底层，在这里就是选择列表为空的时候。 如果明白了这几个名词，可以把「路径」和「选择」列表作为决策树上每个节点的属性，比如下图列出了几个节点的属性： 我们定义的 backtrack 函数其实就像一个指针，在这棵树上游走，同时要正确维护每个节点的属性，每当走到树的底层，其「路径」就是一个全排列。 java实现 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;List&lt;Integer&gt;&gt;(); LinkedList&lt;Integer&gt; track = new LinkedList&lt;Integer&gt;(); backTrack(nums, track, res); return res; &#125; /** * 路径track: 记录已选路径 * 选择列表nums * 结束条件： nums中无元素 */ private void backTrack(int nums[], LinkedList&lt;Integer&gt; track, List&lt;List&lt;Integer&gt;&gt; res)&#123; // 触发结束条件 if(track.size() == nums.length)&#123; res.add(new LinkedList&lt;Integer&gt;(track)); return; &#125; for(int i = 0; i &lt; nums.length; i++)&#123; // 排除不合法的选择 if(track.contains(nums[i]))&#123; continue; &#125; // 做选择 track.add(nums[i]); // 进入下一层决策树 backTrack(nums, track, res); // 取消选择 track.removeLast(); &#125; &#125;&#125; N皇后问题n皇后问题研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 上图为 8 皇后问题的一种解法。 给定一个整数 n，返回所有不同的 n 皇后问题的解决方案。 每一种解法包含一个明确的 n 皇后问题的棋子放置方案，该方案中 ‘Q’ 和 ‘.’ 分别代表了皇后和空位。 解题思路 java实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class Solution &#123; public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) &#123; List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); String[][] queues = new String[n][n]; // 存储当前摆放方式 backTrack(queues, 0, res); return res; &#125; /** * 路径: queues中小于row的那些行都已经成功放置了皇后 * 选择列表： 第row行所有列都是纺织皇后的选择 * 结束条件： row == queues最后一行 */ private void backTrack(String[][] queues, int row, List&lt;List&lt;String&gt;&gt; res) &#123; // 触发结束条件 if (row == queues.length) &#123; res.add(this.formatResult(queues)); return; &#125; int n = queues[row].length; for (int col = 0; col &lt; n; col++) &#123; // 排除不合法选择 if (this.isUnderAttack(queues, row, col)) &#123; continue; &#125; // 做选择 queues[row][col] = "Q"; // 进入下一个决策树 backTrack(queues, row + 1, res); // 撤销选择 queues[row][col] = "."; &#125; &#125; // 判断当前摆放位置是否和已经摆放的冲突 private boolean isUnderAttack(String[][] queues, int row, int col) &#123; // 检查列是否有冲突 for (int i = 0; i &lt; row; i++) &#123; if ("Q".equals(queues[i][col])) &#123; return true; &#125; &#125; // 检查左上方是否有冲突 for (int i = row - 1, j = col - 1; i &gt;= 0 &amp;&amp; j &gt;= 0; i--, j--) &#123; if ("Q".equals(queues[i][j])) &#123; return true; &#125; &#125; int n = queues.length; // 检查右上方是否有冲突 for (int i = row - 1, j = col + 1; i &gt;= 0 &amp;&amp; j &lt; n; i--, j++) &#123; if ("Q".equals(queues[i][j])) &#123; return true; &#125; &#125; return false; &#125; private List&lt;String&gt; formatResult(String[][] queues) &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; queues.length; i++) &#123; StringBuilder sb = new StringBuilder(); for (int j = 0; j &lt; queues[0].length; j++) &#123; if ("Q".equals(queues[i][j])) &#123; sb.append(" ♦️ "); &#125; else &#123; sb.append(" O "); &#125; &#125; res.add(sb.toString()); &#125; return res; &#125;&#125; 动态规划最长公共子序列给定两个字符串 text1 和 text2，返回这两个字符串的最长公共子序列。 一个字符串的子序列是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。例如，”ace” 是 “abcde” 的子序列，但 “aec” 不是 “abcde” 的子序列。两个字符串的「公共子序列」是这两个字符串所共同拥有的子序列。 若这两个字符串没有公共子序列，则返回 0。 示例 1: 输入：text1 = “abcde”, text2 = “ace”输出：3解释：最长公共子序列是 “ace”，它的长度为 3。 示例 2: 输入：text1 = “abc”, text2 = “abc”输出：3解释：最长公共子序列是 “abc”，它的长度为 3。 示例 3: 输入：text1 = “abc”, text2 = “def”输出：0解释：两个字符串没有公共子序列，返回 0。 提示: 1 &lt;= text1.length &lt;= 10001 &lt;= text2.length &lt;= 1000 输入的字符串只含有小写英文字符。 动态规划解法 dp[i][j] 的含义是：对于s1[1..i]和s2[1..j]，它们的 LCS 长度是dp[i][j]。 公式： dp[ i ][ j ] = dp[ i-1 ][ j-1 ] + 1 ; s[i] == s[j]dp[ i ][ j ] = max(dp[ i-1 ][ j ], dp[ i ][ j-1 ]); s[i] ≠ s[j] 12345678910111213141516171819202122232425class Solution &#123; public int longestCommonSubsequence(String text1, String text2) &#123; int m = text1.length(); int n = text2.length(); int[][] dp = new int[m+1][n+1]; for(int i = 0; i &lt;= m ; i++)&#123; dp[i][0] = 0; &#125; for(int i = 0; i &lt;= n ; i++)&#123; dp[0][n] = 0; &#125; for(int i = 1; i &lt;= m; i++)&#123; for(int j = 1; j &lt;= n; j++)&#123; if(text1.charAt(i-1) == text2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1] + 1; &#125; else &#123; dp[i][j] = Math.max(dp[i-1][j], dp[i][j-1]); &#125; &#125; &#125; return dp[m][n]; &#125;&#125; 参考：【LeetCode】1143. 最长公共子序列 经典面试题：最长公共子序列 括号生成给出 n 代表生成括号的对数，请你写出一个函数，使其能够生成所有可能的并且有效的括号组合。 例如，给出 n = 3，生成结果为： 1234567[ &quot;((()))&quot;, &quot;(()())&quot;, &quot;(())()&quot;, &quot;()(())&quot;, &quot;()()()&quot;] 思路当我们清楚所有 i&lt;n 时括号的可能生成排列后，对与 i=n 的情况，我们考虑整个括号排列中最左边的括号。它一定是一个左括号，那么它可以和它对应的右括号组成一组完整的括号 “( )”，我们认为这一组是相比 n-1 增加进来的括号。 那么，剩下的括号要么在这一组新增的括号内部，要么在这一组新增括号的外部（右侧）。既然知道了 i&lt;n 的情况，那我们就可以对所有情况进行遍历： “(“ + 【i=p时所有括号的排列组合】 + “)” + 【j=(n-1-p)时所有括号的排列组合】 其中 p &lt;= n-1，且为非负整数。当上述 p 从 0 取到 n-1，所有情况就遍历完了。 动态规划实现 12345678910111213141516171819202122232425262728class Solution &#123; public List&lt;String&gt; generateParenthesis(int n) &#123; List&lt;List&lt;String&gt;&gt; dp = new ArrayList&lt;&gt;(); dp.add(Arrays.asList("")); // dp[n] = '(' + dp[i] + ')' + dp[j]; i + j = n-1 for(int k = 1; k &lt;= n; k++)&#123; dp.add(combine(dp, k-1)); &#125; return dp.get(n); &#125; private List&lt;String&gt; combine(List&lt;List&lt;String&gt;&gt; dp, int total)&#123; if(total == 0)&#123; return Arrays.asList("()"); &#125; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); for(int i = 0; i &lt;= total; i++)&#123; for(String first : dp.get(i))&#123; String sb = "(" + first + ')'; for(String second : dp.get(total - i))&#123; list.add(sb + second); &#125; &#125; &#125; return list; &#125;&#125; 参考:【LeetCode】22. 括号生成 零钱兑换 给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。 示例 1: 输入: coins = [1, 2, 5], amount = 11输出: 3解释: 11 = 5 + 5 + 1 示例 2: 输入: coins = [2], amount = 3输出: -1 说明:你可以认为每种硬币的数量是无限的。 动态规划(自顶向下) 假设我们知道 F(S) ，对于金额 S 最少的硬币数，最后一枚硬币的面值是 C。那么由于问题的最优子结构以下方程应为： F(S) = F(S - C) + 1但我们不知道最后一枚硬币的面值是多少。我们计算每个硬币 c0, c1, …, cn​, 并选择其中的最小值。下列递推关系成立： 123456789101112131415161718192021class Solution &#123; public int coinChange(int[] coins, int amount) &#123; if(amount == 0)&#123; return 0; &#125; int min = Integer.MAX_VALUE; for(int coin : coins)&#123; // 金额不可达 if(amount - coin &lt; 0)&#123; continue; &#125; int res = coinChange(coins, amount - coin); if(res == -1)&#123; // 子问题无解 continue; &#125; min = Math.min(res + 1, min); &#125; return min == Integer.MAX_VALUE ? -1 : min; &#125;&#125; 动态规划(自下向上) 123456789101112131415161718192021class Solution &#123; public int coinChange(int[] coins, int amount) &#123; if(amount == 0)&#123; return 0; &#125; int[] dp = new int[amount + 1]; Arrays.fill(dp, amount + 1); // 一定需要初始化，并不是每一个金额都有兑换方案 dp[0] = 0; for(int i = 1; i &lt; dp.length; i++)&#123; for(int coin : coins)&#123; if(i &lt; coin)&#123; continue; &#125; dp[i] = Math.min(dp[i], dp[i-coin] + 1); &#125; &#125; return dp[amount] &gt; amount ? -1 : dp[amount]; &#125;&#125; 编辑距离给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1: 输入: word1 = “horse”, word2 = “ros” 输出: 3 解释: horse -&gt; rorse (将 ‘h’ 替换为 ‘r’)rorse -&gt; rose (删除 ‘r’)rose -&gt; ros (删除 ‘e’) 示例 2: 输入: word1 = “intention”, word2 = “execution” 输出: 5 解释: intention -&gt; inention (删除 ‘t’)inention -&gt; enention (将 ‘i’ 替换为 ‘e’)enention -&gt; exention (将 ‘n’ 替换为 ‘x’)exention -&gt; exection (将 ‘n’ 替换为 ‘c’)exection -&gt; execution (插入 ‘u’) 解题思路 编辑距离算法被数据科学家广泛应用，是用作机器翻译和语音识别评价标准的基本算法。 最简单的方法是检查所有可能的编辑序列，从中找出最短的一条。但这个序列总数可能达到指数级，但完全不需要这么多，因为我们只要找到距离最短的那条而不是所有可能的序列。 动态规划 我们的目的是让问题简单化，比如说两个单词 horse 和 ros 计算他们之间的编辑距离 D，容易发现，如果把单词变短会让这个问题变得简单，很自然的想到用 D[n][m] 表示输入单词长度为 n 和 m 的编辑距离。 具体来说，D[i][j] 表示 word1 的前 i 个字母和 word2 的前 j 个字母之间的编辑距离。 当我们获得 D[i-1][j]，D[i][j-1] 和 D[i-1][j-1] 的值之后就可以计算出 D[i][j]。 每次只可以往单个或者两个字符串中插入一个字符 那么递推公式很显然了 如果两个子串的最后一个字母相同，word1[i] = word2[i] 的情况下： D[i][j]=1 + min(D[i−1][j], D[i][j−1], D[i−1][j−1]−1) 否则，word1[i] != word2[i] 我们将考虑替换最后一个字符使得他们相同： D[i][j]=1 + min(D[i−1][j], D[i][j−1], D[i−1][j−1]) 所以每一步结果都将基于上一步的计算结果，示意如下： 同时，对于边界情况，一个空串和一个非空串的编辑距离为 D[i][0] = i 和 D[0][j] = j。 递归实现 从头部开始 1234567891011121314151617181920212223242526class Solution &#123; /** * 当 word1[i] == word2[j]，dp[i][j] = dp[i-1][j-1]； * 当 word1[i] != word2[j]，dp[i][j] = min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) + 1 */ public int minDistance(String word1, String word2) &#123; int len1 = word1.length(); int len2 = word2.length(); if(len1 == 0 || len2 == 0)&#123; return len1 == 0 ? len2 : len1; &#125; int minDistance = 0; if(word1.charAt(0) == word2.charAt(0))&#123; minDistance = minDistance(word1.substring(1), word2.substring(1)); &#125; else &#123; int replace = minDistance(word1.substring(1), word2.substring(1)); int insert = minDistance(word1, word2.substring(1)); int delete = minDistance(word1.substring(1), word2); minDistance = Math.min(Math.min(replace, insert), delete) + 1; &#125; return minDistance; &#125;&#125; 动态规划实现 1234567891011121314151617181920212223242526class Solution &#123; public int minDistance(String word1, String word2) &#123; int l1 = word1.length(); int l2 = word2.length(); int[][] dp = new int[l1 + 1][l2 + 1]; for(int i = 1; i &lt;= l1; i++)&#123; dp[i][0] = i; &#125; for(int i = 1; i &lt;= l2; i++)&#123; dp[0][i] = i; &#125; for(int i = 1; i &lt;= l1; i++)&#123; for(int j = 1; j &lt;= l2; j++)&#123; if(word1.charAt(i-1) == word2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1]; &#125; else &#123; dp[i][j] = 1 + Math.min(dp[i-1][j-1], Math.min(dp[i-1][j], dp[i][j-1])); &#125; &#125; &#125; return dp[l1][l2]; &#125;&#125; 扩展 既然每个dp[i][j]只和它附近的三个状态有关，空间复杂度是可以压缩成 O(min(M,N)) 的（M，N 是两个字符串的长度）。不难，但是可解释性大大降低。 你可能还会问，这里只求出了最小的编辑距离，那具体的操作是什么？这个其实很简单，代码稍加修改，给 dp 数组增加额外的信息即可： 1234567891011// int[][] dp;Node[][] dp;class Node &#123; int val; int choice; // 0 代表啥都不做 // 1 代表插入 // 2 代表删除 // 3 代表替换&#125; val属性就是之前的 dp 数组的数值，choice属性代表操作。在做最优选择时，顺便把操作记录下来，然后就从结果反推具体操作。 我们的最终结果不是dp[m][n]吗，这里的val存着最小编辑距离，choice存着最后一个操作. 参考： 详解一道腾讯面试题：编辑距离 贪心算法（Greedy Algorithm)贪心算法，又名贪婪法，是寻找最优解问题的常用方法，这种方法模式一般将求解过程分成若干个步骤，但每个步骤都应用贪心原则，选取当前状态下最好/最优的选择（局部最有利的选择），并以此希望最后堆叠出的结果也是最好/最优的解。{看着这个名字，贪心，贪婪这两字的内在含义最为关键。这就好像一个贪婪的人，他事事都想要眼前看到最好的那个，看不到长远的东西，也不为最终的结果和将来着想，贪图眼前局部的利益最大化，有点走一步看一步的感觉。} 贪婪法的基本步骤： 步骤1：从某个初始解出发； 步骤2：采用迭代的过程，当可以向目标前进一步时，就根据局部最优策略，得到一部分解，缩小问题规模； 步骤3：将所有解综合起来。 最优子结构 当一个问题的最优解包括其子问题的最优解时，称此问题具有最优子结构性质。 运用贪心策略在每一次转化时都取得了最优解。问题的最优子结构性质是该问题可用贪心算法或动态规划算法求解的关键特征。贪心算法的每一次操作都对结果产生直接影响，而动态规划则不是。贪心算法对每个子问题的解决方案都做出选择，不能回退；动态规划则会根据以前的选择结果对当前进行选择，有回退功能。动态规划主要运用于二维或三维问题，而贪心一般是一维问题. 买卖股票最佳时机给定一个数组，它的第i个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: [7,1,5,3,6,4]输出: 7 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 输入: [1,2,3,4,5]输出: 4 解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 输入: [7,6,4,3,1]输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 方法一：暴力法这种情况下，我们只需要计算与所有可能的交易组合相对应的利润，并找出它们中的最大利润。 class Solution { public int maxProfit(int[] prices) { return maxProfit(prices, 0); } private static int maxProfit(int[] prices, int start){ if(start &gt;= prices.length){ // 到最后一天不能买入 return 0; } int max = 0; for(int i = start; i &lt; prices.length; i++){ int curProfilt = 0; for(int j = i + 1; j &lt; prices.length; j++){ if(prices[i] &lt; prices[j]){ int profilt = maxProfit(prices, j+1) + prices[j] - prices[i]; if(profilt &gt; curProfilt){ curProfilt = profilt; } } } if(max &lt; curProfilt){ max = curProfilt; } } return max; } } 方法二：峰谷法假设给定的数组为： [7, 1, 5, 3, 6, 4] 如果我们在图表上绘制给定数组中的数字，我们将会得到: 如果我们分析图表，那么我们的兴趣点是连续的峰和谷。用数学语言描述为： ![-w419](/images/al_peak_gongshi.jpg) 关键是我们需要考虑到紧跟谷的每一个峰值以最大化利润。如果我们试图跳过其中一个峰值来获取更多利润，那么我们最终将失去其中一笔交易中获得的利润，从而导致总利润的降低。 例如，在上述情况下，如果我们跳过 `peak_ipeak_j` 和 `valley_jvalley_j`试图通过考虑差异较大的点以获取更多的利润，获得的净利润总是会小与包含它们而获得的静利润，因为 C 总是小于 A+B。 class Solution { public int maxProfit(int[] prices) { if(prices.length == 0){ return 0; } int maxProfit = 0; int vallege = prices[0]; int peak = prices[0]; int i = 0; while(i &lt; prices.length - 1){ // 寻找波谷 while(i+1 &lt; prices.length &amp;&amp; prices[i] &gt;= prices[i+1]){ i++; } vallege = prices[i]; // 寻找波峰 while(i+1 &lt; prices.length &amp;&amp; prices[i] &lt;= prices[i+1]){ i++; } peak = prices[i]; maxProfit += peak - vallege; } return maxProfit; } } 方法三：简单的一次遍历 该解决方案遵循 方法二 的本身使用的逻辑，但有一些轻微的变化。在这种情况下，我们可以简单地继续在斜坡上爬升并持续增加从连续交易中获得的利润，而不是在谷之后寻找每个峰值。最后，我们将有效地使用峰值和谷值，但我们不需要跟踪峰值和谷值对应的成本以及最大利润，但我们可以直接继续增加加数组的连续数字之间的差值，如果第二个数字大于第一个数字，我们获得的总和将是最大利润。这种方法将简化解决方案。 class Solution { public int maxProfit(int[] prices) { int maxProfit = 0; for(int i = 0; i &lt; prices.length - 1; i++){ if (prices[i] &lt; prices[i+1]){ maxProfit += prices[i+1] - prices[i]; } } return maxProfit; } } 参考： 【LeetCode】122. 买卖股票的最佳时机 II 常见思路10亿个数字里里面找最小的10个(1) 分治(Hash拆分) + topK(2) 先取10个构建最大堆，在循环更新（TopK） 有1亿个数字，其中有2个是重复的，快速找到它，时间和空间要最优(1) BitSet 2亿个随机生成的无序整数,找出中间大小的值https://flykite.me/?p=41(1) 二分思想在最开始的时候，以0为分界线，对所有整数进行遍历并统计小于0的整数个数和大于等于0的整数个数，假设小于0的整数有94,632,563个，那么大于等于0的数就有105,367,437个，这也就意味着，我们需要的那两个数是大于等于0的。 接下来我们就可以向第一次遍历那样对数据进行拆分了，我们知道int类型正整数最大值是2^31 ，那么第二次遍历我们就以2^30 作为分界线吧，统计小于2^30 的数与大于等于2^30 的数，假设小于2^30 的数有36,524,163个，大于2^30 次方的数有68,843,274个，那么我们需要的那两个数处于0到2^30 -1这个闭区间内。 按照这个方法，一次又一次第进行遍历，当我们统计出我们需要的那两个数处于某一个区间内，并且这个区间内的数比较少，至少能让我们直接在内存中进行排序时，我们就可以将符合这个区间的数全部读取到内存中排序 （2）如果数据不重复，可采用位图排序位图排序是一种空间换时间的排序算法，时间复杂度仅为O(n)，但它的限制很多，比如数据不能有重复项，在排序之前必须知道数据的范围（最小值及最大值，或者大致范围），范围越宽广，占用的内存空间就越大。 打个比方，假设有一万个不重复的整数，已知这些整数里最小的数为0，最大的数为100,000，这时候，我们可以申请一个长度为100,001位的内存空间（即97.66KB），然后遍历这一万个整数并且将内存空间中对应位置的位设为1，在遍历结束后排序也已经结束了。 可能这段话还不是很能让你理解透彻，这样吧，把数据量再缩小一些，比如有[19, 36, 3, 42, 11, 26, 5, 9, 24]这样一个数组，假设我们已经知道这个数组最小值为3，最大值为42，这时候我们就可以申请一个长度为40位的内存空间，如下： 00000 00000 00000 00000 00000 00000 00000 00000 （为了看起来方便，这里以5位相隔一个空格来表示） 对该数组进行遍历，并且将每个整数的值减去3之后，将对应位置设为1，结果如下： 10100 01010 00000 01000 01010 00000 00010 00001 从这串0和1中，我们能看到这里的第0位、第2位、第6位都为1，这几个1的位置加上数组中的最小值3则表示的是3, 5, 9这几个数。 给一个不知道长度的(可能很大)输入字符串，设计一种方案，将重复的字符排重定义一个字符数组记录字符出现次数 int[] a = new int[256];再遍历一遍，删除重复字符 有2n+1个数，其中有n个数出现过2次，找出其中只出现一次的数算法的原理就是：任何数异或0值不变，任何数与自己异或值为0 有3n+1个数字，其中n个数出现过3次，只有1个是不重复的（1）这个用异或不可以可以设置一个长度为32的int数组。统计每位上出现1的次数，如果次数能被3整除，说明x该位上为0，否则为1https://blog.csdn.net/u011960402/article/details/17750993]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机性能监控与故障处理工具]]></title>
    <url>%2F2019%2F08%2F12%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E4%B8%8E%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[概述本文参考的是周志明的 《深入理解Java虚拟机》 第四章 ，为了整理思路，简单记录一下，方便后期查阅。 JDK本身提供了很多方便的JVM性能调优监控工具，除了集成式的VisualVM和jConsole外，还有jps、jstack、jmap、jhat、jstat、hprof等小巧的工具，本文希望能起抛砖引玉之用，让大家能开始对JVM性能调优的常用工具有所了解。 JDK的命令行工具 命令名称 全称 用途 jstat JVM Statistics Monitoring Tool 用于收集Hotspot虚拟机各方面的运行数据 jps JVM Process Status Tool 显示指定系统内所有的HotSpot虚拟机进程 jinfo Configuration Info for Java 显示虚拟机配置信息 jmap JVM Memory Map 生成虚拟机的内存转储快照，生成heapdump文件 jhat JVM Heap Dump Browser 用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户在浏览器上查看分析结果 jstack JVM Stack Trace 显示虚拟机的线程快照 详情参考： https://www.ymq.io/2017/08/01/jvm-4/]]></content>
      <categories>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Jvm</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程面试集锦]]></title>
    <url>%2F2019%2F08%2F01%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6%2F</url>
    <content type="text"><![CDATA[什么是线程？进程？协程？进程 具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位.（比如一个qq程序就是一个进程或者多个进程），系统进行资源分配的最小单位.。进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全 线程程序执行流的最小单元（操作系统可识别的最小执行和调度单位）。也可以理解线程是一个程序里面不同的执行路径。是为了提高cpu的利用率而设计的。线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。线程是轻量级的进程，它们是共享在父进程拥有的资源下，每个线程在父进程的环境中顺序的独立的执行一个活动，每个CPU核心在同一时刻只能执行一个线程，尽管我们有时感觉自己的计算机同时开着多个任务，其实他们每个的执行都是走走停停的，CPU轮流给每个进程及线程分配时间。 协程（线程的线程）协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 什么是线程安全和线程不安全？ 通俗的说：加锁的就是是线程安全的，不加锁的就是是线程不安全的 线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问，直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。 一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。 Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。 线程不安全线程不安全：就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。线程安全问题都是由全局变量及静态变量引起的。 若每个线程中对全局变量、静态变量只有读操作，而无写操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步，否则的话就可能影响线程安全。 什么是自旋锁？自旋锁 是指当一个线程在获取锁的时候，如果锁已经被其它线程获取，那么该线程将循环等待，然后不断的判断锁是否能够被成功获取，直到获取到锁才会退出循环。获取锁的线程一直处于活跃状态，但是并没有执行任何有效的任务，使用这种锁会造成busy-waiting。 它是为实现保护共享资源而提出一种锁机制。其实，自旋锁与互斥锁比较类似，它们都是为了解决对某项资源的互斥使用。无论是互斥锁，还是自旋锁，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，”自旋”一词就是因此而得名。 Java如何实现自旋锁1234567891011121314public class SpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); public void lock() &#123; Thread current = Thread.currentThread(); // 利用CAS while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread current = Thread.currentThread(); cas.compareAndSet(current, null); &#125;&#125; lock() 方法利用的CAS，当第一个线程A获取锁的时候，能够成功获取到，不会进入while循环，如果此时线程A没有释放锁，另一个线程B又来获取锁，此时由于不满足CAS，所以就会进入while循环，不断判断是否满足CAS，直到A线程调用unlock方法释放了该锁。 自旋锁优缺点 缺点 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。 上面Java实现的自旋锁不是公平的，即无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。 优点 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。 （线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能） 可重入的自旋锁和不可重入的自旋锁上面那段代码，仔细分析一下就可以看出，它是不支持重入的，即当一个线程第一次已经获取到了该锁，在锁释放之前又一次重新获取该锁，第二次就不能成功获取到。由于不满足CAS，所以第二次获取会进入while循环等待，而如果是可重入锁，第二次也是应该能够成功获取到的。 而且，即使第二次能够成功获取，那么当第一次释放锁的时候，第二次获取到的锁也会被释放，而这是不合理的。 为了实现可重入锁，我们需要引入一个计数器，用来记录获取锁的线程数。 12345678910111213141516171819202122232425public class ReentrantSpinLock &#123; private AtomicReference&lt;Thread&gt; cas = new AtomicReference&lt;Thread&gt;(); private int count; public void lock() &#123; Thread current = Thread.currentThread(); if (current == cas.get()) &#123; // 如果当前线程已经获取到了锁，线程数增加一，然后返回 count++; return; &#125; // 如果没获取到锁，则通过CAS自旋 while (!cas.compareAndSet(null, current)) &#123; // DO nothing &#125; &#125; public void unlock() &#123; Thread cur = Thread.currentThread(); if (cur == cas.get()) &#123; if (count &gt; 0) &#123;// 如果大于0，表示当前线程多次获取了该锁，释放锁通过count减一来模拟 count--; &#125; else &#123;// 如果count==0，可以将锁释放，这样就能保证获取锁的次数与释放锁的次数是一致的了。 cas.compareAndSet(cur, null); &#125; &#125; &#125;&#125; 自旋锁与互斥锁 自旋锁与互斥锁都是为了实现保护资源共享的机制。 无论是自旋锁还是互斥锁，在任意时刻，都最多只能有一个保持者。 获取互斥锁的线程，如果锁已经被占用，则该线程将进入睡眠状态；获取自旋锁的线程则不会睡眠，而是一直循环等待锁释放。 总结 自旋锁：线程获取锁的时候，如果锁被其他线程持有，则当前线程将循环等待，直到获取到锁。 自旋锁等待期间，线程的状态不会改变，线程一直是用户态并且是活动的(active)。 自旋锁如果持有锁的时间太长，则会导致其它等待获取锁的线程耗尽CPU。 自旋锁本身无法保证公平性，同时也无法保证可重入性。 基于自旋锁，可以实现具备公平性和可重入性质的锁。 参考： https://segmentfault.com/a/1190000015795906#articleHeader0 什么是Java内存模型？ VM包括两个子系统和两个组件。 两个子系统： Class loader（类装载）根据给定的全限定名类名(如：java.lang.Object)来装载class文件到Runtime data area中的method area。程序中可以extends java.lang.ClassLoader类来实现自己的Class loader。 Execution engine（执行引擎）执行classes中的指令。任何JVM specification实现(JDK)的核心都是Execution engine，不同的JDK例如Sun的JDK和IBM的JDK好坏主要就取决于他们各自实现的Execution engine的好坏。 两个组件 Native interface(本地接口)与native libraries交互，是其它编程语言交互的接口。当调用native方法的时候，就进入了一个全新的并且不再受虚拟机限制的世界，所以也很容易出现JVM无法控制的native heap OutOfMemory。 Runtime data area（运行时数据区）这就是我们常说的JVM的内存。主要分为五个部分： 方法区 有时候也成为永久代，在该区内很少发生垃圾回收，但是并不代表不发生GC，在这里进行的GC主要是对方法区里的常量池和对类型的卸载 方法区主要用来存储已被虚拟机加载的类的信息、常量、静态变量和即时编译器编译后的代码等数据，方法区也称持久代（Permanent Generation）。 该区域是被线程共享的。 方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量并不一定是编译时确定，运行时生成的常量也会存在这个常量池中。 虚拟机栈 虚拟机栈也就是我们平常所称的栈内存, 它为java方法服务，每个方法在执行的时候都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接和方法出口等信息。 虚拟机栈是线程私有的，它的生命周期与线程相同。 局部变量表里存储的是基本数据类型、returnAddress类型（指向一条字节码指令的地址）和对象引用，这个对象引用有可能是指向对象起始地址的一个指针，也有可能是代表对象的句柄或者与对象相关联的位置。局部变量所需的内存空间在编译器间确定 操作数栈的作用主要用来存储运算结果以及运算的操作数，它不同于局部变量表通过索引来访问，而是压栈和出栈的方式 每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接. 动态链接就是将常量池中的符号引用在运行期转化为直接引用 可通过参数-Xss设置栈容量 本地方法栈本地方法栈和虚拟机栈类似，只不过本地方法栈为Native方法服务 堆 Java堆是所有线程所共享的一块内存，在虚拟机启动时创建 Java堆唯一的目的是存放对象实例，几乎所有的对象实例和数组都在这里创建，因此该区域经常发生垃圾回收操作 可通过参数 -Xms 和-Xmx设置 Java堆为了便于更好的回收和分配内存，可以细分为：新生代和老年代； 新生代：包括Eden区、From Survivor区、To Survivor区，系统默认大小Eden:Survivor=8:1。 老年代：在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 程序计数器内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理和线程恢复等功能都需要依赖这个计数器完成。该内存区域是唯一一个java虚拟机规范没有规定任何OOM情况的区域 什么是CAS？介绍CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A 时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。 CAS 不通过JVM，直接利用java本地方 JNI（Java Native Interface为JAVA本地调用），直接调用CPU 的cmpxchg（汇编指令）指令。利用CPU的CAS指令，同时借助JNI来完成Java的非阻塞算法,实现原子操作。其它原子操作都是利用类似的特性完成的。 整个java.util.concurrent都是建立在CAS之上的，因此对于synchronized阻塞算法，J.U.C在性能上有了很大的提升。 CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS优点确保对内存的读-改-写操作都是原子操作执行 CAS缺点CAS虽然很高效的解决原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大和只能保证一个共享变量的原子操作 总结 使用CAS在线程冲突严重时，会大幅降低程序性能；CAS只适合于线程冲突较少的情况使用。 synchronized在jdk1.6之后，已经改进优化。synchronized的底层实现主要依靠Lock-Free的队列，基本思路是自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 什么是乐观锁和悲观锁？悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中 synchronized和 ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中 java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 使用场景 乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 悲观锁就比较合适多写的场景。多写的情况，一般会经常产生冲突，如果使用乐观锁，就会导致上层应用会不断的进行retry，这样反倒是降低了性能。 乐观锁常见的两种实现方式 乐观锁一般会使用版本号机制或CAS算法实现。 版本号机制一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。 举一个简单的例子：假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。 1. 操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。 1. 在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。 2. 操作员 A 完成了修改工作，将数据版本号加一（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。 3. 操作员 B 完成了操作，也将版本号加一（ version=2 ）试图向数据库提交数据（ balance=$80 ），但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，数据库记录当前版本也为 2 ，不满足 “ 提交版本必须大于记录当前版本才能执行更新 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。 CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数 需要读写的内存值 V 进行比较的值 A 拟写入的新值 B 当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。 乐观锁的缺点 1 ABA 问题如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 “ABA”问题。 JDK 1.5 以后的 AtomicStampedReference类就提供了此种能力，其中的 compareAndSet方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2 循环时间长开销大自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 3 只能保证一个共享变量的原子操作CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。但是从 JDK 1.5开始，提供了 AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用 AtomicReference类把多个共享变量合并成一个共享变量来操作。 CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多） 对于资源竞争较少（线程冲突较轻）的情况，使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。 对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 *和 *轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。 什么是AQS？简介AbstractQueuedSynchronizer简称AQS，是一个用于构建锁和同步容器的框架。事实上concurrent包内许多类都是基于AQS构建，例如ReentrantLock，Semaphore，CountDownLatch，ReentrantReadWriteLock，FutureTask等。AQS解决了在实现同步容器时设计的大量细节问题。 AQS使用一个FIFO的队列表示排队等待锁的线程，队列头节点称作“哨兵节点”或者“哑节点”，它不与任何线程关联。其他的节点与等待线程关联，每个节点维护一个等待状态waitStatus。 CAS 原子操作在concurrent包的实现参考 https://blog.52itstyle.com/archives/948/ 由于java的CAS同时具有 volatile 读和volatile写的内存语义，因此Java线程之间的通信现在有了下面四种方式： A线程写volatile变量，随后B线程读这个volatile变量。 A线程写volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 Java的CAS会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile变量的读/写和CAS可以实现线程之间的通信。把这些特性整合在一起，就形成了整个concurrent包得以实现的基石。 如果我们仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为volatile； 然后，使用CAS的原子条件更新来实现线程之间的同步； 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（Java.util.concurrent.atomic包中的类），这些concurrent包中的基础类都是使用这种模式来实现的，而concurrent包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent包的实现示意图如下： AQS实现AQS没有锁之类的概念，它有个state变量，是个int类型，在不同场合有着不同含义。 AQS围绕state提供两种基本操作“获取”和“释放”，有条双向队列存放阻塞的等待线程，并提供一系列判断和处理方法，简单说几点： state是独占的，还是共享的； state被获取后，其他线程需要等待； state被释放后，唤醒等待线程； 线程等不及时，如何退出等待。 至于线程是否可以获得state，如何释放state，就不是AQS关心的了，要由子类具体实现。 例如ReentrantLocky用它表示线程重入锁的次数，Semaphore用它表示剩余的许可数量，FutureTask用它表示任务的状态。对state变量值的更新都采用CAS操作保证更新操作的原子性。 AbstractQueuedSynchronizer继承了AbstractOwnableSynchronizer，这个类只有一个变量：exclusiveOwnerThread，表示当前占用该锁的线程，并且提供了相应的get，set方法。 什么是原子操作？在Java Concurrent API中有哪些原子类(atomic classes)？原子操作是指一个不受其他操作影响的操作任务单元。原子操作是在多线程环境下避免数据不一致必须的手段。 Atomic包一共提供了13个类，包含四种类型的原子更新方式，分别是： 基本类型 AtomicBoolean AtomicInteger AtomicLong 数组类型 AtomicIntegerArray AtomicLongArray AtomicReferenceArray 引用类型 AtomicReference AtomicReferenceFieldUpdater AtomicMarkableReference 属性类型 AtomicIntegeFeildUpdater AtomicLongFieldUpdater AtomicStampedRefernce 什么是Executors框架？Executor框架同java.util.concurrent.Executor 接口在Java 5中被引入，是一个静态工具类。Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。 无限制的创建线程会引起应用程序内存溢出。所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。 利用Executors框架可以非常方便的创建一个线程池，Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 什么是阻塞队列？如何使用阻塞队列来实现生产者-消费者模型？什么是阻塞队列？阻塞队列是一个在队列基础上又支持了两个附加操作的队列。 2个附加操作： 支持阻塞的插入方法队列满时，队列会阻塞插入元素的线程，直到队列不满。 支持阻塞的移除方法队列空时，获取元素的线程会等待队列变为非空。 几个关键方法在阻塞队列不可用的时候，上述2个附加操作提供了四种处理方法 方法/处理方式 抛出异常 返回特殊值 一直阻塞 超时退出 插入方法 add(e) offer(e) put(e) offer(e,time,unit) 移除方法 remove() poll() take() poll(time,unit) 检查方法 element() peek() 不可用 不可用 阻塞队列的应用场景阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。简而言之，阻塞队列是生产者用来存放元素、消费者获取元素的容器。 java里的阻塞队列JDK 7 提供了7个阻塞队列，如下 1、ArrayBlockingQueue 数组结构组成的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序，但是默认情况下不保证线程公平的访问队列，即如果队列满了，那么被阻塞在外面的线程对队列访问的顺序是不能保证线程公平（即先阻塞，先插入）的。 2、LinkedBlockingQueue一个由链表结构组成的有界阻塞队列此队列按照先出先进的原则对元素进行排序 3、PriorityBlockingQueue支持优先级的无界阻塞队列 4、DelayQueue支持延时获取元素的无界阻塞队列，即可以指定多久才能从队列中获取当前元素 5、SynchronousQueue不存储元素的阻塞队列，每一个put必须等待一个take操作，否则不能继续添加元素。并且他支持公平访问队列。 6、LinkedTransferQueue由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，多了tryTransfer和transfer方法 transfer方法如果当前有消费者正在等待接收元素（take或者待时间限制的poll方法），transfer可以把生产者传入的元素立刻传给消费者。如果没有消费者等待接收元素，则将元素放在队列的tail节点，并等到该元素被消费者消费了才返回。 tryTransfer方法用来试探生产者传入的元素能否直接传给消费者。如果没有消费者在等待，则返回false。和上述方法的区别是该方法无论消费者是否接收，方法立即返回。而transfer方法是必须等到消费者消费了才返回。 7、LinkedBlockingDeque链表结构的双向阻塞队列，优势在于多线程入队时，减少一半的竞争。 什么是Callable和Future? Callable 和 Future 是比较有趣的一对组合。当我们需要获取线程的执行结果时，就需要用到它们。Callable用于产生结果，Future用于获取结果。 Callable接口使用泛型去定义它的返回类型。Executors类提供了一些有用的方法去在线程池中执行Callable内的任务。由于Callable任务是并行的，必须等待它返回的结果。java.util.concurrent.Future对象解决了这个问题。 在线程池提交Callable任务后返回了一个Future对象，使用它可以知道Callable任务的状态和得到Callable返回的执行结果。Future提供了get()方法，等待Callable结束并获取它的执行结果。 什么是FutureTask?FutureTask可用于异步获取执行结果或取消执行任务的场景。通过传入Runnable或者Callable的任务给FutureTask，直接调用其run方法或者放入线程池执行，之后可以在外部通过FutureTask的get方法异步获取执行结果。 FutureTask非常适合用于耗时的计算，主线程可以在完成自己的任务后，再去获取结果。 FutureTask还可以确保即使调用了多次run方法，它都只会执行一次Runnable或者Callable任务，或者通过cancel取消FutureTask的执行等。 高并发场景示例：FutureTask在高并发环境下确保任务只执行一次 在很多高并发的环境下，往往我们只需要某些任务只执行一次。这种使用情景FutureTask的特性恰能胜任。举一个例子，假设有一个带key的连接池，当key存在时，即直接返回key对应的对象；当key不存在时，则创建连接。对于这样的应用场景，通常采用的方法为使用一个Map对象来存储key和连接池对应的对应关系，典型的代码如下面所示： 12345678910111213141516171819202122private Map&lt;String, Connection&gt; connectionPool = new HashMap&lt;String, Connection&gt;();private ReentrantLock lock = new ReentrantLock();public Connection getConnection(String key) &#123; try &#123; lock.lock(); if (connectionPool.containsKey(key)) &#123; return connectionPool.get(key); &#125; else &#123; //创建 Connection Connection conn = createConnection(); connectionPool.put(key, conn); return conn; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; //创建Connection private Connection createConnection() &#123; return null;&#125; 在上面的例子中，我们通过加锁确保高并发环境下的线程安全，也确保了connection只创建一次，然而确牺牲了性能。改用ConcurrentHash的情况下，几乎可以避免加锁的操作，性能大大提高，但是在高并发的情况下有可能出现Connection被创建多次的现象。这时最需要解决的问题就是当key不存在时，创建Connection的动作能放在connectionPool之后执行，这正是FutureTask发挥作用的时机，基于ConcurrentHashMap和FutureTask的改造代码如下： 123456789101112131415161718192021222324252627282930private ConcurrentHashMap&lt;String, FutureTask&lt;Connection&gt;&gt; connectionPool = new ConcurrentHashMap&lt;String, FutureTask&lt;Connection&gt;&gt;();public Connection getConnection(String key) throws Exception &#123; FutureTask&lt;Connection&gt; connectionTask = connectionPool.get(key); if (connectionTask != null) &#123; return connectionTask.get(); &#125; else &#123; Callable&lt;Connection&gt; callable = new Callable&lt;Connection&gt;() &#123; @Override public Connection call() throws Exception &#123; // TODO Auto-generated method stub return createConnection(); &#125; &#125;; FutureTask&lt;Connection&gt; newTask = new FutureTask&lt;Connection&gt;(callable); connectionTask = connectionPool.putIfAbsent(key, newTask); if (connectionTask == null) &#123; connectionTask = newTask; connectionTask.run(); &#125; return connectionTask.get(); &#125;&#125;//创建Connectionprivate Connection createConnection() &#123; return null;&#125; 经过这样的改造，可以避免由于并发带来的多次创建连接及锁的出现。 什么是同步容器和并发容器的实现？一、同步容器主要代表有Vector和HashTable，以及Collections.synchronizedXxx等。 锁的粒度为当前对象整体。 迭代器是及时失败的，即在迭代的过程中发现被修改，就会抛出ConcurrentModificationException。 二、并发容器主要代表有ConcurrentHashMap、CopyOnWriteArrayList、ConcurrentSkipListMap、ConcurrentSkipListSet。 锁的粒度是分散的、细粒度的，即读和写是使用不同的锁。 迭代器具有弱一致性，即可以容忍并发修改，不会抛出ConcurrentModificationException。 ConcurrentHashMap采用分离锁技术，同步容器中，是一个容器一个锁，但在ConcurrentHashMap中，会将hash表的数组部分分成若干段，每段维护一个锁，以达到高效的并发访问； 三、阻塞队列主要代表有LinkedBlockingQueue、ArrayBlockingQueue、PriorityBlockingQueue(Comparable, Comparator)、SynchronousQueue。 提供了可阻塞的put和take方法，以及支持定时的offer和poll方法。 适用于生产者、消费者模式（线程池和工作队列-Executor），同时也是同步容器 四、双端队列主要代表有ArrayDeque和LinkedBlockingDeque。意义：正如阻塞队列适用于生产者消费者模式，双端队列同样适用与另一种模式，即工作密取。在生产者-消费者设计中，所有消费者共享一个工作队列，而在工作密取中，每个消费者都有各自的双端队列。 如果一个消费者完成了自己双端队列中的全部工作，那么他就可以从其他消费者的双端队列末尾秘密的获取工作。具有更好的可伸缩性，这是因为工作者线程不会在单个共享的任务队列上发生竞争。 在大多数时候，他们都只是访问自己的双端队列，从而极大的减少了竞争。当工作者线程需要访问另一个队列时，它会从队列的尾部而不是头部获取工作，因此进一步降低了队列上的竞争。 适用于：网页爬虫等任务中 五、比较及适用场景 如果不需要阻塞队列，优先选择ConcurrentLinkedQueue； 如果需要阻塞队列，队列大小固定优先选择ArrayBlockingQueue，队列大小不固定优先选择LinkedBlockingQueue； 如果需要对队列进行排序，选择PriorityBlockingQueue； 如果需要一个快速交换的队列，选择SynchronousQueue； 如果需要对队列中的元素进行延时操作，则选择DelayQueue。 什么是多线程？优缺点？多线程：是指从软件或者硬件上实现多个线程的并发技术。 多线程的好处：使用多线程可以把程序中占据时间长的任务放到后台去处理，如图片、视屏的下载发挥多核处理器的优势，并发执行让系统运行的更快、更流畅，用户体验更好 多线程的缺点： 大量的线程降低代码的可读性； 更多的线程需要更多的内存空间 当多个线程对同一个资源出现争夺时候要注意线程安全的问题。 什么是多线程的上下文切换？即使是单核CPU也支持多线程执行代码，CPU通过给每个线程分配CPU时间片来实现这个机制。时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程时同时执行的，时间片一般是几十毫秒（ms） 上下文切换过程中，CPU会停止处理当前运行的程序，并保存当前程序运行的具体位置以便之后继续运行。 CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再次加载这个任务的状态。 从任务保存到再加载的过程就是一次上下文切换。 ThreadLocal的设计理念与作用？Java中的ThreadLocal类允许我们创建只能被同一个线程读写的变量。因此，如果一段代码含有一个ThreadLocal变量的引用，即使两个线程同时执行这段代码，它们也无法访问到对方的ThreadLocal变量 InheritableThreadLocal 1public static ThreadLocal&lt;Integer&gt; threadLocal = new InheritableThreadLocal&lt;Integer&gt;(); InheritableThreadLocal类是ThreadLocal类的子类。ThreadLocal中每个线程拥有它自己的值，与ThreadLocal不同的是，InheritableThreadLocal允许一个线程以及该线程创建的所有子线程都可以访问它保存的值。 ThreadPool（线程池）用法与优势？为什么要用线程池: 减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。 可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为消耗过多的内存，而把服务器累趴下(每个线程需要大约1MB内存，线程开的越多，消耗的内存也就越大，最后死机)。 减少在创建和销毁线程上所花的时间以及系统资源的开销,如不使用线程池，有可能造成系统创建大量线程而导致消耗完系统内存 Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。 new Thread 缺点 每次new Thread新建对象性能差。 线程缺乏统一管理，可能无限制新建线程，相互之间竞争，及可能占用过多系统资源导致死机或oom。 缺乏更多功能，如定时执行、定期执行、线程中断。 Executors提供四种线程池 newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 ThreadPoolExecutor的构造函数123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 参数： corePoolSize核心线程数大小，当线程数&lt;corepoolsize ，会创建线程执行 maximumPoolSize 最大线程数， 当线程数 &gt;= corePoolSize的时候，会把runnable放入workQueue中 keepAliveTime 保持存活时间，当线程数大于corePoolSize的空闲线程能保持的最大时间。 unit 时间单位 workQueue 保存任务的阻塞队列 threadFactory 创建线程的工厂 handler 拒绝策略 任务执行顺序： 当线程数小于corePoolSize时，创建线程执行任务。 当线程数大于等于corePoolSize并且workQueue没有满时，放入workQueue中 线程数大于等于corePoolSize并且当workQueue满时，新任务新建线程运行，线程总数要小于maximumPoolSize 当线程总数等于maximumPoolSize并且workQueue满了的时候执行handler的rejectedExecution。也就是拒绝策略。 ThreadPoolExecutor默认有四个拒绝策略： ThreadPoolExecutor.AbortPolicy() 直接抛出异常RejectedExecutionException ThreadPoolExecutor.CallerRunsPolicy() 直接调用run方法并且阻塞执行 ThreadPoolExecutor.DiscardPolicy() 直接丢弃后来的任务 ThreadPoolExecutor.DiscardOldestPolicy() 丢弃在队列中队首的任务 当然可以自己继承 RejectedExecutionHandler 来写拒绝策略. Synchronized和ReentrantLock的区别？java在编写多线程程序时，为了保证线程安全，需要对数据同步，经常用到两种同步方式就是Synchronized和重入锁ReentrantLock。 基础知识 可重入锁可重入锁是指同一个线程可以多次获取同一把锁。ReentrantLock和synchronized都是可重入锁。 可中断锁可中断锁是指线程尝试获取锁的过程中，是否可以响应中断。synchronized是不可中断锁，而ReentrantLock则提供了中断功能。 公平锁与非公平锁公平锁是指多个线程同时尝试获取同一把锁时，获取锁的顺序按照线程达到的顺序，而非公平锁则允许线程“插队”。synchronized是非公平锁，而ReentrantLock的默认实现是非公平锁，但是也可以设置为公平锁。 CAS操作(CompareAndSwap)CAS操作简单的说就是比较并交换。CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存位置的值与预期原值相匹配，那么处理器会自动将该位置值更新为新值。否则，处理器不做任何操作。无论哪种情况，它都会在 CAS 指令之前返回该位置的值。CAS 有效地说明了“我认为位置 V 应该包含值 A；如果包含该值，则将 B 放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可。” Synchronizedsynchronized是java内置的关键字，它提供了一种独占的加锁方式。synchronized的获取和释放锁由JVM实现，用户不需要显示的释放锁，非常方便。然而synchronized也有一定的局限性 例如： 当线程尝试获取锁的时候，如果获取不到锁会一直阻塞； 如果获取锁的线程进入休眠或者阻塞，除非当前线程异常，否则其他线程尝试获取锁必须一直等待。 ReentrantLockReentrantLock它是JDK 1.5之后提供的API层面的互斥锁，需要lock()和unlock()方法配合try/finally语句块来完成。 用法示例： 12345678910private Lock lock = new ReentrantLock();public void test() &#123; lock.lock(); try &#123; doSomeThing(); &#125; finally &#123; lock.unlock(); &#125;&#125; lock() 如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于休眠状态，直到获取锁 tryLock() 如果获取了锁立即返回true，如果别的线程正持有锁，立即返回false； tryLock(long timeout,TimeUnit unit) 如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false； lockInterruptibly 如果获取了锁定立即返回，如果没有获取锁定，当前线程处于休眠状态，直到或者锁定，或者当前线程被别的线程中断 ReentrantLock特性 等待可中断避免，出现死锁的情况（如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false） 公平锁与非公平锁多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好 ReenTrantLock实现的原理：简单来说，ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。 总结在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁) 后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。 Synchronized： 在资源竞争不是很激烈的情况下，偶尔会有同步的情形下，synchronized是很合适的。原因在于，编译程序通常会尽可能的进行优化synchronize，另外可读性非常好。 ReentrantLock: ReentrantLock用起来会复杂一些。在基本的加锁和解锁上，两者是一样的，所以无特殊情况下，推荐使用synchronized。ReentrantLock的优势在于它更灵活、更强大，增加了轮训、超时、中断等高级功能。 ReentrantLock默认使用非公平锁是基于性能考虑，公平锁为了保证线程规规矩矩地排队，需要增加阻塞和唤醒的时间开销。如果直接插队获取非公平锁，跳过了对队列的处理，速度会更快。 Semaphore有什么作用？Semaphore就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个int型整数n，表示某段代码最多只有n个线程可以访问，如果超出了n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。 由此可以看出如果Semaphore构造函数中传入的int型整数n=1，相当于变成了一个synchronized了。 1234567891011121314151617// 阻塞// 用来获取一个许可，若无许可能够获得，则会一直等待，直到获得许可public void acquire() throws InterruptedException;// 用来释放许可。注意，在释放许可之前，必须先获获得许可public void release(); // 非阻塞//尝试获取一个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire() &#123;&#125;; //尝试获取一个许可，若在指定的时间内获取成功，则立即返回true，否则则立即返回false public boolean tryAcquire(long timeout , TimeUnit unit ) throws InterruptedException &#123;&#125;; //尝试获取permits个许可，若获取成功，则立即返回true，若获取失败，则立即返回false public boolean tryAcquire(int permits ) &#123;&#125;; //尝试获取permits个许可，若在指定的时间内获取成功，则立即返回true public boolean tryAcquire(int permits , long timeout , TimeUnit unit ) throws InterruptedException &#123;&#125;; //得到当前可用的许可数目 public int availablePermits(); 示例:假若一个工厂有5台机器，但是有8个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过Semaphore来实现： 12345678910111213141516171819202122232425262728293031323334public class Test&#123; public static void main(String[] args) &#123; int N = 8 ; //工人数 Semaphore semaphore = new Semaphore(5); //机器数目 for (int i = 0; i &lt; N; i++) &#123; new Worker(i, semaphore ).start (); &#125; static class Worker extends Thread &#123; private int num; private Semaphore semaphore; public Worker(int num, Semaphore semaphore)&#123; this.num = num; this.semaphore = semaphore; &#125; @Override public void run() &#123; try &#123; semaphore.acquire(); System.out.println ("工人"+ this.num + "占用一个机器在生产..."); Thread.sleep (2000 ); System.out.println ("工人"+ this.num + "释放出机器"); semaphore.release (); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; Java Concurrency API中的Lock接口(Lock interface)是什么？对比同步它有什么优势？Lock接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。 它的优势有： 可以使锁更公平 可以使线程在等待锁的时候响应中断 可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间 可以在不同的范围，以不同的顺序获取和释放锁 ReentrantReadWriteLock读写锁的使用？Lock比传统线程模型中的synchronized方式更加面向对象，与生活中的锁类似，锁本身也应该是一个对象。两个线程执行的代码片段要实现同步互斥的效果，它们必须用同一个Lock对象。 读写锁：分为读锁和写锁，多个读锁不互斥，读锁与写锁互斥，这是由jvm自己控制的，你只要上好相应的锁即可。 如果你的代码只读数据，可以很多人同时读，但不能同时写，那就上读锁；如果你的代码修改数据，只能有一个人在写，且不能同时读取，那就上写锁。总之，读的时候上读锁，写的时候上写锁！ ReentrantReadWriteLock会使用两把锁来解决问题，一个读锁，一个写锁 线程进入读锁的前提条件： 没有其他线程的写锁 没有写请求或者有写请求，但调用线程和持有锁的线程是同一个 线程进入写锁的前提条件： 没有其他线程的读锁 没有其他线程的写锁 注意点： 读锁的重入是允许多个申请读操作的线程的，而写锁同时只允许单个线程占有，该线程的写操作可以重入。 如果一个线程占有了写锁，在不释放写锁的情况下，它还能占有读锁，即写锁降级为读锁。 对于同时占有读锁和写锁的线程，如果完全释放了写锁，那么它就完全转换成了读锁，以后的写操作无法重入，在写锁未完全释放时写操作是可以重入的。 公平模式下无论读锁还是写锁的申请都必须按照AQS锁等待队列先进先出的顺序。非公平模式下读操作插队的条件是锁等待队列head节点后的下一个节点是SHARED型节点，写锁则无条件插队。 读锁不允许newConditon获取Condition接口，而写锁的newCondition接口实现方法同ReentrantLock。 CyclicBarrier和CountDownLatch的用法及区别？ CountDownLatch CyclicBarrier 减计数方式 加计数方式 计算为0时释放所有等待的线程 计数达到指定值时释放所有等待线程 计数为0时，无法重置 计数达到指定值时，计数置为0重新开始 调用countDown()方法计数减一，调用await()方法只进行阻塞，对计数没任何影响 调用await()方法计数加1，若加1后的值不等于构造方法的值，则线程阻塞 不可重复利用 可重复利用 LockSupport工具？1、LockSupport基本介绍与基本使用LockSupport是JDK中比较底层的类，用来创建锁和其他同步工具类的基本线程阻塞。java锁和同步器框架的核心 AQS: AbstractQueuedSynchronizer，就是通过调用 LockSupport .park()和 LockSupport .unpark()实现线程的阻塞和唤醒 的。 LockSupport 很类似于二元信号量(只有1个许可证可供使用)，如果这个许可还没有被占用，当前线程获取许可并继 续 执行；如果许可已经被占用，当前线 程阻塞，等待获取许可。 全部操作： park()/park(Object)等待通行准许。 parkNanos(long)/parkNanos(Object, long)在指定运行时间（即相对时间）内，等待通行准许。 parkUntil(long)/parkUntil(Object, long)在指定到期时间（即绝对时间）内，等待通行准许。 unpark(Thread)发放通行准许或提前发放。（注：不管提前发放多少次，只用于一次性使用。） getBlocker(Thread)进入等待通行准许时，所提供的对象。 主要用途：当前线程需要唤醒另一个线程，但是只确定它会进入阻塞，但不确定它是否已经进入阻塞，因此不管是否已经进入阻塞，还是准备进入阻塞，都将发放一个通行准许。 Condition接口及其实现原理？ 在java.util.concurrent包中，有两个很特殊的工具类，Condition和ReentrantLock，使用过的人都知道，ReentrantLock（重入锁）是jdk的concurrent包提供的一种独占锁的实现 我们知道在线程的同步时可以使一个线程阻塞而等待一个信号，同时放弃锁使其他线程可以能竞争到锁在synchronized中我们可以使用Object的wait()和notify方法实现这种等待和唤醒 但是在Lock中怎么实现这种wait和notify呢？答案是Condition，学习Condition主要是为了方便以后学习blockqueue和concurrenthashmap的源码，同时也进一步理解ReentrantLock。Condition是一个多线程间协调通信的工具类，使得某个，或者某些线程一起等待某个条件（Condition）,只有当该条件具备( signal 或者 signalAll方法被带调用)时 ，这些等待线程才会被唤醒，从而重新争夺锁。 Fork/Join框架的理解?Oracle的官方给出的定义是：Fork/Join框架是一个实现了ExecutorService接口的多线程处理器。它可以把一个大的任务划分为若干个小的任务并发执行，充分利用可用的资源，进而提高应用的执行效率。 我们再通过Fork和Join这两个单词来理解下Fork/Join框架，Fork就是把一个大任务切分为若干子任务并行的执行，Join就是合并这些子任务的执行结果，最后得到这个大任务的结果。 工作窃取算法工作窃取算法是指线程从其他任务队列中窃取任务执行（可能你会很诧异，这个算法有什么用。待会你就知道了）。 考虑下面这种场景：有一个很大的计算任务，为了减少线程的竞争，会将这些大任务切分为小任务并分在不同的队列等待执行，然后为每个任务队列创建一个线程执行队列的任务。那么问题来了，有的线程可能很快就执行完了，而其他线程还有任务没执行完，执行完的线程与其空闲下来不如帮助其他线程执行任务，这样也能加快执行进程。所以，执行完的空闲线程从其他队列的尾部窃取任务执行，而被窃取任务的线程则从队列的头部取任务执行（这里使用了双端队列，既不影响被窃取任务的执行过程又能加快执行进度）。 从以上的介绍中，能够发现工作窃取算法的优点是充分利用线程提高并行执行的进度。当然缺点是在某些情况下仍然存在竞争，比如双端队列只有一个任务需要执行的时候 使用Fork/Join框架两步： 分割任务：首先需要创建一个ForkJoin任务，执行该类的fork方法可以对任务不断切割，直到分割的子任务足够小 合并任务执行结果：子任务执行的结果同一放在一个队列中，通过启动一个线程从队列中取执行结果。 Fork/Join实现了ExecutorService，所以它的任务也需要放在线程池中执行。它的不同在于它使用了工作窃取算法，空闲的线程可以从满负荷的线程中窃取任务来帮忙执行。 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.ExecutionException;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;import java.util.concurrent.RecursiveTask;public class CountTask extends RecursiveTask&lt;Integer&gt;&#123; //阈值 private static final int THRESHOLD = 2; //起始值 private int start; //结束值 private int end; public CountTask(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; boolean compute = (end - start) &lt;= THRESHOLD; int res = 0; if (compute)&#123; for (int i = start; i &lt;= end; i++)&#123; res += i; &#125; &#125;else &#123; //如果长度大于阈值，则分割为小任务 int mid = (start + end) / 2; CountTask task1 = new CountTask(start,mid); CountTask task2 = new CountTask(mid + 1, end); //计算小任务的值 task1.fork(); task2.fork(); //得到两个小任务的值 int task1Res = task1.join(); int task2Res = task2.join(); res = task1Res + task2Res; &#125; return res; &#125; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ForkJoinPool pool = new ForkJoinPool(); CountTask task = new CountTask(1,5); ForkJoinTask&lt;Integer&gt; submit = pool.submit(task); System.out.println("Final result:" + submit.get()); &#125;&#125; 代码中使用了FokJoinTask，其与一般任务的区别在于它需要实现compute方法，在方法需要判断任务是否在阈值区间内，如果不是则需要把任务切分到足够小，直到能够进行计算。 每个被切分的子任务又会重新进入compute方法，再继续判断是否需要继续切分，如果不需要则直接得到子任务执行的结果，如果需要的话则继续切分，如此循环，直到调用join方法得到最终的结果。 wait()和sleep()的区别? sleep() 方法是线程类（Thread）的静态方法，让调用线程进入睡眠状态，让出执行机会给其他线程，等到休眠时间结束后，线程进入就绪状态和其他线程一起竞争cpu的执行时间。 因为sleep() 是static静态的方法，他不能改变对象的机锁，当一个synchronized块中调用了sleep() 方法，线程虽然进入休眠，但是对象的机锁没有被释放，其他线程依然无法访问这个对象。 wait()wait()是Object类的方法，当一个线程执行到wait方法时，它就进入到一个和该对象相关的等待池，同时释放对象的机锁，使得其他线程能够访问，可以通过notify，notifyAll方法来唤醒等待的线程 线程的五个状态（五种状态，创建、就绪、运行、阻塞和死亡）? start()方法和run()方法的区别？每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，方法run()称为线程体。通过调用Thread类的start()方法来启动一个线程。 start()启动一个线程，真正实现了多线程运行。这时无需等待run方法体代码执行完毕，可以直接继续执行下面的代码；这时此线程是处于就绪状态， 并没有运行。 然后通过此Thread类调用方法run()来完成其运行状态， 这里方法run()称为线程体，它包含了要执行的这个线程的内容， Run方法运行结束， 此线程终止。然后CPU再调度其它线程。 run()方法是在本线程里的，只是线程里的一个函数,而不是多线程的。如果直接调用run(),其实就相当于是调用了一个普通函数而已，直接待用run()方法必须等待run()方法执行完毕才能执行下面的代码，所以执行路径还是只有一条，根本就没有线程的特征，所以在多线程执行时要使用start()方法而不是run()方法。 Runnable接口和Callable接口的区别？ Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已； Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，非常有用。 Runnable和Thread实现多线程的区别Java中实现多线程有两种方法：继承Thread类、实现Runnable接口，在程序开发中只要是多线程，肯定永远以实现Runnable接口为主，因为实现Runnable接口相比继承Thread类有如下优势： 1、可以避免由于Java的单继承特性而带来的局限； 2、增强程序的健壮性，代码能够被多个线程共享，代码与数据是独立的； 3、适合多个相同程序代码的线程区处理同一资源的情况。 下面以典型的买票程序（基本都是以这个为例子）为例，来说明二者的区别。 首先通过继承Thread类实现，代码如下： 12345678910111213141516171819class MyThread extends Thread&#123; private int ticket = 5; public void run()&#123; for (int i=0;i&lt;10;i++) &#123; if(ticket &gt; 0)&#123; System.out.println("ticket = " + ticket--); &#125; &#125; &#125;&#125; public class ThreadDemo&#123; public static void main(String[] args)&#123; new MyThread().start(); new MyThread().start(); new MyThread().start(); &#125;&#125; 某次的执行结果如下： 从结果中可以看出，每个线程单独卖了5张票，即独立地完成了买票的任务，但实际应用中，比如火车站售票，需要多个线程去共同完成任务，在本例中，即多个线程共同买5张票。 下面是通过实现Runnable接口实现的多线程程序，代码如下： 123456789101112131415161718192021class MyThread implements Runnable&#123; private int ticket = 5; public void run()&#123; for (int i=0;i&lt;10;i++) &#123; if(ticket &gt; 0)&#123; System.out.println("ticket = " + ticket--); &#125; &#125; &#125;&#125; public class RunnableDemo&#123; public static void main(String[] args)&#123; MyThread my = new MyThread(); new Thread(my).start(); new Thread(my).start(); new Thread(my).start(); &#125;&#125; 某次的执行结果如下: 从结果中可以看出，三个线程一共卖了5张票，即它们共同完成了买票的任务，实现了资源的共享。 总结如果一个类继承Thread，则不适合资源共享。但是如果实现了Runable接口的话，则很容易的实现资源共享。实现Runnable接口比继承Thread类所具有的优势： 1）：适合多个相同的程序代码的线程去处理同一个资源 2）：可以避免java中的单继承的限制 3）：增加程序的健壮性，代码可以被多个线程共享，代码和数据独立 4）：线程池只能放入实现Runable或callable类线程，不能直接放入继承Thread的类 提醒一下大家：main方法其实也是一个线程。在java中所有的线程都是同时启动的，至于什么时候，哪个先执行，完全看谁先得到CPU的资源。 在java中，每次程序运行至少启动2个线程。一个是main线程，一个是垃圾收集线程。因为每当使用java命令执行一个类的时候，实际上都会启动一个JVM，每一个JVM实例就是在操作系统中启动了一个进程。 volatile关键字的作用？volatile关键字的作用主要有两个： （1）多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据; （2）代码底层执行是Java代码–&gt;字节码–&gt;根据字节码执行对应的C/C++代码–&gt;C/C++代码被编译成汇编语言–&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率; 从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性. Java中如何获取到线程dump文件？死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： （1）获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java （2）打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈， 线程和进程有什么区别？ 进程是系统进行资源分配的基本单位，有独立的内存地址空间 线程是CPU独立运行和独立调度的基本单位，没有单独地址空间，有独立的栈，局部变量，寄存器， 程序计数器等。 创建进程的开销大，包括创建虚拟地址空间等需要大量系统资源 创建线程开销小，基本上只有一个内核对象和一个堆栈。 一个进程无法直接访问另一个进程的资源；同一进程内的多个线程共享进程的资源。 进程切换开销大，线程切换开销小；进程间通信开销大，线程间通信开销小。 线程属于进程，不能独立执行。每个进程至少要有一个线程，成为主线程 线程实现的方式有几种（四种）？ 继承Thread类，重写run方法 实现Runnable接口，重写run方法，实现Runnable接口的实现类的实例对象作为Thread构造函数的target 实现Callable接口通过FutureTask包装器来创建Thread线程 1FutureTask&lt;Object&gt; oneTask = new FutureTask&lt;Object&gt;(oneCallable); 通过线程池创建线程 12ExecutorService executorService = Executors.newFixedThreadPool(5);executorService.execute(new RunnableTask()); 高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ （1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 （2）并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 （3）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考（2）。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。 锁的等级：方法锁、对象锁、类锁?1. 通过在方法声明中加入 synchronized关键字来声明 synchronized 方法 synchronized 方法控制对类成员变量的访问：每个类实例对应一把锁，每个 synchronized 方法都必须获得调用该方法的类实例的锁方能执行，否则所属线程阻塞，方法一旦执行，就独占该锁，直到从该方法返回时才将锁释放，此后被阻塞的线程方能获得该锁，重新进入可执行状态。 这种机制确保了同一时刻对于每一个类实例，其所有声明为 synchronized 的成员函数中至多只有一个处于可执行状态，从而有效避免了类成员变量的访问冲突。 2. 对象锁（synchronized修饰方法或代码块） 当一个对象中有synchronized method或synchronized block的时候调用此对象的同步方法或进入其同步区域时，就必须先获得对象锁。如果此对象的对象锁已被其他调用者占用，则需要等待此锁被释放。（方法锁也是对象锁） java的所有对象都含有1个互斥锁，这个锁由JVM自动获取和释放。线程进入synchronized方法的时候获取该对象的锁，当然如果已经有线程获取了这个对象的锁，那么当前线程会等待；synchronized方法正常返回或者抛异常而终止，JVM会自动释放对象锁。这里也体现了用synchronized来加锁的1个好处，方法抛异常的时候，锁仍然可以由JVM来自动释放。 3. 类锁(synchronized 修饰静态的方法或代码块) 由于一个class不论被实例化多少次，其中的静态方法和静态变量在内存中都只有一份。所以，一旦一个静态的方法被申明为synchronized。此类所有的实例化对象在调用此方法，共用同一把锁，我们称之为类锁。 对象锁是用来控制实例方法之间的同步，类锁是用来控制静态方法（或静态变量互斥体）之间的同步 如果同步块内的线程抛出异常会发生什么？无论你的同步块是正常还是异常退出的，里面的线程都由JVM来自动释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 并发编程（concurrency）并行编程（parallellism）有什么区别？并发和并行是： 解释一：并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔发生。 解释二：并行是在不同实体上的多个事件，并发是在同一实体上的多个事件。 解释三：在一台处理器上“同时”处理多个任务，在多台处理器上同时处理多个任务。如hadoop分布式集群 所以并发编程的目标是充分的利用处理器的每一个核，以达到最高的处理性能。 如何在两个线程之间共享数据? 通过在线程之间共享对象, 然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的； Exchanger 用于进行线程间数据交换； 生产者消费者模型的作用是什么?这个问题很理论，但是很重要： （1）通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用 （2）解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 怎么唤醒一个阻塞的线程? 如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它； 如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 Java中用到的线程调度算法是什么抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。 单例模式的线程安全性?首先要说的是单例模式的线程安全意味着：某个类的实例在多线程环境下只会被创建一次出来。单例模式有很多种的写法，我总结一下： （1）饿汉式单例模式的写法：线程安全 （2）懒汉式单例模式的写法：非线程安全 （3）双检锁单例模式的写法：线程安全 同步方法和同步块，哪个是更好的选择?同步块是更好的选择，因为它不会锁住整个对象（当然也可以让它锁住整个对象）。 同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。 123456789101112131415161718public class SyncObj&#123; // 同步方法会锁住整个对象 public synchronized void showA()&#123; System.out.println("showA.."); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public void showB() &#123; // 同步块 synchronized (this) &#123; System.out.println("showB.."); &#125; &#125;&#125; 如何检测死锁？怎么预防死锁？死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁。通俗地讲就是两个或多个进程被无限期地阻塞、相互等待的一种状态 死锁产生的原因？ 1.因竞争资源发生死锁 现象：系统中供多个进程共享的资源的数目不足以满足全部进程的需要时，就会引起对诸资源的竞争而发生死锁现象 2.进程推进顺序不当发生死锁 死锁的四个必要条件： 互斥条件：进程对所分配到的资源不允许其他进程进行访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源 请求和保持条件：进程获得一定的资源之后，又对其他资源发出请求，但是该资源可能被其他进程占有，此事请求阻塞，但又对自己获得的资源保持不放 不可剥夺条件：是指进程已获得的资源，在未完成使用之前，不可被剥夺，只能在使用完后自己释放 环路等待条件：是指进程发生死锁后，若干进程之间形成一种头尾相接的循环等待资源关系这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。 检测死锁有两个容器，一个用于保存线程正在请求的锁，一个用于保存线程已经持有的锁。每次加锁之前都会做如下检测: 检测当前正在请求的锁是否已经被其它线程持有,如果有，则把那些线程找出来 遍历第一步中返回的线程，检查自己持有的锁是否正被其中任何一个线程请求，如果第二步返回真,表示出现了死锁 死锁的解除与预防：理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁。 所以，在系统设计、进程调度等方面注意如何不让这四个必要条件成立，如何确定资源的合理分配算法，避免进程永久占据系统资源。 此外，也要防止进程在处于等待状态的情况下占用资源。因此，对资源的分配要给予合理的规划。 转载：想进大厂？50个多线程面试题，你会多少？（一）想进大厂？50个多线程面试题，你会多少？（二）]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池ThreadPoolExecutor源码详解]]></title>
    <url>%2F2019%2F07%2F19%2FJava%E7%BA%BF%E7%A8%8B%E6%B1%A0ThreadPoolExecutor%E6%BA%90%E7%A0%81%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文从源码层面去解读下java线程池的实现思想和代码。 总览先看一张java线程池的继承关系图： 简单介绍下： Executor 位于最顶层，也是最简单的，只有一个 execute(Runnable runnable) 接口方法定义 ExecutorService 也是接口，在 Executor 接口的基础上添加了很多的接口方法，很多时候我们使用这个接口就够了 AbstractExecutorService，这是抽象类，这里实现了非常有用的一些方法供子类直接使用，例如: invokeAll()、 invokeAny() ThreadPoolExecutor 类，这个类才是真正的线程池实现，提供了非常丰富的功能。 从图中的方法可以看到，还涉及到一些其他类： 其中： Executors类这个是工具类，里面的方法都是静态方法，如以下我们最常用的用于生成 ThreadPoolExecutor 的实例的一些方法： 123456789101112public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; Future接口由于线程池支持获取线程执行的结果，所以，引入了 Future 接口，RunnableFuture 继承自此接口，然后我们最需要关心的就是它的实现类 FutureTask。 FutureTask类在线程池的使用过程中，我们是往线程池提交任务（task），我们提交的每个任务是实现了 Runnable 接口的，其实就是先将 Runnable 的任务包装成 FutureTask，然后再提交到线程池。它首先是一个任务（Task），然后具有 Future 接口的语义，即可以在将来（Future）得到执行的结果。 BlockingQueue如果线程数达到 corePoolSize，我们的每个任务会提交到等待队列中，等待线程池中的线程来取任务并执行。这里的 BlockingQueue 通常我们使用其实现类 LinkedBlockingQueue、ArrayBlockingQueue 和 SynchronousQueue，每个实现类都有不同的特征. Executor接口1234567/* * @since 1.5 * @author Doug Lea */public interface Executor &#123; void execute(Runnable command);&#125; 可以看到 Executor 接口非常简单，就一个 void execute(Runnable command) 方法，代表提交一个任务。为了理解 java 线程池的整个设计方案，我会按照 Doug Lea 的设计思路来多说一些相关的东西。 我们经常这样启动一个线程： 123new Thread(new Runnable()&#123; // do something&#125;).start(); 用了线程池 Executor 后就可以像下面这么使用： 123Executor executor = anExecutor;executor.execute(new RunnableTask1());executor.execute(new RunnableTask2()); 如果我们希望线程池同步执行每一个任务，我们可以这么实现这个接口： 12345class DirectExecutor implements Executor &#123; public void execute(Runnable r) &#123; r.run(); // 这里不是用的new Thread(r).start()，也就是说没有启动任何一个新的线程。 &#125;&#125; 如果我们希望每个任务提交进来后，直接启动一个新的线程来执行这个任务，我们可以这么实现： 12345class ThreadPerTaskExecutor implements Executor &#123; public void execute(Runnable r) &#123; new Thread(r).start(); // 每个任务都用一个新的线程来执行 &#125;&#125; 我们再来看下怎么组合两个 Executor 来使用，下面这个实现是将所有的任务都加到一个 queue 中，然后从 queue 中取任务，交给真正的执行器执行，这里采用 synchronized 进行并发控制： 12345678910111213141516171819202122232425262728293031323334353637class SerialExecutor implements Executor &#123; // 任务队列 final Queue&lt;Runnable&gt; tasks = new ArrayDeque&lt;Runnable&gt;(); // 这个才是真正的执行器 final Executor executor; // 当前正在执行的任务 Runnable active; // 初始化的时候，指定执行器 SerialExecutor(Executor executor) &#123; this.executor = executor; &#125; // 添加任务到线程池: 将任务添加到任务队列，scheduleNext 触发执行器去任务队列取任务 public synchronized void execute(final Runnable r) &#123; tasks.offer(new Runnable() &#123; public void run() &#123; try &#123; r.run(); &#125; finally &#123; scheduleNext(); &#125; &#125; &#125;); if (active == null) &#123; scheduleNext(); &#125; &#125; protected synchronized void scheduleNext() &#123; if ((active = tasks.poll()) != null) &#123; // 具体的执行转给真正的执行器 executor executor.execute(active); &#125; &#125;&#125; Executor 这个接口只有提交任务的功能，太简单了，我们想要更丰富的功能，比如我们想知道执行结果、我们想知道当前线程池有多少个线程活着、已经完成了多少任务等等，这些都是这个接口的不足的地方。接下来我们要介绍的是继承自 Executor 接口的 ExecutorService 接口，这个接口提供了比较丰富的功能，也是我们最常使用到的接口。 ExecutorService简单初略地来看一下这个接口中都有哪些方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152简单初略地来看一下这个接口中都有哪些方法：public interface ExecutorService extends Executor &#123; // 关闭线程池，已提交的任务继续执行，不接受继续提交新任务 void shutdown(); // 关闭线程池，尝试停止正在执行的所有任务，不接受继续提交新任务 // 它和前面的方法相比，加了一个单词“now”，区别在于它会去停止当前正在进行的任务 List&lt;Runnable&gt; shutdownNow(); // 线程池是否已关闭 boolean isShutdown(); // 如果调用了 shutdown() 或 shutdownNow() 方法后，所有任务结束了，那么返回true // 这个方法必须在调用shutdown或shutdownNow方法之后调用才会返回true boolean isTerminated(); // 等待所有任务完成，并设置超时时间 // 我们这么理解，实际应用中是，先调用 shutdown 或 shutdownNow， // 然后再调这个方法等待所有的线程真正地完成，返回值意味着有没有超时 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个 Callable 任务 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); // 提交一个 Runnable 任务，第二个参数将会放到 Future 中，作为返回值， // 因为 Runnable 的 run 方法本身并不返回任何东西 &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); // 提交一个 Runnable 任务 Future&lt;?&gt; submit(Runnable task); // 执行所有任务，等全部完成后返回 Future 类型的一个 list &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; // 也是执行所有任务，但是这里设置了超时时间 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; // 只要其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; // 同上一个方法，只要其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果， // 不过这个带超时，超过指定的时间，抛出 TimeoutException 异常 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 这些方法都很好理解，一个简单的线程池主要就是这些功能，能提交任务，能获取结果，能关闭线程池，这也是为什么我们经常用这个接口的原因。 FutureTask在继续往下层介绍 ExecutorService 的实现类之前，我们先来说说相关的类 FutureTask。 FutureTask 通过 RunnableFuture 间接实现了 Runnable 接口，所以每个 Runnable 通常都先包装成 FutureTask，然后调用 executor.execute(Runnable command) 将其提交给线程池. Runnable 的 void run() 方法是没有返回值的，所以，通常，如果我们需要的话，会在 submit 中指定第二个参数作为返回值： 1&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); 其实到时候会通过这两个参数，将其包装成 Callable。 Callable 也是因为线程池的需要，所以才有了这个接口。它和 Runnable 的区别在于 run() 没有返回值，而 Callable 的 call() 方法有返回值，同时，如果运行出现异常，call() 方法会抛出异常。 123public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 123public interface Runnable &#123; public abstract void run();&#125; 下面，我们来看看 ExecutorService 的抽象实现 AbstractExecutorService 。 AbstractExecutorServiceAbstractExecutorService 抽象类派生自 ExecutorService 接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 invokeAny方法： invokeAll方法： newTaskFor方法： 用于将任务包装成 FutureTask 定义于最上层接口 Executor中的 void execute(Runnable command) 由于不需要获取结果，不会进行 FutureTask 的包装。 需要获取结果（FutureTask），用 submit 方法，不需要获取结果，可以用 execute 方法。 下面重点讲解下newTaskFor和invokeAny、invokeAll方法源码。 newTaskFor &amp;&amp; submit1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public abstract class AbstractExecutorService implements ExecutorService &#123; /** * RunnableFuture 是用于获取执行结果的，我们常用它的子类 FutureTask * 下面两个 newTaskFor 方法用于将我们的任务包装成 FutureTask 提交到线程池中执行 */ protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; return new FutureTask&lt;T&gt;(runnable, value); &#125; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable); &#125; /** * 提交任务 */ public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) &#123; throw new NullPointerException(); &#125; // 1. 将任务包装成 FutureTask RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); // 2. 交给子类执行器执行 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) &#123; throw new NullPointerException(); &#125; // 1. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); // 2. 交给子类执行器执行 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) &#123; throw new NullPointerException(); &#125; // 1. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task); // 2. 交给子类执行器执行 execute(ftask); return ftask; &#125;&#125; invokeAny123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122/** * 此方法目的：将 tasks 集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了 * 第二个参数 timed 代表是否设置超时机制，超时时间为第三个参数， * 如果 timed 为 true，同时超时了还没有一个线程返回结果，那么抛出 TimeoutException 异常 */private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; if (tasks == null) &#123; throw new NullPointerException(); &#125; int ntasks = tasks.size(); if (ntasks == 0) &#123; throw new IllegalArgumentException(); &#125; ArrayList&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(ntasks); // ExecutorCompletionService 不是一个真正的执行器，参数 this 才是真正的执行器 // 它对执行器进行了包装，每个任务结束后，将结果保存到内部的一个 completionQueue 队列中 // 这也是为什么这个类的名字里面有个 Completion 的原因。 ExecutorCompletionService&lt;T&gt; ecs = new ExecutorCompletionService&lt;T&gt;(this); // For efficiency, especially in executors with limited // parallelism, check to see if previously submitted tasks are // done before submitting more of them. This interleaving // plus the exception mechanics account for messiness of main // loop. try &#123; // 用于保存异常信息，此方法如果没有得到任何有效的结果，那么我们可以抛出最后得到的一个异常 ExecutionException ee = null; final long deadline = timed ? System.nanoTime() + nanos : 0L; Iterator&lt;? extends Callable&lt;T&gt;&gt; it = tasks.iterator(); // 首先先提交一个任务，后面的任务到下面的 for 循环一个个提交 futures.add(ecs.submit(it.next())); --ntasks; // 提交了一个任务，所以任务数量减 1 int active = 1; // 正在执行的任务数(提交的时候 +1，任务结束的时候 -1) for (; ; ) &#123; // ecs 上面说了，其内部有一个 completionQueue 用于保存执行完成的结果 // BlockingQueue的poll方法不阻塞，返回 null 代表队列为空 Future&lt;T&gt; f = ecs.poll(); // 非阻塞 // 为 null，说明刚刚提交的第一个线程还没有执行完成 // 在前面先提交一个任务，加上这里做一次检查，也是为了提高性能 if (f == null) &#123; if (ntasks &gt; 0) &#123; // 再提交一个任务 --ntasks; futures.add(ecs.submit(it.next())); ++active; &#125; else if (active == 0) &#123; // 没有任务了，同时active为0,说明 任务都执行完成了 break; &#125; else if (timed) &#123; f = ecs.poll(nanos, TimeUnit.NANOSECONDS); // 带等待时间的poll方法 if (f == null) &#123; throw new TimeoutException(); // 如果已经超时，抛出 TimeoutException 异常，这整个方法就结束了 &#125; nanos = deadline - System.nanoTime(); &#125; else &#123; f = ecs.take(); // 没有任务了，有一个在运行中，再获取一次结果，阻塞方法，直到任务结束 &#125; &#125; /* * 我感觉上面这一段并不是很好理解，这里简单说下： * 1. 首先，这在一个 for 循环中，我们设想每一个任务都没那么快结束， * 那么，每一次都会进到第一个分支，进行提交任务，直到将所有的任务都提交了 * 2. 任务都提交完成后，如果设置了超时，那么 for 循环其实进入了“一直检测是否超时” 这件事情上 * 3. 如果没有设置超时机制，那么不必要检测超时，那就会阻塞在 ecs.take() 方法上， 等待获取第一个执行结果 * ?. 这里我还没理解 active == 0 这个分支的到底是干嘛的？ */ if (f != null) &#123; // 有任务结束了 --active; try &#123; return f.get(); // 阻塞获取执行结果，如果有异常，都包装成 ExecutionException &#125; catch (ExecutionException eex) &#123; ee = eex; &#125; catch (RuntimeException rex) &#123; ee = new ExecutionException(rex); &#125; &#125; &#125; if (ee == null) &#123; ee = new ExecutionException(); &#125; throw ee; &#125; finally &#123; // 方法退出之前，取消其他的任务 for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; futures.get(i).cancel(true); &#125; &#125;&#125;/** * 将tasks集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了，不设置超时时间 */public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; try &#123; return doInvokeAny(tasks, false, 0); &#125; catch (TimeoutException cannotHappen) &#123; assert false; return null; &#125;&#125;/** * 将tasks集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了，需要指定超时时间 */public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; return doInvokeAny(tasks, true, unit.toNanos(timeout));&#125; invokeAll123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103 /** * 将tasks集合中的任务提交到线程池执行，全部线程执行完后才可以结束了 * 其实我们自己提交任务到线程池，也是想要线程池执行所有的任务 * 只不过，我们是每次 submit 一个任务，这里以一个集合作为参数提交 */public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; if (tasks == null) &#123; throw new NullPointerException(); &#125; ArrayList&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; for (Callable&lt;T&gt; t : tasks) &#123; // 包装成 FutureTask RunnableFuture&lt;T&gt; f = newTaskFor(t); futures.add(f); // 提交任务 execute(f); &#125; for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; Future&lt;T&gt; f = futures.get(i); if (!f.isDone()) &#123; try &#123; // 这是一个阻塞方法，直到获取到值，或抛出了异常 // 这里有个小细节，其实 get 方法签名上是会抛出 InterruptedException 的 // 可是这里没有进行处理，而是抛给外层去了。此异常发生于还没执行完的任务被取消了 f.get(); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; &#125; &#125; done = true; // 这个方法返回返回 List&lt;Future&gt;，而且是任务都结束了 return futures; &#125; finally &#123; if (!done) &#123; // 异常情况下才会进入 // 方法退出之前，取消其他的任务 for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; futures.get(i).cancel(true); &#125; &#125; &#125;&#125;/** * 带超时的 invokeAll */public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; if (tasks == null) &#123; throw new NullPointerException(); &#125; long nanos = unit.toNanos(timeout); ArrayList&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; for (Callable&lt;T&gt; t : tasks) &#123; futures.add(newTaskFor(t)); &#125; final long deadline = System.nanoTime() + nanos; // 直接计算出超时时刻 final int size = futures.size(); // 提交一个任务，检测一次是否超时 for (int i = 0; i &lt; size; i++) &#123; execute((Runnable) futures.get(i)); nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; return futures; &#125; &#125; for (int i = 0; i &lt; size; i++) &#123; Future&lt;T&gt; f = futures.get(i); if (!f.isDone()) &#123; if (nanos &lt;= 0L) &#123; return futures; &#125; try &#123; // 调用带超时的 get 方法，这里的参数 nanos 是剩余的时间， // 因为上面其实已经用掉了一些时间了 f.get(nanos, TimeUnit.NANOSECONDS); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; catch (TimeoutException toe) &#123; return futures; &#125; nanos = deadline - System.nanoTime(); // 更新剩余时间 &#125; &#125; done = true; return futures; &#125; finally &#123; if (!done) &#123; for (int i = 0, size = futures.size(); i &lt; size; i++) &#123; futures.get(i).cancel(true); &#125; &#125; &#125;&#125; 到这里，我们发现，这个抽象类包装了一些基本的方法，可是像 submit、invokeAny、invokeAll 等方法，它们都没有真正开启线程来执行任务，它们都只是在方法内部调用了 execute 方法，所以最重要的 execute(Runnable runnable) 方法还没出现，需要等具体执行器来实现这个最重要的部分，这里我们要说的就是 ThreadPoolExecutor 类了。 ThreadPoolExecutorThreadPoolExecutor 是 JDK 中的线程池实现，这个类实现了一个线程池需要的各个方法，它实现了任务提交、线程管理、监控等等方法。 构造函数Executors 这个工具类来快速构造一个线程池，对于初学者而言，这种工具类是很有用的，开发者不需要关注太多的细节，只要知道自己需要一个线程池，仅仅提供必需的参数就可以了，其他参数都采用作者提供的默认值。其调用的就是构造函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 构造方法 * * @param corePoolSize 核心线程数 * @param maximumPoolSize 最大线程数，线程池允许创建的最大线程数 * @param keepAliveTime 空闲线程的保活时间，如果某线程的空闲时间超过这个值都没有任务给它做，那么可以被关闭了。 * 注意这个值并不会对所有线程起作用，如果线程池中的线程数少于等于核心线程数 corePoolSize， * 那么这些线程不会因为空闲太长时间而被关闭，当然，也可以通过调用 allowCoreThreadTimeOut(true) * 使核心线程数内的线程也可以被回收 * @param unit 时间单位 * @param workQueue 任务队列，BlockingQueue 接口的某个实现（常使用 ArrayBlockingQueue 和 LinkedBlockingQueue） * @param threadFactory 用于生成线程，一般我们可以用默认的就可以了。 * 通常，我们可以通过它将我们的线程的名字设置得比较可读一些，如 Message-Thread-1， Message-Thread-2 类似这样。 * @param handler 当线程池已经满了，但是又有新的任务提交的时候，该采取什么策略由这个来指定。有 * 几种方式可供选择，像抛出异常、直接拒绝然后返回等，也可以自己实现相应的接口实现自己的逻辑。 */ public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) &#123; throw new IllegalArgumentException(); &#125; if (workQueue == null || threadFactory == null || handler == null) &#123; throw new NullPointerException(); &#125; this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 关键变量除了构造函数以外，还需要重点关注下几个重要的属性和函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3; // 29// 线程容量(2^29-1=536 870 911)private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; // 000 1111111111111111111111111111/* * RUNNING 定义为 -1， * SHUTDOWN 定义为 0， * 其他的都比 0 大， * 所以等于 0 的时候不能提交任务，大于 0 的话，连正在执行的任务也需要中断 */// runState存储在高3位// 接收新任务，处理队列任务private static final int RUNNING = -1 &lt;&lt; COUNT_BITS; // 111 00000000000000000000000000000 (-536870912)// 不接受新的任务提交，但是会继续处理等待队列中的任务private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; // 000 00000000000000000000000000000 (0)// 不接收新任务，也不处理队列任务，并且中断所有处理中的任务private static final int STOP = 1 &lt;&lt; COUNT_BITS; // 001 00000000000000000000000000000 ( 268435456)// 所有任务都被终结，有效线程为0。会触发terminated()方法private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; // 010 00000000000000000000000000000 (1073741824)// 当terminated()方法执行结束时状态private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; // 011 00000000000000000000000000000 (1610612736)// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; // 取高三位，状态&#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; // 取低29位，线程数&#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc;&#125;/* * Bit field accessors that don't require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */private static boolean runStateLessThan(int c, int s) &#123; return c &lt; s;&#125;private static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s;&#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125;/** * 线程数自增 1 * Attempts to CAS-increment the workerCount field of ctl. */private boolean compareAndIncrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect + 1);&#125;/** * 线程数自减 1 * Attempts to CAS-decrement the workerCount field of ctl. */private boolean compareAndDecrementWorkerCount(int expect) &#123; return ctl.compareAndSet(expect, expect - 1);&#125;/** * 只有当某线程被突然终止时才会调用该方法，其他线程数自减是在执行新的task时 * &lt;p&gt; */private void decrementWorkerCount() &#123; do &#123; &#125; while (!compareAndDecrementWorkerCount(ctl.get()));&#125; 看了这几种状态的介绍，读者大体也可以猜到十之八九的状态转换了，各个状态的转换过程有以下几种： 123456789RUNNING -&gt; SHUTDOWN：当调用了 shutdown() 后，会发生这个状态转换，这也是最重要的(RUNNING or SHUTDOWN) -&gt; STOP：当调用 shutdownNow() 后，会发生这个状态转换，这下要清楚 shutDown() 和 shutDownNow() 的区别了SHUTDOWN -&gt; TIDYING：当任务队列和线程池都清空后，会由 SHUTDOWN 转换为 TIDYINGSTOP -&gt; TIDYING：当任务队列清空后，发生这个转换TIDYING -&gt; TERMINATED：这个前面说了，当 terminated() 方法结束后 上面的几个记住核心的就可以了，尤其第一个和第二个。 另外，我们还要看看一个内部类 Worker，因为 Doug Lea 把线程池中的线程包装成了一个个 Worker，翻译成工人，就是线程池中做任务的线程。所以到这里，我们知道任务是 Runnable（内部叫 task 或 command），线程是 Worker。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/*** 继承了AbstractQueuedSynchronizer以简化获取和释放围绕每个任务执行的锁。* 这可以防止中断旨在唤醒等待任务的工作线程，而不是中断正在运行的任务。*/private final class Worker extends AbstractQueuedSynchronizer implements Runnable &#123; private static final long serialVersionUID = 6138294804551838833L; /** * 由ThreadFactory创建的真实执行任务的线程 */ final Thread thread; /** * 前面说了，这里的 Runnable 是任务。 * 为什么叫 firstTask？因为在创建线程的时候，如果同时指定了这个线程起来以后需要执行的第一个任务， * 那么第一个任务就是存放在这里的(线程可不止执行这一个任务) * 当然了，也可以为 null，这样线程起来了，自己到任务队列（BlockingQueue）中取任务（getTask 方法） */ Runnable firstTask; /** * 用于存放此线程完全的任务数，注意了，这里用了 volatile，保证可见性 */ volatile long completedTasks; /** * Worker 只有这一个构造方法，传入 firstTask，也可以传 null */ Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 调用 ThreadFactory 来创建一个新的线程 this.thread = getThreadFactory().newThread(this); &#125; /** * 这里调用了外部类的 runWorker 方法 */ public void run() &#123; runWorker(this); &#125; ...// 其他几个方法没什么好看的，就是用 AQS 操作，来获取这个线程的执行权，用了独占锁&#125; excute()有了上面的这些基础后，我们终于可以看看 ThreadPoolExecutor 的 execute 方法了，前面源码分析的时候也说了，各种方法都最终依赖于 execute 方法. 首先分析下execute主要工作流程： （1）任务submit后先通过newTaskFor()封装成可返回结果的FutureTask; （2）调用execute方法执行； （3）execute方法在当前线程数（WC）小于coreSize时，直接创建新线程处理； （4）如果创建新线程失败，尝试加入任务队列，若此时线程池已经处于非Running状态，则不做处理； （5）成功加入任务队列后需要再次确认线程池状态（有可能在加入队列操作的过程中，线程池被shutdown了），如果此时线程池非Running,则移除该任务，执行拒绝策略；如果状态正常，则判断WC==0，如果等于0说明线程池中没有线程了，则创建一个新线程添加到pool中； （6）如果加入队列失败或者当前状态非Running, 则尝试创建新线程来处理该任务，如果失败，则执行拒绝策略； 具体流程如下图： 本来想用一张图表示整个流程，结果发现图还没有看源代码清晰，干脆放弃了，直接看代码吧。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public void execute(Runnable command) &#123; if (command == null) &#123; throw new NullPointerException(); &#125; /* * 三步走： * * 1. 当前线程数小于corePoolSize，添加一个新的worker,并把commond作为其第一个任务。 * 调用addWorker()方法会自动检查runState和workerCount，避免因为状态问题报错 * * 2. 任务成功加入队列后，仍然需要再次确认是否增加新的工作线程（有可能在上次检测运行线程数之后某些线程挂了）， * 或者在进入这个方法时，线程池shut down了。 * So we recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. 如果无法添加到队列，则尝试创建新线程。如果失败，则表示线程池shutdown了或者需要执行拒绝策略了。 * * 由此可见：在线程数超过corePoolSize后，只有队列满了才会再次创建新线程 */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加任务成功，那么就结束了。提交任务嘛，线程池已经接受了这个任务，这个方法也就可以返回了 // 至于执行的结果，到时候会包装到 FutureTask 中。 // 返回 false 代表线程池不允许提交任务 if (addWorker(command, true)) &#123; return; &#125; c = ctl.get(); &#125; // 到这里说明，要么当前线程数大于等于核心线程数，要么刚刚 addWorker 失败了 // 如果线程池处于 RUNNING 状态，把这个任务添加到任务队列 workQueue 中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; /* 如果任务进入了 workQueue，我们是否需要开启新的线程 * 因为线程数在 [0, corePoolSize) 是无条件开启新的线程 * 如果线程数已经大于等于 corePoolSize，那么将任务添加到队列中，然后进到这里 */ int recheck = ctl.get(); if (!isRunning(recheck) &amp;&amp; remove(command)) &#123; // 如果线程池已不处于RUNNING状态，那么移除已经入队的这个任务 reject(command); // 执行拒绝策略 &#125; else if (workerCountOf(recheck) == 0) &#123; // 如果线程池还是 RUNNING 的，并且线程数为 0，那么开启新的线程 // 这块代码的真正意图是：担心任务提交到队列中了，但是线程都关闭了 addWorker(null, false); &#125; &#125; else if (!addWorker(command, false)) &#123; // 线程池非Running或者队列满了，尝试创建新线程 // 创建新线程失败，说明当前线程数已经达到 maximumPoolSize，执行拒绝策略 reject(command); &#125;&#125; addWorker()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 简单分析： * 还是状态控制的问题，当线程池处于 SHUTDOWN 的时候，不允许提交任务，但是已有的任务继续执行 * 当状态大于 SHUTDOWN 时，不允许提交任务，且中断正在执行的任务 * 多说一句： * 如果线程池处于 SHUTDOWN，但是firstTask为null，且 workQueue 非空，那么是允许创建 worker 的 * * @param firstTask 准备提交给这个线程执行的第一个任务，可以为null. * @param core true 代表使用核心线程数 corePoolSize 作为创建线程的界线，也就说创建这个线程的时候， * 如果线程池中的线程总数已经达到 corePoolSize，那么不能响应这次创建线程的请求 * 如果是 false，代表使用最大线程数 maximumPoolSize 作为界线 */private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (; ; ) &#123; int c = ctl.get(); int rs = runStateOf(c); // 获取当前状态 // 如果线程池已关闭，并满足以下条件之一，那么不创建新的 worker： // 1. 线程池状态大于SHUTDOWN，其实也就是 STOP, TIDYING, 或 TERMINATED // 2. firstTask不为空 // 3. 任务队列为空 if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; !workQueue.isEmpty())) &#123; return false; &#125; for (; ; ) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) &#123; // 超容量了或者超过当前限制了，不允许创建 return false; &#125; // 如果成功，那么就是所有创建线程前的条件校验都满足了，准备创建线程执行任务了 // 这里失败的话，说明有其他线程也在尝试往线程池中创建线程 if (compareAndIncrementWorkerCount(c)) &#123; break retry; // 退出循环，准备创建线程执行任务 &#125; c = ctl.get(); // 由于有并发，重新再读取一下 ctl // 正常如果是 CAS 失败的话，进到下一个里层的for循环就可以了 // 可是如果是因为其他线程的操作，导致线程池的状态发生了变更，如有其他线程关闭了这个线程池 // 那么需要回到外层的for循环 if (runStateOf(c) != rs) &#123; continue retry; &#125; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; /* * 到这里，我们认为在当前这个时刻，可以开始创建线程来执行任务了， * 因为该校验的都校验了，至于以后会发生什么，那是以后的事，至少当前是满足条件的 */ boolean workerStarted = false; // worker 是否已经启动 boolean workerAdded = false; // 是否已将这个 worker 添加到 workers 这个 HashSet 中 Worker w = null; try &#123; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; // 这个是整个类的全局锁，持有这个锁才能让下面的操作“顺理成章”， // 因为关闭一个线程池需要这个锁，至少我持有锁的期间，线程池不会被关闭 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 小于 SHUTTDOWN 那就是 RUNNING，这个自不必说，是最正常的情况 // 如果等于 SHUTDOWN，前面说了，不接受新的任务，但是会继续执行等待队列中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // 检测线程是否已经是start状态 &#123; // 新添加的worker里面的 thread 可不能是已经启动的 throw new IllegalThreadStateException(); &#125; workers.add(w); // 加到 workers 这个 HashSet 中 int s = workers.size(); // largestPoolSize 用于记录 workers 中的个数的历史最大值 // 因为 workers 是不断增加减少的，通过这个值可以知道线程池的大小曾经达到的最大值 if (s &gt; largestPoolSize) &#123; largestPoolSize = s; &#125; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 添加成功的话，启动这个线程 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (!workerStarted) &#123; // 如果线程没有启动，需要做一些清理工作，如前面 workCount 加了 1，将其减掉 addWorkerFailed(w); &#125; &#125; // 返回线程是否启动成功 return workerStarted;&#125; addWorkFailed()12345678910111213141516171819/** * 线程创建失败回滚 * workers 中删除掉相应的 worker * workCount 减 1 * 终止检查，防止这个线程的存在阻碍了线程池的terminate */private void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (w != null) &#123; workers.remove(w); &#125; decrementWorkerCount(); tryTerminate(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; runWorker()回过头来，继续往下走。我们知道，worker 中的线程 start 后，其 run 方法会调用 runWorker 方法： 1234// Worker 类的 run() 方法public void run() &#123; runWorker(this);&#125; 继续往下看 runWorker 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 实际执行task, 循环从队列中取任务执行 * * @param w the worker */final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; // 循环调用 getTask 获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // 如果线程池状态大于等于 STOP，那么意味着该线程也要中断 // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) &#123; wt.interrupt(); &#125; try &#123; // 这是一个钩子方法，留给需要的子类实现 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 到这里终于可以执行任务了 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; // 这里不允许抛出 Throwable，所以转换为 Erro thrown = x; throw new Error(x); &#125; finally &#123; // 也是一个钩子方法，将 task 和异常作为参数，留给需要的子类实现 afterExecute(task, thrown); &#125; &#125; finally &#123; // 置空 task，准备 getTask 获取下一个任务 task = null; // 累加该worker完成的任务数 w.completedTasks++; // 释放掉 worker 的独占锁 w.unlock(); &#125; &#125; completedAbruptly = false; // 执行到这儿说明是getTask()为空，而不是报异常了 &#125; finally &#123; // 如果到这里，需要执行线程关闭： // 1. 说明 getTask 返回 null，也就是说，这个 worker 的使命结束了，执行关闭 // 2. 任务执行过程中发生了异常 // 第一种情况，已经在代码处理了将 workCount 减 1，这个在 getTask 方法分析中会说 // 第二种情况，workCount 没有进行处理，所以需要在 processWorkerExit 中处理 processWorkerExit(w, completedAbruptly); &#125;&#125; processWorkerExit()12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 线程终止后: * 1. 如果是异常退出, 则需要减掉当前workercCount * 2. 更新线程池完成任务数 * 3. 从workers中移除终止的线程； * 4. 终止检测 * 5. 如果线程池当前处于RUNNING/SHUTDOWN状态： * a) 允许回收核心线程时，至少要保证有一个worker线程； * b) 不允许回收核心线程时，当前线程小于corePoolSize，则创建新的线程； * c）如果worker线程是由于异常退出，则直接创建一个新的worker线程 * * @param w the worker * @param completedAbruptly true woker执行异常 */private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted &#123; decrementWorkerCount(); &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; completedTaskCount += w.completedTasks; workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 终止检查，防止这个线程的存在阻碍了线程池的terminate tryTerminate(); int c = ctl.get(); if (runStateLessThan(c, STOP)) &#123; // RUNNING/SHUTDOWN if (!completedAbruptly) &#123; // 说明当前任务队列中没有任务 int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; !workQueue.isEmpty()) &#123; // 允许回收核心线程 min = 1; &#125; if (workerCountOf(c) &gt;= min) &#123; // 当前线程数大于1 或者 corePoolSize, 暂时不创建新的线程 return; // replacement not needed &#125; &#125; addWorker(null, false); // 添加新的备用线程 &#125;&#125; getTask()getTask() 是怎么获取任务的，这个方法写得真的很好，每一行都很简单，组合起来却所有的情况都想好了： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 此方法有三种可能： * 1. 阻塞直到获取到任务返回。我们知道，默认 corePoolSize 之内的线程是不会被回收的，它们会一直等待任务 * 2. 超时退出。keepAliveTime 起作用的时候，也就是如果这么多时间内都没有任务，那么应该执行关闭 * 3. 如果发生了以下条件，此方法必须返回 null: * - 池中有大于 maximumPoolSize 个 workers 存在(通过调用 setMaximumPoolSize 进行设置) * - 线程池处于 SHUTDOWN，而且 workQueue 是空的，前面说了，这种不再接受新的任务 * - 线程池处于 STOP，不仅不接受新的线程，连 workQueue 中的线程也不再执行 */private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? for (; ; ) &#123; int c = ctl.get(); int rs = runStateOf(c); // 两种可能 // 1. rs == SHUTDOWN &amp;&amp; workQueue.isEmpty() // 2. rs &gt;= STOP if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); // 减少工作线程数, processWorkerExit()方法中会将该线程移除 return null; &#125; int wc = workerCountOf(c); // 允许核心线程数内的线程回收，或当前线程数超过了核心线程数，那么有可能发生超时关闭 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 当前线程数超过maximumPoolSize // 允许回收核心线程或者当前线程超过corePoolSize &amp;&amp; 超时 // wc &gt; 1 或者 队列为空 if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) &#123; // 减掉线程数 return null; // 获取任务的worker取不到任务就会退出 &#125; continue; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : // 等待一定时间，如果仍然获取不到说明线程数过多，任务不够 workQueue.take(); // 阻塞获取 if (r != null) &#123; return r; &#125; timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果此 worker 发生了中断，采取的方案是重试 // 解释下为什么会发生中断，这个读者要去看 setMaximumPoolSize 方法， // 如果开发者将 maximumPoolSize 调小了，导致其小于当前的 workers 数量， // 那么意味着超出的部分线程要被关闭。重新进入 for 循环，自然会有部分线程会返回 null timedOut = false; &#125; &#125;&#125; tryTerminate()1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 把线程池状态设置为TERMINATED，在以下条件之一： * 1. SHUTDOWN 并且 pool线程和队列都为空； * 2. STOP 并且 pool线程为空； * &lt;p&gt; * 如果线程不为0时想要优雅终止，则中断空闲的worker线程以保证shutdown信号得到传播。 * * This method must be called following any action that might make * termination possible -- reducing worker count or removing tasks * from the queue during shutdown. The method is non-private to * allow access from ScheduledThreadPoolExecutor. */final void tryTerminate() &#123; for (; ; ) &#123; int c = ctl.get(); // 状态为RUNNING、SHUTDOWN、STOP 或者 (SHUTDOWN &amp;&amp; 队列不为空) 不允许 if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; !workQueue.isEmpty())) &#123; return; &#125; if (workerCountOf(c) != 0) &#123; // 终止一个空闲线程 interruptIdleWorkers(ONLY_ONE); return; &#125; // 当前线程数为0 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; // 设置状态为TIDYING try &#123; terminated(); // 触发终止后方法 &#125; finally &#123; ctl.set(ctlOf(TERMINATED, 0)); // 最终设置为TERMINATED状态 termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125; 拒绝策略ThreadPoolExecutor 中的拒绝策略。 12345678910111213141516171819/** * 此处的 handler 我们需要在构造线程池的时候就传入这个参数，它是 RejectedExecutionHandler 的实例。 * RejectedExecutionHandler 在 ThreadPoolExecutor 中有四个已经定义好的实现类可供我们直接使用， * 当然，我们也可以实现自己的策略，不过一般也没有必要。简要介绍下四中默认的拒绝策略： * &lt;p&gt; * 1. CallerRunsPolicy： 只要线程池没有被关闭，那么由提交任务的线程自己来执行这个任务 * &lt;p&gt; * 2. AbortPolicy：不管怎样，直接抛出 RejectedExecutionException 异常， 这个是默认的策略， * 如果我们构造线程池的时候不传相应的 handler 的话，那就会指定使用这个 * &lt;p&gt; * 3. DiscardPolicy：不做任何处理，直接忽略掉这个任务 * &lt;p&gt; * 4. DiscardOldestPolicy： 这个相对霸道一点，如果线程池没有被关闭的话， 把队列队头的任务(也就是等待了最长时间的)直接扔掉， * 然后提交这个任务到等待队列中 */ final void reject(Runnable command) &#123; // 执行拒绝策略 handler.rejectedExecution(command, this); &#125; Executors Executors它仅仅是工具类，它的所有方法都是 static 的。 FixedThreadPoole生成一个固定大小的线程池： 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 最大线程数设置为与核心线程数相等，此时 keepAliveTime 设置为 0（因为这里它是没用的，即使不为 0 也不会执行 corePoolSize 内的线程），任务队列采用 LinkedBlockingQueue，无界队列。 过程分析：刚开始，每提交一个任务都创建一个 worker，当 worker 的数量达到 nThreads 后，不再创建新的线程，而是把任务提交到 LinkedBlockingQueue 中，而且之后线程数始终为 nThreads。 SingleThreadExecutor生成只有一个线程的固定线程池，这个更简单，和上面的一样，只要设置线程数为 1 就可以了： 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; newCachedThreadPool生成一个需要的时候就创建新的线程，同时可以复用之前创建的线程（如果这个线程当前没有任务）的线程池： 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 过程分析：鉴于 corePoolSize 是 0，那么提交任务的时候，直接将任务提交到队列中，由于采用了 SynchronousQueue，所以如果是第一个任务提交的时候，offer 方法肯定会返回 false，因为此时没有任何 worker 对这个任务进行接收，那么将进入到最后一个分支来创建第一个 worker。之后再提交任务的话，取决于是否有空闲下来的线程对任务进行接收，如果有，会进入到第二个 if 语句块中，否则就是和第一个任务一样，进到最后的 else if 分支。 这种线程池对于任务可以比较快速地完成的情况有比较好的性能。如果线程空闲了 60 秒都没有任务，那么将关闭此线程并从线程池中移除。所以如果线程池空闲了很长时间也不会有问题，因为随着所有的线程都会被关闭，整个线程池不会占用任何的系统资源。 SynchronousQueue 是一个比较特殊的 BlockingQueue，其本身不储存任何元素，它有一个虚拟队列（或虚拟栈），不管读操作还是写操作，如果当前队列中存储的是与当前操作相同模式的线程，那么当前操作也进入队列中等待；如果是相反模式，则配对成功，从当前队列中取队头节点。具体的信息，可以看我的另一篇关于 BlockingQueue 的文章。 代码带注释完整代码： https://github.com/austin-brant/thread-pool-source-code]]></content>
      <categories>
        <category>Java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Boot + Kafka实现生产+批量消费]]></title>
    <url>%2F2019%2F07%2F16%2FSpring-Boot-Kafka%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7-%E6%89%B9%E9%87%8F%E6%B6%88%E8%B4%B9%2F</url>
    <content type="text"><![CDATA[本文是Springboot + Kafka实现消息写入和批量消费，属于一个学习demo，下面直接上代码。 POM依赖1234567891011121314151617181920212223242526272829303132333435&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.2.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#============== kafka ===================# 指定kafka 代理地址，可以多个spring.kafka.bootstrap-servers=10.101.38.213:8092#指定template默认topic idspring.kafka.template.default-topic=topic-test#=============== provider =======================## 重试次数spring.kafka.producer.retries=3# 批量发送消息数量Bytesspring.kafka.producer.batch-size=16384# 32M批处理缓冲区spring.kafka.producer.buffer-memory=33554432spring.kafka.producer.properties.linger-ms=1# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer#=============== consumer =======================# 指定默认消费者group idspring.kafka.consumer.group-id=etl# 最早未被消费的offset, 若设置为earliest，那么会从头开始读partitionspring.kafka.consumer.auto-offset-reset=earliest# 批量一次最大拉取数据量spring.kafka.consumer.max-poll-records=5# 如果没有足够的数据立即满足“fetch.min.bytes”给出的要求，服务器在回答获取请求之前将阻塞的最长时间（以毫秒为单位）spring.kafka.consumer.fetch-max-wait=10000# 自动提交spring.kafka.consumer.enable-auto-commit=falsespring.kafka.consumer.auto-commit-interval=10000# 连接超时时间, 自定义spring.kafka.consumer.session-timeout=15000# 指定消息key和消息体的编解码方式spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer#=============== listener =======================# 指定listener 容器中的线程数，用于提高并发量spring.kafka.listener.concurrency=1# 轮询消费者时使用的超时（以毫秒为单位）spring.kafka.listener.poll-timeout=50000# 是否开启批量消费，true表示批量消费spring.kafka.listener.batch-listener=truetopic.name=springDemo Kafka配置 如果不需要批量消费，只需KafkaTemplate进行produce， 则不需要该显式配置类，spring-boot的自动配置会根据配置文件帮我们创建好KafkaTemplate对象。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136package com.austin.brant.kafka.demo.config;import java.util.HashMap;import java.util.Map;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.common.serialization.StringDeserializer;import org.apache.kafka.common.serialization.StringSerializer;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.kafka.annotation.EnableKafka;import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;import org.springframework.kafka.config.KafkaListenerContainerFactory;import org.springframework.kafka.core.DefaultKafkaConsumerFactory;import org.springframework.kafka.core.DefaultKafkaProducerFactory;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.kafka.core.ProducerFactory;import org.springframework.kafka.listener.ContainerProperties;/** * @author austin-brant * @since 2019/7/15 21:45 */@Configuration@EnableKafkapublic class KafkaConfig &#123; @Value("$&#123;spring.kafka.bootstrap-servers&#125;") private String bootstrapServers; @Value("$&#123;spring.kafka.producer.retries&#125;") private String producerRetries; // 生产者重试次数 @Value("$&#123;spring.kafka.producer.batch-size&#125;") private String producerBatchSize; @Value("$&#123;spring.kafka.producer.properties.linger-ms&#125;") private String producerLingerMs; @Value("$&#123;spring.kafka.producer.buffer-memory&#125;") private String producerBufferMemory; @Value("$&#123;spring.kafka.consumer.enable-auto-commit&#125;") private Boolean autoCommit; @Value("$&#123;spring.kafka.consumer.auto-commit-interval&#125;") private Integer autoCommitInterval; @Value("$&#123;spring.kafka.consumer.group-id&#125;") private String groupId; @Value("$&#123;spring.kafka.consumer.max-poll-records&#125;") private Integer maxPollRecords; @Value("$&#123;spring.kafka.consumer.fetch-max-wait&#125;") private Integer maxPollIntervals; @Value("$&#123;spring.kafka.consumer.auto-offset-reset&#125;") private String autoOffsetReset; @Value("$&#123;spring.kafka.listener.concurrency&#125;") private Integer concurrency; @Value("$&#123;spring.kafka.listener.poll-timeout&#125;") private Long pollTimeout; @Value("$&#123;spring.kafka.consumer.session-timeout&#125;") private String sessionTimeout; @Value("$&#123;spring.kafka.listener.batch-listener&#125;") private Boolean batchListener; /** * ProducerFactory */ @Bean public ProducerFactory&lt;String, String&gt; producerFactory() &#123; Map&lt;String, Object&gt; configs = new HashMap&lt;String, Object&gt;(); //参数 configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); configs.put(ProducerConfig.RETRIES_CONFIG, producerRetries); configs.put(ProducerConfig.BATCH_SIZE_CONFIG, producerBatchSize); configs.put(ProducerConfig.LINGER_MS_CONFIG, producerLingerMs); configs.put(ProducerConfig.BUFFER_MEMORY_CONFIG, producerBufferMemory); configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); return new DefaultKafkaProducerFactory&lt;String, String&gt;(configs); &#125; /** * KafkaTemplate */ @Bean public KafkaTemplate&lt;String, String&gt; kafkaTemplate() &#123; return new KafkaTemplate&lt;String, String&gt;(producerFactory(), true); &#125; /** * 添加KafkaListenerContainerFactory，用于批量消费消息 */ @Bean public KafkaListenerContainerFactory&lt;?&gt; batchFactory() &#123; ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(new DefaultKafkaConsumerFactory&lt;Object, Object&gt;(consumerConfigs())); factory.setBatchListener(batchListener); // 开启批量监听 factory.setConcurrency(concurrency); // 并发消费线程 factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE); factory.getContainerProperties().setPollTimeout(pollTimeout); return factory; &#125; @Bean public Map&lt;String, Object&gt; consumerConfigs() &#123; Map&lt;String, Object&gt; props = new HashMap&lt;&gt;(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers); props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, maxPollRecords); // 批量消费的数量 props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, maxPollIntervals); //每一批读取间隔时间 props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetReset); // 最早未被消费的offset props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, autoCommit); // 是否自动提交 props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, autoCommitInterval); // 自动提交间隔 props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, sessionTimeout); // props.put(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG, 180000); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); return props; &#125;&#125; 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.austin.brant.kafka.demo.provider;import java.util.Date;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.kafka.support.SendResult;import org.springframework.stereotype.Component;import org.springframework.util.concurrent.ListenableFuture;import org.springframework.util.concurrent.ListenableFutureCallback;import com.austin.brant.kafka.demo.model.Message;import lombok.extern.slf4j.Slf4j;/** * 生产者 * * @author austin-brant * @since 2019/7/15 19:39 */@Component@Slf4jpublic class KafkaProducer &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; public void send(String topic, String message) &#123; ListenableFuture&lt;SendResult&lt;String, String&gt;&gt; future = kafkaTemplate.send(topic, Message.builder() .id(System.currentTimeMillis()) .msg(message) .sendTime(new Date()).build().toString()); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, String&gt;&gt;() &#123; @Override public void onFailure(Throwable throwable) &#123; log.error("send message [&#123;&#125;] to topic [&#123;&#125;] failed, ", message, topic); &#125; @Override public void onSuccess(SendResult&lt;String, String&gt; stringStringSendResult) &#123; log.info("send message [&#123;&#125;] to topic [&#123;&#125;] success, ", message, topic); &#125; &#125;); log.info("send message end"); &#125; public void batchSend(String topic, List&lt;String&gt; message) &#123; message.forEach(it -&gt; kafkaTemplate.send(topic, it)); &#125;&#125; 消费者12345678910111213141516171819202122232425262728293031323334353637383940414243package com.austin.brant.kafka.demo.consumer;import java.util.List;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.kafka.support.Acknowledgment;import org.springframework.stereotype.Component;import lombok.extern.slf4j.Slf4j;/** * 消费者 * * @author austin-brant * @since 2019/7/15 19:58 */@Slf4j@Componentpublic class KafkaConsumer &#123; // @KafkaListener(topics = "$&#123;topic.name&#125;") // public void listen(ConsumerRecord&lt;String, String&gt; record) &#123; // consumer(record); // &#125; @KafkaListener(topics = &#123;"$&#123;topic.name&#125;"&#125;, containerFactory = "batchFactory", id = "consumer") public void listen(List&lt;ConsumerRecord&lt;String, String&gt;&gt; records, Acknowledgment ack) &#123; log.info("batch listen size &#123;&#125;.", records.size()); try &#123; records.forEach(it -&gt; consumer(it)); &#125; finally &#123; ack.acknowledge(); //手动提交偏移量 &#125; &#125; /** * 单条消费 */ public void consumer(ConsumerRecord&lt;String, String&gt; record) &#123; log.info("主题:&#123;&#125;, 内容: &#123;&#125;", record.topic(), record.value()); &#125;&#125; 完整代码：https://github.com/austin-brant/kafka-spring-boot-demo]]></content>
      <categories>
        <category>中间件</category>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>入门</tag>
        <tag>Spring-Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java常用设计模式]]></title>
    <url>%2F2019%2F07%2F15%2FJava%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Java常用设计模式设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结，其中最出名的当属 Gang of Four (GoF) 的分类了，他们将设计模式分类为 23 种经典的模式，根据用途我们又可以分为三大类，分别为: 创建型模式 结构型模式 行为型模式 六大原则有6大重要的设计原则在开篇和大家分享下，这些原则将贯通全文： 单一职责原则每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来。 开发-封闭原则对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。它是面向对象设计的核心所在。 依赖倒转原则抽象不应该依赖细节，细节应该依赖于抽象，说白了就是要针对接口编程，不要面向实现编程。a)高层模块不应依赖低层模块。两个都应该依赖抽象；b)抽象不应该依赖细节。细节应该依赖抽象。 里氏代换原则白话翻译：一个软件实体如果使用的是一个父类的话，那么一定适用于其子类，而且它察觉不出父类对象和子类对象的区别。也就是说：在软件里把父类都用子类替换，程序行为不会变化。 总结成一句话就是：子类型必须能够替换它们的父类型。 接口隔离原则类间的依赖关系应该建立在最小的接口上。通俗来讲：建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。也就是说，我们要为各个类建立专用的接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 迪米特法则也叫最少知识原则。如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法的话，可以通过第三者转发这个调用。其根本思想就是强调类之间的松耦合。 创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式和名字一样简单，非常简单，直接上代码吧： 123456789101112131415public class FoodFactory &#123; public static Food makeFood(String name) &#123; if (name.equals("noodle")) &#123; Food noodle = new LanZhouNoodle(); noodle.addSpicy("more"); return noodle; &#125; else if (name.equals("chicken")) &#123; Food chicken = new HuangMenChicken(); chicken.addCondiment("potato"); return chicken; &#125; else &#123; return null; &#125; &#125;&#125; 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 1234567891011121314151617181920212223242526272829public interface FoodFactory &#123; Food makeFood(String name);&#125;public class ChineseFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new ChineseFoodA(); &#125; else if (name.equals("B")) &#123; return new ChineseFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;public class AmericanFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals("A")) &#123; return new AmericanFoodA(); &#125; else if (name.equals("B")) &#123; return new AmericanFoodB(); &#125; else &#123; return null; &#125; &#125;&#125; 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 12345678public class APP &#123; public static void main(String[] args) &#123; // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood("A"); &#125;&#125; 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样读者看着比较清晰： 抽象工厂模式当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： 12345678910// 得到 Intel 的 CPUCPUFactory cpuFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard); 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果 Intel 家产的 CPU 和 AMD 产的主板不能兼容使用，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 12345678910111213public static void main(String[] args) &#123; // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);&#125; 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式单例模式用得最多，错得最多。 饿汉模式最简单： 1234567891011121314151617public class Singleton &#123; // 首先，将 new Singleton() 堵死 private Singleton() &#123;&#125;; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 private static Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125; // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) &#123; return new Date(); &#125;&#125; 很多人都能说出饿汉模式的缺点，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 1234567891011121314151617181920public class Singleton &#123; // 首先，也是先堵死 new Singleton() 这条路 private Singleton() &#123;&#125; // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() &#123; if (instance == null) &#123; // 加锁 synchronized (Singleton.class) &#123; // 这一次判断也是必须的，不然会有并发问题 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 双重检查，指的是两次检查 instance 是否为 null。volatile 在这里是需要的，希望能引起读者的关注。很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 1234567891011public class Singleton3 &#123; private Singleton3() &#123;&#125; // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder &#123; private static Singleton3 instance = new Singleton3(); &#125; public static Singleton3 getInstance() &#123; return Holder.instance; &#125;&#125; 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： 12Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build(); 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class User &#123; // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) &#123; this.name = name; this.password = password; this.nickName = nickName; this.age = age; &#125; // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() &#123; return new UserBuilder(); &#125; public static class UserBuilder &#123; // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() &#123; &#125; // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) &#123; this.name = name; return this; &#125; public UserBuilder password(String password) &#123; this.password = password; return this; &#125; public UserBuilder nickName(String nickName) &#123; this.nickName = nickName; return this; &#125; public UserBuilder age(int age) &#123; this.age = age; return this; &#125; // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() &#123; if (name == null || password == null) &#123; throw new RuntimeException("用户名和密码必填"); &#125; if (age &lt;= 0 || age &gt;= 150) &#123; throw new RuntimeException("年龄不合法"); &#125; // 还可以做赋予”默认值“的功能 if (nickName == null) &#123; nickName = name; &#125; return new User(name, password, nickName, age); &#125; &#125;&#125; 代码核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 123456789public class APP &#123; public static void main(String[] args) &#123; User d = User.builder() .name("foo") .password("pAss12345") .age(25) .build(); &#125;&#125; 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 1234567@Builderclass User &#123; private String name; private String password; private String nickName; private int age;&#125; 怎么样，省下来的时间是不是又可以干点别的了。当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： 1User user = new User().setName("").setPassword("").setAge(20); 原型模式这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 1protected native Object clone() throws CloneNotSupportedException; java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 总结 创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式。 单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源； 建造者模式专门对付属性很多的那种类，为了让代码更优美； 原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken("1kg"); f.setSpicy("1g"); f.setSalt("3g"); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle("500g"); f.setSalt("5g"); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println("我们马上要开始制作鸡肉了"); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println("鸡肉制作完成啦，加点胡椒粉"); // 增强 food.addCondiment("pepper"); return food; &#125; public Food makeNoodle() &#123; System.out.println("准备制作拉面~"); Food food = foodService.makeNoodle(); System.out.println("制作完成啦") return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。 因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 12345678910111213141516171819public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println("咕咕叫"); &#125; public void fly() &#123; System.out.println("鸡也会飞哦"); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 1Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合；类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。总体来说，对象适配用得比较多。 适配器模式和代理模式的异同比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样: 代理模式做的是增强原方法的活； 适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 1234567891011121314151617181920public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 12345678910111213141516171819202122232425262728// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 12345678910111213141516171819public class BlackTea extends Beverage &#123; public String getDescription() &#123; return "红茶"; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return "绿茶"; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829303132333435public class Lemon extends Condiment &#123; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 private Beverage bevarage; public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + ", 加柠檬"; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + ", 加芒果"; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + " 价格：￥" + beverage.cost()); //"绿茶, 加柠檬, 加芒果 价格：￥16"&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如： BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能 LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了 DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 123LineNumberInputStream is = new LineNumberInputStream( new BufferedInputStream( new FileInputStream(""))); InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println("Circle::draw()"); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println("Rectangle::draw()"); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return ("Employee :[ Name : " + name + ", dept : " + dept + ", salary :" + salary+" ]"); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren()这些方法。这说的其实就是组合模式。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。 Flyweight 是轻量级的意思，享元分开来说就是 共享元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。 代理模式是做方法增强的 适配器模式是把鸡包装成鸭这种用来适配接口的 桥梁模式做到了很好的解耦 装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景 门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可 组合模式用于描述具有层次结构的数据 享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能 行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。首先，先定义一个策略接口： 123public interface Strategy &#123; public void draw(int radius, int x, int y);&#125; 然后我们定义具体的几个策略： 1234567891011121314151617181920public class RedPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用红色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class GreenPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用绿色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125;public class BluePen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println("用蓝色笔画图，radius:" + radius + ", x:" + x + ", y:" + y); &#125;&#125; 使用策略的类： 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeDraw(int radius, int x, int y)&#123; return strategy.draw(radius, x, y); &#125;&#125; 客户端演示： 1234public static void main(String[] args) &#123; Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);&#125; 放到一张图上，让大家看得清晰些： 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 1234567891011121314151617181920212223242526public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; // 通知观察者们 public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 定义观察者接口： 1234public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125; 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 123456789101112131415161718192021222324252627282930public class BinaryObserver extends Observer &#123; // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) &#123; this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); &#125; // 该方法由主题类在数据变更的时候进行调用 @Override public void update() &#123; String result = Integer.toBinaryString(subject.getState()); System.out.println("订阅的数据发生变化，新的数据处理为二进制值为：" + result); &#125;&#125;public class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println("订阅的数据发生变化，新的数据处理为十六进制值为：" + result); &#125;&#125; 客户端使用也非常简单： 12345678910public static void main(String[] args) &#123; // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);&#125; output: 12订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 1234567891011121314public abstract class RuleHandler &#123; // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) &#123; this.successor = successor; &#125; public RuleHandler getSuccessor() &#123; return successor; &#125;&#125; 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 1234567891011121314public class NewUserRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; if (context.isNewUser()) &#123; // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("该活动仅限新用户参与"); &#125; &#125;&#125; 校验用户所在地区是否可以参与： 123456789101112public class LocationRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException("非常抱歉，您所在的地区无法参与本次活动"); &#125; &#125;&#125; 校验奖品是否已领完： 123456789101112public class LimitRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(userInfo); &#125; &#125; else &#123; throw new RuntimeException("您来得太晚了，奖品被领完了"); &#125; &#125;&#125; 客户端： 123456789public static void main(String[] args) &#123; RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);&#125; 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。通常会有一个抽象类： 123456789101112131415public abstract class AbstractTemplate &#123; // 这就是模板方法 public void templateMethod()&#123; init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 &#125; protected void init() &#123; System.out.println("init 抽象层已经实现，子类也可以选择覆写"); &#125; // 留给子类实现 protected abstract void apply(); protected void end() &#123; &#125;&#125; 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。 也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 12345678public class ConcreteTemplate extends AbstractTemplate &#123; public void apply() &#123; System.out.println("子类实现抽象方法 apply"); &#125; public void end() &#123; System.out.println("我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了"); &#125;&#125; 客户端调用演示： 12345public static void main(String[] args) &#123; AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();&#125; 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。定义状态接口： 123public interface State &#123; public void doAction(Context context);&#125; 定义减库存的状态： 12345678910111213public class DeductState implements State &#123; public void doAction(Context context) &#123; System.out.println("商品卖出，准备减库存"); context.setState(this); //... 执行减库存的具体操作 &#125; public String toString()&#123; return "Deduct State"; &#125;&#125; 定义补库存状态： 1234567891011public class RevertState implements State &#123; public void doAction(Context context) &#123; System.out.println("给此商品补库存"); context.setState(this); //... 执行加库存的具体操作 &#125; public String toString() &#123; return "Revert State"; &#125;&#125; 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 1234567891011121314public class Context &#123; private State state; private String name; public Context(String name) &#123; this.name = name; &#125; public void setState(State state) &#123; this.state = state; &#125; public void getState() &#123; return this.state; &#125;&#125; 我们来看下客户端调用，大家就一清二楚了： 123456789101112131415public static void main(String[] args) &#123; // 我们需要操作的是 iPhone X Context context = new Context("iPhone X"); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();&#125; 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，这里不进行介绍了。 转载&amp;参考 【转载】 https://juejin.im/post/5bc96afff265da0aa94a4493【参考】 https://www.tutorialspoint.com/design_pattern/index.htm]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper概念介绍]]></title>
    <url>%2F2019%2F07%2F12%2FZookeeper%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Zookeeper简介概念Zookeeper最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 后来，Apache ZooKeeper成为Hadoop，HBase和其他分布式框架使用的有组织服务的标准。 例如，Apache HBase使用ZooKeeper跟踪分布式数据的状态。ZooKeeper 的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 名字由来Zookeeper名字的由来是比较有趣的，下面的片段摘抄自《从PAXOS到ZOOKEEPER分布式一致性原理与实践》一书： Zookeeper最早起源于雅虎的研究院的一个研究小组。在立项初期，考虑到很多项目都是用动物的名字来命名的(例如著名的Pig项目)，雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家Raghu Ramakrishnan开玩笑说：再这样下去，我们这儿就变成动物园了。此话一出，大家纷纷表示就叫动物园管理员吧——因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而Zookeeper正好用来进行分布式环境的协调——于是，Zookeeper的名字由此诞生了。 Curator无疑是Zookeeper客户端中的瑞士军刀，它译作”馆长”或者’’管理者’’，不知道是不是开发小组有意而为之，笔者猜测有可能这样命名的原因是说明Curator就是Zookeeper的馆长(脑洞有点大：Curator就是动物园的园长)。 应用场景 ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。 ZooKeeper的一些概念会话（Session）Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。 通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向Zookeeper服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的Watch事件通知。 Session的sessionTimeout值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。 在为客户端创建会话之前，服务端首先会为每个客户端都分配一个sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。 Znode在谈到分布式的时候，我们通常说的“节点”是指组成集群的每一台机器。然而，在Zookeeper中，“节点”分为两类，第一类同样是指构成集群的机器，我们称之为机器节点；第二类则是指数据模型中的数据单元，我们称之为数据节点一一ZNode。 Zookeeper将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。 在Zookeeper中，node可以分为持久节点和临时节点两类。所谓持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。而临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。 一旦节点被标记上这个属性，那么在这个节点被创建的时候，Zookeeper会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。 版本Zookeeper 的每个 ZNode 上都会存储数据，对应于每个ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构，Stat中记录了这个 ZNode 的三个数据版本，分别是： version（当前ZNode数据内容的版本号） cversion（当前ZNode子节点的版本号 aversion（当前ZNode的ACL变更版本号） 特别说明： ZK 中版本就是修改次数：即使修改前后，内容不变，但版本仍会+1： version=0 表示节点创建之后，修改的次数为 0。 cversion 子节点列表：ZNode，其中 cversion 只会感知子节点列表变更信息，新增子节点、删除子节点，而不会感知子节点数据内容的变更。 目标：解决 ZNode 的并发更新问题，实现 CAS（Compare And Switch）乐观锁。 WatcherWatcher（事件监听器），是Zookeeper中的一个很重要的特性。Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去，该机制是Zookeeper实现分布式协调服务的重要特性。 ACLZookeeper采用ACL（Access-Control-Lists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。Zookeeper 定义了如下5种权限。 CREATE: 能创建子节点 READ：能获取节点数据和列出其子节点 WRITE: 能设置节点数据 DELETE: 能删除子节点 ADMIN: 能设置ACL权限 其中尤其需要注意的是，CREATE和DELETE这两种权限都是针对子节点的权限控制. 重要概念总结 ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。 ZooKeeper 底层其实只提供了两个功能：①管理（存储、读取）用户程序提交的数据；②为用户程序提交数据节点监听服务。 Zookeeper特性 节点类型： 临时节点：客户端和服务端之间的Session过期之后节点会自动消失。 持久节点：创建节点之后，节点就会一直存在，除非手动删除。 临时顺序节点：拥有临时节点的特性，同时会根据创建的顺序给节点添加一个编号（编号作为节点名字的一部分）。 持久顺序节点：拥有持久节点的特性，同时会根据创建的顺序给节点添加一个编号（编号作为节点名字的一部分）。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 Watcher机制：节点数据变更注册时，在该节点的Watcher都会被通知。子节点列表变化注册该节点的Watcher也会被通知。 多个客户端同时创建一个节点，保证只有一个客户端可以创建成功。 对于有N台服务器组成的集群，保证有小于等于（N/2）-1 台服务器不能提供服务时，集群的数据仍然保持完整。 ZooKeeper 设计目标简单的数据模型ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。 名称空间由 ZooKeeper 中的数据寄存器组成 - 称为znode，这些类似于文件和目录。 与为存储设计的典型文件系统不同，ZooKeeper数据保存在内存中，这意味着ZooKeeper可以实现高吞吐量和低延迟。 可构建集群为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么zookeeper本身仍然是可用的。 客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务，客户端使用这个TCP链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。 ZooKeeper 官方提供的架构图： 上图中每一个Server代表一个安装Zookeeper服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 Zab 协议（Zookeeper Atomic Broadcast）来保持数据的一致性。 顺序访问对于来自客户端的每个更新请求，ZooKeeper 都会分配一个全局唯一的递增编号，这个编号反应了所有事务操作的先后顺序，应用程序可以使用 ZooKeeper 这个特性来实现更高层次的同步原语。 这个编号也叫做时间戳——zxid（Zookeeper Transaction Id） ZooKeeper 集群角色介绍最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了Leader、Follower 和 Observer 三种角色。如下图所示 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。 除了 Leader 外，Follower 和 Observer 都只能提供读服务。 Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>入门</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-Boot + Zookeeper(Curator)实现分布式锁]]></title>
    <url>%2F2019%2F07%2F12%2FSpring-Boot-Zookeeper-Curator-%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[Curator简介Apache Curator是Netflix公司开源的一个Zookeeper客户端，目前已经是Apache的顶级项目，与Zookeeper提供的原生客户端相比，Curator的抽象层次更高，简化了Zookeeper客户端的开发量，通过封装的一套高级API，里面提供了更多丰富的操作，例如session超时重连、主从选举、分布式计数器、分布式锁等等适用于各种复杂场景的zookeeper操作。本文介绍如果通过Zookeeper实现一个分布式锁。 代码结构 核心依赖123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.10&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; 配置文件1234567891011server.port=8012#重试次数curator.retryCount=5#重试间隔时间curator.elapsedTimeMs=5000# zookeeper 地址curator.connectString=10.101.38.213:8181# session超时时间curator.sessionTimeoutMs=60000# 连接超时时间curator.connectionTimeoutMs=5000 初始化ZK-Client1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.austin.brant.zk.demo.config;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.CuratorFrameworkFactory;import org.apache.curator.retry.RetryNTimes;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * curator配置 * * @author austin-brant * @since 2019/7/12 17:03 */@Configurationpublic class CuratorConfiguration &#123; @Value("$&#123;curator.retryCount&#125;") private int retryCount; @Value("$&#123;curator.elapsedTimeMs&#125;") private int elapsedTimeMs; @Value("$&#123;curator.connectString&#125;") private String connectString; @Value("$&#123;curator.sessionTimeoutMs&#125;") private int sessionTimeoutMs; @Value("$&#123;curator.connectionTimeoutMs&#125;") private int connectionTimeoutMs; @Bean(name = "curatorFramework", initMethod = "start") public CuratorFramework curatorFramework() &#123; return CuratorFrameworkFactory.newClient( connectString, sessionTimeoutMs, connectionTimeoutMs, new RetryNTimes(retryCount, elapsedTimeMs) ); &#125;&#125; 分布式锁实现类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132package com.austin.brant.zk.demo.utils;import java.util.concurrent.CountDownLatch;import javax.annotation.Resource;import org.apache.curator.framework.CuratorFramework;import org.apache.curator.framework.recipes.cache.PathChildrenCache;import org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent;import org.apache.curator.framework.recipes.cache.PathChildrenCacheListener;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.ZooDefs;import org.springframework.beans.factory.InitializingBean;import org.springframework.stereotype.Service;import lombok.extern.slf4j.Slf4j;/** * 基于zk的分布锁实现 * * @author austin-brant * @since 2019/7/12 17:17 */@Slf4j@Servicepublic class DistributedLockByZk implements InitializingBean &#123; private final static String ROOT_PATH_LOCK = "rootlock"; private CountDownLatch countDownLatch = new CountDownLatch(1); @Resource(name = "curatorFramework") private CuratorFramework curatorFramework; /** * 获取分布式锁 */ public void acquireDistributedLock(String path) &#123; String keyPath = "/" + ROOT_PATH_LOCK + "/" + path; while (true) &#123; try &#123; curatorFramework .create() .creatingParentsIfNeeded() .withMode(CreateMode.EPHEMERAL) // 临时节点 .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(keyPath); log.info("success to acquire lock for path:&#123;&#125;", keyPath); break; &#125; catch (Exception e) &#123; log.info("failed to acquire lock for path:&#123;&#125;", keyPath); log.info("while try again ......."); if (countDownLatch.getCount() &lt;= 0) &#123; countDownLatch = new CountDownLatch(1); &#125; try &#123; // 阻塞等待锁释放，重新获取 countDownLatch.wait(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; &#125; &#125; &#125; /** * 释放分布式锁 */ public boolean releaseDistributedLock(String path) &#123; String keyPath = "/" + ROOT_PATH_LOCK + "/" + path; try &#123; if (curatorFramework.checkExists().forPath(keyPath) != null) &#123; curatorFramework.delete().forPath(keyPath); &#125; &#125; catch (Exception e) &#123; log.error("failed to release lock"); return false; &#125; return true; &#125; /** * 创建 watcher 事件 */ private void addWatcher(String path) throws Exception &#123; String keyPath; if (path.equals(ROOT_PATH_LOCK)) &#123; keyPath = "/" + path; &#125; else &#123; keyPath = "/" + ROOT_PATH_LOCK + "/" + path; &#125; final PathChildrenCache cache = new PathChildrenCache(curatorFramework, keyPath, false); cache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT); cache.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_REMOVED)) &#123; String oldPath = event.getData().getPath(); log.info("success to release lock for path:&#123;&#125;", oldPath); if (oldPath.contains(path)) &#123; //释放计数器，让当前的请求获取锁 countDownLatch.countDown(); &#125; &#125; &#125; &#125;); &#125; /** * 初始化创建永久父节点 */ @Override public void afterPropertiesSet() &#123; curatorFramework = curatorFramework.usingNamespace("lock-namespace"); String path = "/" + ROOT_PATH_LOCK; try &#123; if (curatorFramework.checkExists().forPath(path) == null) &#123; curatorFramework .create() .creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(path); &#125; addWatcher(ROOT_PATH_LOCK); log.info("root path 的 watcher 事件创建成功"); &#125; catch (Exception e) &#123; log.error("connect zookeeper fail，please check the log &gt;&gt; &#123;&#125;", e.getMessage(), e); &#125; &#125;&#125; 完整代码https://github.com/austin-brant/zookeeper-spring-boot Curator使用参考Curator包含了几个包： curator-framework：对zookeeper的底层api的一些封装 curator-client：提供一些客户端的操作，例如重试策略等 curator-recipes：封装了一些高级特性，如：Cache事件监听、选举、分布式锁、分布式计数器、分布式Barrier等 Maven依赖(使用curator的版本：2.12.0，对应Zookeeper的版本为：3.4.x，如果跨版本会有兼容性问题，很有可能导致节点操作失败)： Curator使用详解参考：Zookeeper客户端Curator使用详解]]></content>
      <categories>
        <category>中间件</category>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Springboot</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github Page + Hexo + Next 搭建个人博客]]></title>
    <url>%2F2019%2F07%2F11%2FGithub-Page-Hexo-Next-%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[什么是 Hexo？Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 准备环境准备 node 和 git 环境， 安装NodeJs首先，安装 NodeJS，因为 Hexo 是基于 Node.js 驱动的一款博客框架，相比起前面提到过的 Jekyll 框架更快更简洁。 安装git然后，安装 git，一个分布式版本控制系统，用于项目的版本控制管理，作者是 Linux 之父。如果 Git 还不熟悉可以参考廖雪峰大神的 Git 教程。 在命令行中输入相应命令验证是否成功，如果成功会有相应的版本号。 123git versionnode -vnpm -v 安装 Hexo安装如果以上环境准备好了就可以使用 npm 开始安装 Hexo 了。也可查看 Hexo 的详细文档。在命令行输入执行以下命令： 1npm install -g hexo-cli 预览安装 Hexo 完成后，再执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。 123hexo init myBlogcd myBlognpm install 新建完成后，指定文件夹的目录如下： 12345678.├── _config.yml # 网站的配置信息，您可以在此配置大部分的参数。 ├── package.json├── scaffolds # 模版文件夹├── source # 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹| ├── _drafts # 草稿文件| └── _posts # 文章Markdowm文件 └── themes # 主题文件夹 好了，如果上面的命令都没报错的话，就恭喜了，运行 1hexo s 命令，其中 s 是 server 的缩写，在浏览器中输入 http://localhost:4000 回车就可以预览效果了。 配置GitHub Page首先如果你还没有 Github 账号的先 注册 一个。然后创建git hub同名仓库。 PS: Github 仅能使用一个同名仓库的代码托管一个静态站点 然后打开仓库创建一个 index.html 文件，并随意先写点内容，比如 Hello World. 这个时候打开 http://你的用户名.github.io 就可以看到你的站点啦，是不是很简单！index.html 内容只是暂时的预览效果，后面把 Hexo 的文件部署上去就可以在 http://你的用户名.github.io 看到你自己的博客啦！ 部署到 Github此时，本地和Github的工作做得差不了，是时候把它们两个连接起来了。你也可以查看官网的部署教程。先不着急，部署之前还需要修改配置和安装部署插件。第一：打开项目根目录下的 _config.yml 配置文件配置参数。拉到文件末尾，填上如下配置（也可同时部署到多个仓库，后面再说）： 第二：要安装一个部署插件 hexo-deployer-git。 1npm install hexo-deployer-git --save 最后执行以下命令就可以部署上传啦，以下 g 是 generate 缩写，d 是 deploy 缩写： 1hexo g -d 稍等一会，在浏览器访问网址： https://你的用户名.github.io 就会看到你的博客啦！！ Hexo写作头部规则相关设置文章中的头部会需要根据规则编写标题、更新时间，标签分类等类容。对读者是不可见的，语法如下： 1234567---title: Git添加账号date: 2017-06-08 19:49:26tags: [git]categories: gitcomments: true--- 以下是预先定义的参数，您可在模板中使用这些参数值并加以利用。 参数 描述 默认值 说明 layout 布局 title 标题 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 只有文章支持分类和标签.在 Hexo 中两者有着明显的差别：分类具有顺序性和层次性，也就是说 Foo, Bar 不等于 Bar, Foo；而标签没有顺序和层次。 12345categories:- Diarytags:- PS3- Games 常用Hexo指令官方文档： https://hexo.io/zh-cn/docs/commands init1hexo init [folder] 新建一个网站。如果没有设置 folder ，Hexo 默认在目前的文件夹建立网站。 new1hexo new [layout] &lt;title&gt; 新建一篇文章。如果没有设置 layout 的话，默认使用 _config.yml 中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。 1hexo new &quot;post title with whitespace&quot; generate1$ hexo generate 生成静态文件。 123选项 描述-d, --deploy 文件生成后立即部署网站-w, --watch 监视文件变动 该命令可以简写为 1$ hexo g deploy1$ hexo deploy 部署网站。 12参数 描述-g, --generate 部署之前预先生成静态文件 该命令可以简写为： 1$ hexo d clean1$ hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。 在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。 server1$ hexo server 启动服务器。默认情况下，访问网址为： http://localhost:4000/。 1234选项 描述-p, --port 重设端口-s, --static 只使用静态文件-l, --log 启动日记记录，使用覆盖记录格式 该命令可以简写： 1$ hexo s 多终端编辑如果我想要在公司写博客怎么办，或者说如果我换电脑了怎么办，因为在github中的我们github.io项目是只有编译后的文件的，没有源文件的，也就是说，如果我们的电脑坏了，打不开了，我们的博客就不能进行更新了，所以需要把源文件也上传到github（或者Coding）上，然后对我们的源文件进行版本管理，这样我们就可以在另一台电脑上pull我们的源码，然后编译完之后push上去。 配置提交git将博客编辑路径hexo配置及soure文件等提交到git hub仓库；为了筛选出配置文件、主题目录、博文等重要信息，作为需要GItHub管理的文件, public内文件是根据source文件夹内容自动生成，不需要备份，不然每次改动内容太多, 即使是私有仓库，除去在线服务商员工可以看到的风险外，还有云服务商被攻击造成泄漏等可能，所以不建议将配置文件传上去。.gitignore文件： 12345678.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/_config.yml 注意这里有个坑！！！如果你用的是第三方的主题theme，是使用git clone下来的话，要把主题文件夹下面把.git文件夹删除掉，不然主题无法push到远程仓库，导致你发布的博客是一片空白。 另一台电脑操作 首先需要搭建环境（Node，Git） 安装Hexo 1npm install -g hexo-cli 拉上一步提交的代码到本地进入文件路径执行 1hexo g 然后根据提示进行操作即可。 发布报错错误如下 12345678Connection to github.com closed by remote host.fatal: The remote end hung up unexpectedlyerror: failed to push some refs to &apos;git@github.com:xxxxx/xxxxx.github.io.git&apos;FATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: Spawn failed at ChildProcess.&lt;anonymous&gt; (/home/pi/blog/node_modules/_hexo-util@0.6.3@hexo-util/lib/spawn.js:52:19) at ChildProcess.emit (events.js:182:13) at Process.ChildProcess._handle.onexit (internal/child_process.js:240:12) 这个错误是因为本地的博客版本与远程的版本不一致，解决方法: 删除博客目录下的.deploy_git文件夹，然后克隆远程(也就是将要发布的地址)的仓库到博客目录里面，然后改名字为.deploy_git 另外一个不那么绕的办法是把远端仓库删除，删除本地的.deploy_git，再次发布，不过这样做会导致之前的提交记录丢失。]]></content>
      <categories>
        <category>教程</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[Spring-Boot]过滤器vs拦截器]]></title>
    <url>%2F2019%2F07%2F10%2FSpring-Boot-%E8%BF%87%E6%BB%A4%E5%99%A8vs%E6%8B%A6%E6%88%AA%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言在spring-boot中，经常会用到滤器和拦截器，但是什么场景适合用过滤器，什么场景适合用拦截器，而且有什么异同点？下面来详细分析一下。 过滤器过滤器Filter，是在Servlet规范中定义的，是Servlet容器支持的，该接口定义在javax.servlet包下，主要是在客户端请求(HttpServletRequest)进行预处理，以及对服务器响应(HttpServletResponse)进行后处理。接口代码如下: 1234567891011package javax.servlet;import java.io.IOException;public interface Filter &#123; void init(FilterConfig var1) throws ServletException; void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException; void destroy();&#125; 对上面三个接口方法进行分析: init(FilterConfig)初始化接口，在用户自定义的Filter初始化时被调用，它与Servlet的 init方法的作用是一样的。 doFilter(ServletRequest,ServletResponse,FilterChain)在每个用户的请求进来时这个方法都会被调用，并在Servlet的service方法之前调用(如果我们是开发Servlet项目)，而FilterChain就代表当前的整个请求链，通过调用 FilterChain.doFilter可以将请求继续传递下去，如果想拦截这个请求，可以不调用FilterChain.doFilter，那么这个请求就直接返回了，所以Filter是一种责任链设计模式，在spring security就大量使用了过滤器，有一条过滤器链。 destroy当Filter对象被销毁时，这个方法被调用，注意，当Web容器调用这个方法之后，容器会再调用一次doFilter方法。 自定义Filter过滤器在springboot自定义Filter类如下: 123456789101112131415161718192021@Componentpublic class MyFilter implements Filter &#123; private Logger logger = LoggerFactory.getLogger(MyFilter.class); @Override public void init(FilterConfig filterConfig) throws ServletException &#123; logger.info("filter init"); &#125; @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123; logger.info("doFilter"); //对request,response进行预处理 //TODO 进行业务逻辑 filterChain.doFilter(servletRequest, servletResponse); &#125; @Override public void destroy() &#123; logger.info("filter destroy"); &#125;&#125; FilterRegistrationBean方式在springboot中提供了FilterRegistrationBean方式，此类提供setOrder方法，可以为多个filter设置排序值。代码如下: 1234567891011121314151617181920212223242526272829303132333435@Configurationpublic class FilterConfig &#123; /** * 配置一个Filter注册器 * * @return */ @Bean public FilterRegistrationBean filterRegistrationBean1() &#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(filter1()); registrationBean.setName("filter1"); //设置顺序 registrationBean.setOrder(10); return registrationBean; &#125; @Bean public FilterRegistrationBean filterRegistrationBean2() &#123; FilterRegistrationBean registrationBean = new FilterRegistrationBean(); registrationBean.setFilter(filter2()); registrationBean.setName("filter2"); //设置顺序 registrationBean.setOrder(3); return registrationBean; &#125; @Bean public Filter filter1() &#123; return new MyFilter(); &#125; @Bean public Filter filter2() &#123; return new MyFilter2(); &#125;&#125; 拦截器拦截器是Spring提出的概念，它的作用于过滤器类似，可以拦截用户请求并进行相应的处理，它可以进行更加精细的控制。在SpringMVC中，DispatcherServlet捕获每个请求，在到达对应的Controller之前，请求可以被拦截器处理，在拦截器中进行前置处理后，请求最终才到达Controller。 拦截器的接口是org.springframework.web.servlet.HandlerInterceptor接口，接口代码如下: 1234567891011public interface HandlerInterceptor &#123; default boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return true; &#125; default void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable ModelAndView modelAndView) throws Exception &#123; &#125; default void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, @Nullable Exception ex) throws Exception &#123; &#125;&#125; 复制代码接口方法解读: preHandle方法对客户端发过来的请求进行前置处理，如果方法返回true,继续执行后续操作，如果返回false，执行中断请求处理，请求不会发送到Controller postHandler方法在请求进行处理后执行，也就是在Controller方法调用之后处理，当然前提是之前的 preHandle方法返回 true。具体来说，postHandler方法会在DispatcherServlet进行视图返回渲染前被调用，也就是说我们可以在这个方法中对 Controller 处理之后的ModelAndView对象进行操作 afterCompletion方法该方法在整个请求结束之后执行，当然前提依然是 preHandle方法的返回值为 true才行。该方法一般用于资源清理工作. 自定义拦截器123456789101112131415161718public class MyInterceptor implements HandlerInterceptor &#123; private Logger logger = LoggerFactory.getLogger(MyInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; logger.info("preHandle...."); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; logger.info("postHandle..."); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; logger.info("afterCompletion..."); &#125;&#125; 注册拦截器同时配置拦截器规则12345678910111213@Configurationpublic class WebMvcConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(handlerInterceptor()) //配置拦截规则 .addPathPatterns("/**"); &#125; @Bean public HandlerInterceptor handlerInterceptor() &#123; return new MyInterceptor(); &#125;&#125; 多个拦截器协同工作在springMVC中我们可以实现多个拦截器，并依次将他们注册进去，如下： 123456public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(handlerInterceptor()) .addPathPatterns("/**"); registry.addInterceptor(handlerInterceptor2()) .addPathPatterns("/**");&#125; 拦截器的顺序也跟他们注册时的顺序有关，至少 preHandle方法是这样，下图表示了两个拦截器协同工作时的执行顺序： 后台打印日志显示了执行顺序： 1234567io-9999-exec-2] c.p.filter.interceptor.MyInterceptor : preHandle....2018-09-13 12:13:31.292 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor2 : preHandle2....2018-09-13 12:13:31.388 INFO 9736 --- [nio-9999-exec-2] c.p.filter.controller.HelloController : username:pjmike,password:1234562018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor2 : postHandle2...2018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor : postHandle...2018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor2 : afterCompletion2...2018-09-13 12:13:31.418 INFO 9736 --- [nio-9999-exec-2] c.p.filter.interceptor.MyInterceptor : afterCompletion... 拦截器与过滤器之间的区别从上面对拦截器与过滤器的描述来看，它俩是非常相似的，都能对客户端发来的请求进行处理，它们的区别如下： 作用域不同 过滤器依赖于servlet容器，只能在 servlet容器，web环境下使用 拦截器依赖于spring容器，可以在spring容器中调用，不管此时Spring处于什么环境 细粒度的不同 过滤器的控制比较粗，只能在请求进来时进行处理，对请求和响应进行包装 拦截器提供更精细的控制，可以在controller对请求处理之前或之后被调用，也可以在渲染视图呈现给用户之后调用 中断链执行的难易程度不同 拦截器可以 preHandle方法内返回 false 进行中断 过滤器就比较复杂，需要处理请求和响应对象来引发中断，需要额外的动作，比如将用户重定向到错误页面 小结简单总结一下，拦截器相比过滤器有更细粒度的控制，依赖于Spring容器，可以在请求之前或之后启动，过滤器主要依赖于servlet，过滤器能做的，拦截器基本上都能做。 [转载] springboot系列文章之过滤器 vs 拦截器]]></content>
      <tags>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx Https 配置]]></title>
    <url>%2F2019%2F07%2F10%2FNginx-Https-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Https简介要保证Web浏览器到服务器的安全连接，HTTPS几乎是唯一选择。HTTPS其实就是HTTP over SSL，也就是让HTTP连接建立在SSL安全连接之上, 并不是一个单独的应用层协议。那么对于Https, 只需要了解Http（参考HTTP详解）和SSL协议即可。而所谓的HTTPS报文也就是SSL报文： HTTPS作用不使用SSL/TLS的HTTP通信，就是不加密的通信。所有信息明文传播，带来了三大风险。 （1） 窃听风险（eavesdropping）：第三方可以获知通信内容。 （2） 篡改风险（tampering）：第三方可以修改通信内容。 （3） 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 SSL/TLS协议是为了解决这三大风险而设计的，希望达到： （1） 所有信息都是加密传播，第三方无法窃听。 （2） 具有校验机制，一旦被篡改，通信双方会立刻发现。 （3） 配备身份证书，防止身份被冒充。 SSL协议SSL（Secure Socket Layer）安全套接层，是一种位于应用层与传输层之间，为网络通信提供安全及完整性验证的一种网络协议。相对于TCP或HTTP协议，SSL协议要复杂很多。由于它也是建立在TCP协议之上的，所以在使用SSL传输数据之前需要先进行三次握手和服务器建立连接，具体的流程如图所示： SSL协议的握手过程 (1) 客户端先给服务端发送一个消息，消息内容包括：客户端支持的加密方式，支持的压缩方法，SSL的版本号，客户端生成的随机数，文本内容“Hello”等； (2) 服务端接收到消息后，也回复一个Hello，并携带从客户端支持的加密方式中选择的加密方式，服务端生成的随机数，服务端的SSL版本号等信息； (3) 随后服务器给客户端发送一个Certificate报文，报文中包含服务端的公钥证书； (4) 紧接着服务器给客户端发送Server Hello Done, 表示最初的协商握手过程结束； (5) 客户端接收到服务端发送的握手结束的消息后，以Client Key Exchange作为回应，此报文中包含通信加密过程中使用的一种被称为Pre-master secret的随机密码串，并使用第三步接收到的公钥证书进行了加密； (6) 接着客户端发送Change Cipher Spec报文，该报文告知服务端，此步骤之后的所有数据将使用第五步中生成的master secret进行加密（master secret的生成过程看后面的介绍）； (7) 随后客户端发送Finish报文，此报文中包含连接至今所有报文的整体校验值，用于完整性验证； (8) 服务端接收到客户端发送的Change Cliper Spec报文后，同样以Change Cliper Spec报文 作为回应； (9) 接着服务端发送Finish报文给客户端，表示服务端已正确解析客户端发送的整体校验值，至此，SSL握手的过程结束。 随后开始使用HTTP协议传输使用master secret加密过的数据。 说明 前两步是协商加密算法以及传输各自生成的随机数（为后续生成master secret做准备）的过程； 第三步服务端将自己的证书发送给客户端，这个证书中包含一个数字签名（CA签名）和服务端CA证书的公钥，客户端对证书中包含的服务端信息进行Hash, 同时使用接收到的公钥对数字证书解密，获取其中的Hash值，与前面计算得到的Hash值进行比较，即可验证证书的有效性（完整性&amp;真实性）； 服务端收到客户端发送的Change Cipher Spec（第五步），会使用自己的私钥进行解密，获取报文中的Pre-master secret，这时通信双方都拥有对方的Random(前两步生成的)，Pre-master secret，以及自身的Random, 将三个数作为种子通过算法生成master secret, 用来加密后续Http请求过程中的数据。其中master secret的生成规则为: 123456master_secret =MD5(pre_master_secret + SHA(&apos;A&apos; + pre_master_secret + ClientHello.random + ServerHello.random)) +MD5(pre_master_secret + SHA(&apos;BB&apos; + pre_master_secret + ClientHello.random + ServerHello.random)) +MD5(pre_master_secret + SHA(&apos;CCC&apos; + pre_master_secret + ClientHello.random + ServerHello.random)); CA证书 私钥私钥就是一个算法名称加上密码串，自己保存，从不给任何人看 公钥公钥也是一个算法名称加上密码串，一般不会单独给别人，而是嵌在证书里面一起给别人 CA专门用自己的私钥给别人进行签名的单位或者机构 申请（签名）文件在公钥的基础上加上一些申请人的属性信息，比如我是谁，来自哪里，名字叫什么，证书适用于什么场景等的信息，然后带上进行的签名，发给CA（私下安全的方式发送），带上自己签名的目的是为了防止别人篡改文件。 证书文件证书由公钥加上描述信息，然后经过私钥签名之后得到，一般都是一个人的私钥给另一个人的公钥签名，如果是自己的私钥给自己的公钥签名，就叫自签名。 签名过程CA收到申请文件后，会走核实流程，确保申请人确实是证书中描述的申请人，防止别人冒充申请者申请证书，核实通过后，会用CA的私钥对申请文件进行签名，签名后的证书包含申请者的基本信息，CA的基本信息，证书的使用年限，申请人的公钥，签名用到的摘要算法，CA的签名。 签完名之后，证书就可以用了。 证书如何验证下面以浏览器为例，说明证书的验证过程： 在TLS握手的过程中，浏览器得到了网站的证书 打开证书，查看是哪个CA签名的这个证书 在自己信任的CA库中，找相应CA的证书， 用CA证书里面的公钥解密网站证书上的签名，取出网站证书的校验码（指纹），然后用同样的算法（比如sha256）算出出网站证书的校验码，如果校验码和签名中的校验码对的上，说明这个证书是合法的，且没被人篡改过 读出里面的CN，对于网站的证书，里面一般包含的是域名 检查里面的域名和自己访问网站的域名对不对的上，对的上的话，就说明这个证书确实是颁发给这个网站的 到此为止检查通过。 如果浏览器发现证书有问题，一般是证书里面的签名者不是浏览器认为值得信任的CA，浏览器就会给出警告页面，这时候需要谨慎，有可能证书被掉包了。如访问12306网站，由于12306的证书是自己签的名，并且浏览器不认为12306是受信的CA，所以就会给警告，但是一旦你把12306的根证书安装到了你的浏览器中，那么下次就不会警告了，因为你配置了浏览器让它相信12306是一个受信的CA。 SSL证书类型SSL使用证书来创建安全连接。有两种验证模式： 1 仅客户端验证服务器的证书，客户端自己不提供证书； 2 客户端和服务器都互相验证对方的证书。 显然第二种方式安全性更高，一般用网上银行会这么搞，但是，大部分服务只需要采用第一种方式就可以。 服务器自己的证书必须经过某“权威”证书的签名，而这个“权威”证书又可能经过更权威的证书签名，这么一级一级追溯上去，最顶层那个最权威的证书就称为根证书。根证书直接内置在浏览器中，这样，浏览器就可以利用自己自带的根证书去验证某个服务器的证书是否有效。 SSL证书级别分为三种类型，域名型SSL证书（DV SSL）、企业型SSL证书（OVSSL）、增强型SSL证书（EVSSL） 1. 域名型 SSL 证书（DV SSL - Domain Validation SSL）即证书颁布机构只对域名的所有者进行在线检查，通常是验证域名下某个指定文件的内容，或者验证与域名相关的某条 TXT 记录； 比如访问 [http|https]://www.mimvp.com/.../test.txt，文件内容： 2016082xxxxxmimvpcom2016 或添加一条 TXT 记录：www.mimvp.com –&gt; TXT –&gt; 20170xxxxxmimvpcom2066 2. 企业型 SSL 证书（OV SSL - Organization Validation SSL）是要购买者提交组织机构资料和单位授权信等在官方注册的凭证， 证书颁发机构在签发 SSL 证书前，不仅仅要检验域名所有权， 还必须对这些资料的真实合法性进行多方查验，只有通过验证的才能颁发 SSL 证书。 3. 增强型 SSL 证书（EV SSL - Extended Validation SSL）与其他 SSL 证书一样，都是基于 SSL/TLS 安全协议， 但是验证流程更加具体详细，验证步骤更多， 这样一来证书所绑定的网站就更加的可靠、可信。 它跟普通 SSL 证书的区别也是明显的，安全浏览器的地址栏变绿， 如果是不受信的 SSL 证书则拒绝显示，如果是钓鱼网站，地址栏则会变成红色，以警示用户。 自签名证书生成下面简单介绍如何创建一个自签名的SSL证书。执行脚本create_cert.sh即可。 12345678910111213141516171819202122232425#!/bin/bash## 生成秘钥key## 会有两次要求输入密码,输入同一个即可,## 然后你就获得了一个server.key文件.openssl genrsa -des3 -out server.key 2048## 以后使用此文件(通过openssl提供的命令或API)可能经常回要求输入密码## 如果想去除输入密码的步骤可以使用以下命令:openssl rsa -in server.key -out server.key## 创建服务器证书的申请文件server.csr## 其中Country Name填CN,Common Name填主机名也可以不填,如果不填浏览器会认为不安全.## (例如你以后的url为https://abcd/xxxx....这里就可以填abcd),其他的都可以不填.openssl req -new -key server.key -out server.csr## 创建CA证书## 此时,你可以得到一个ca.crt的证书,这个证书用来给自己的证书签名.openssl req -new -x509 -key server.key -out ca.crt -days 3650## 创建自当前日期起有效期为期十年的服务器证书server.crtopenssl x509 -req -days 3650 -in server.csr -CA ca.crt -CAkey server.key -CAcreateserial -out server.crt 其中第三条指令在第二步第二条时会出来一个填写资料的界面（我已经填好参考，有些地方可以空着） 1234567Country Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:BEIJINGLocality Name (eg, city) []:haidianOrganization Name (eg, company) [Internet Widgits Pty Ltd]:BaiduOrganizational Unit Name (eg, section) []:Common Name (e.g. server FQDN or YOUR name) []:localhostEmail Address []: 这里有点要注意， Common Name (e.g. server FQDN or YOUR name) []: 这一项，是最后可以访问的域名，我这里为了方便测试，写成 localhost ，如果是为了给网站生成证书，需要写成 xxxx.com 。 脚本执行完成后，ls你的文件夹,可以看到一共生成了5个文件:ca.crt ca.srl server.crt server.csr server.key其中,server.crt和server.key就是你的nginx需要的证书文件. 配置nginx(1) 查看nginx是否安装了ssl模块 nginx -V 又红框中标记内容，则表示已经安装了ssl模块，否则需要手动安装或是更换nginx版本。 ###（2）配置nginx 将之前生成的密钥及证书文件server.crt和server.key拷贝到nginx配置文件nginx.conf同级目录，然后修改nginx.conf. 1234567891011121314151617181920212223242526272829303132333435363738394041424344# HTTPS serverserver &#123; listen 443 ssl; server_name localhost; # 域名 ssl_certificate server.crt; ssl_certificate_key server.key; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root /home/bigdata/dayu/www/; index index.html; proxy_set_header Host $http_host; rewrite (.*) /index.html break; &#125; location /public/ &#123; root /home/bigdata/dayu/www; &#125; location /metamap/ &#123; proxy_pass http://127.0.0.1:8083/; proxy_redirect /metamap/ http://127.0.0.1:8083/; proxy_set_header Host $http_host; &#125; location /bflow/ &#123; proxy_pass http://127.0.0.1:8084/; proxy_redirect /workflow/ http://127.0.0.1:8084/; proxy_set_header Host $http_host; &#125; location /authenticate/ &#123; proxy_pass http://cp01-mxnet-test.epc.baidu.com:8010/; proxy_redirect /authenticate/ http://127.0.0.1:8082/; proxy_set_header Host $http_host; &#125; &#125;]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA基础面试题]]></title>
    <url>%2F2019%2F07%2F07%2FJAVA%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JAVA语言基础java垃圾回收？ 引用计数法 (Reference Counting)引用计数器有一个严重的问题，即无法处理循环引用的情况。因此，在 Java 的垃圾回收器中没有使用这种算法。 标记-清除算法 (Mark-Sweep)标记-清除算法将垃圾回收分为两个阶段：标记阶段和清除阶段。该算法最大的问题是存在大量的空间碎片，因为回收后的空间是不连续的。 复制算法 (Copying)将现有的内存空间分为两快，每次只使用其中一块，在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中，之后，清除正在使用的内存块中的所有对象，交换两个内存的角色，完成垃圾回收。 算法的缺点是将系统内存折半。 Java 的新生代串行垃圾回收器中使用了复制算法的思想。 标记-压缩算法 (Mark-Compact)标记-压缩算法是一种老年代的回收算法，它在标记-清除算法的基础上做了一些优化。也首先需要从根节点开始对所有可达对象做一次标记，但之后，它并不简单地清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。这种方法既避免了碎片的产生，又不需要两块相同的内存空间，因此，其性价比比较高。 增量算法 (Incremental Collecting)增量算法的基本思想是，如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程和应用程序线程交替执行。每次，垃圾收集线程只收集一小片区域的内存空间，接着切换到应用程序线程。依次反复，直到垃圾收集完成。 能减少系统的停顿时间。但是，因为线程切换和上下文转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 分代 (Generational Collecting)根据垃圾回收对象的特性，不同阶段最优的方式是使用合适的算法用于本阶段的垃圾回收，分代算法即是基于这种思想，它将内存区间根据对象的特点分成几块，根据每块内存区间的特点，使用不同的回收算法，以提高垃圾回收的效率。 HashMap的实现原理？ 数组 + 链表 “模”运算的消耗还是比较大的，在 HashMap 中是这样做的：调用 indexFor(int h, int length) 方法来计算该对象应该保存在 table 数组的哪个索引处。indexFor(int h, int length) 方法的代码如下： 123456/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 这个方法非常巧妙，它通过 h &amp; (table.length -1) 来得到该对象的保存位，而 HashMap 底层数组的长度总是 2 的 n 次方，这是 HashMap 在速度上的优化。在 HashMap 构造器中有如下代码： 1234// Find a power of 2 &gt;= initialCapacityint capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; 这段代码保证初始化时 HashMap 的容量总是 2 的 n 次方，即底层数组的长度总是为 2 的 n 次方。当 length 总是 2 的 n 次方时，h&amp; (length-1)运算等价于对 length 取模，也就是 h%length，但是 &amp; 比 % 具有更高的效率。这看上去很简单，其实比较有玄机的。 我们也可以发现，当数组长度为 15 的时候，hash 值会与 15-1（1110）进行“与”，那么最后一位永远是 0，而 0001，0011，0101，1001，1011，0111，1101 这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！而当数组长度为16时，即为2的n次方时，2n-1 得到的二进制数的每个位上的值都为 1，这使得在低位上&amp;时，得到的和原 hash 的低位相同，加之 hash(int h)方法对 key 的 hashCode 的进一步优化，加入了高位计算，就使得只有相同的 hash 值的两个值才会被放到数组中的同一个位置上形成链表。所以说，当数组长度为 2 的 n 次幂的时候，不同的 key 算得得 index 相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。 在 JDK1.8 中对 HashMap 进行了优化：当hash碰撞之后写入链表的长度超过了阈值(默认为8)，链表将会转换为红黑树。假设 hash 冲突非常严重，一个数组后面接了很长的链表，此时重新的时间复杂度就是 O(n) 。如果是红黑树，时间复杂度就是 O(logn)，大大提高了查询效率。 什么是值传递和引用传递？java中是值传递还是引用传递，还是都有? 值传递就是在方法调用的时候，实参是将自己的一份拷贝赋给形参，在方法内，对该参数值的修改不影响原来实参，常见的例子就是刚开始学习c语言的时候那个交换方法的例子了。 引用传递是在方法调用的时候，实参将自己的地址传递给形参，此时方法内对该参数值的改变，就是对该实参的实际操作。 在java中只有一种传递方式，那就是值传递.可能比较让人迷惑的就是java中的对象传递时，对形参的改变依然会影响到该对象的内容。 下面这个例子来说明java中是值传递. 1234567891011public class Test &#123; public static void main(String[] args) &#123; StringBuffer sb = new StringBuffer("hello "); getString(sb); System.out.println(sb); &#125; public static void getString(StringBuffer s) &#123; //s = new StringBuffer("ha"); s.append("world"); &#125;&#125; 在上面这个例子中,当前输出结果为:hello world。这并没有什么问题，可能就是大家平常所理解的引用传递，那么当然会改变StringBuffer的内容。但是如果把上面的注释去掉，那么就会输出:hello.此时sb的值并没有变成ha hello. 假如说是引用传递的话，那么形参的s也就是sb的地址，此时在方法里new StringBuffer（），并将该对象赋给s，也就是说s现在指向了这个新创建的对象.按照引用传递的说法，此时对s的改变就是对sb的操作，也就是说sb应该也指向新创建的对象，那么输出的结果应该为ha world.但实际上输出的仅是hello.这说明sb指向的还是原来的对象，而形参s指向的才是创建的对象,这也就验证了java中的对象传递也是值传递。 接口和抽象类的差同是什么?差异： 接口中所有的方法隐含的都是抽象的。而抽象类则可以同时包含抽象和非抽象的方法。 类可以实现很多个接口，但是只能继承一个抽象类 类如果要实现一个接口，它必须要实现接口声明的所有方法。但是，类可以不实现抽象类声明的所有方法，当然，在这种情况下，类也必须得声明成是抽象的。 抽象类可以在不提供接口方法实现的情况下实现接口。 Java 接口中声明的变量默认都是 final 的，即为常量。抽象类可以包含非final的变量，即成员变量。 Java 接口中的成员函数默认是 public 的。抽象类的成员函数可以是 private，protected 或者是 public 。 接口是绝对抽象的，不可以被实例化(java 8已支持在接口中实现默认的方法)。抽象类也不可以被实例化，但是，如果它包含 main 方法的话是可以被调用的。 接口比抽象类更加抽象，因为抽象类中可以定义构造器，可以有抽象方法和具体方法，而接口中不能定义构造器而且其中的方法全部都是抽象方法。 有抽象方法的类必须被声明为抽象类，而抽象类未必要有抽象方法。 相同： 抽象类和接口都不能够实例化，但可以定义抽象类和接口类型的引用。 一个类如果继承了某个抽象类或者实现了某个接口都需要对其中的抽象方法全部进行实现，否则该类仍然需要被声明为抽象类。 接口与继承接口是否可继承（extends）接口？抽象类是否可实现（implements）接口？抽象类是否可继承具体类（concrete class）？答：接口可以继承接口，而且支持多重继承。抽象类可以实现(implements)接口，抽象类可继承具体类也可以继承抽象类。 为什么wait, notify 和 notifyAll这些方法不在thread类里面?这是个设计相关的问题，它考察的是面试者对现有系统和一些普遍存在但看起来不合理的事物的看法。回答这些问题的时候，你要说明为什么把这些方法放在Object类里是有意义的，还有不把它放在Thread类里的原因。一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。 String、StringBuffer与StringBuilder的区别 String：适用于少量的字符串操作的情况。StringBuilder：适用于单线程下在字符缓冲区进行大量操作的情况。 StringBuffer：适用多线程下在字符缓冲区进行大量操作的情况。StringBuilder：是线程不安全的，而StringBuffer是线程安全的。 这三个类之间的区别主要是在两个方面，即运行速度和线程安全这两方面。 首先说运行速度，或者说是执行速度，在这方面运行速度快慢为： StringBuilder &gt; StringBuffer &gt; String。 String最慢的原因String为字符串常量，而StringBuilder和StringBuffer均为字符串变量，即String对象一旦创建之后该对象是不可更改的，但后两者的对象是变量，是可以更改的。 再来说线程安全在线程安全上，StringBuilder是线程不安全的，而StringBuffer是线程安全的。如果一个StringBuffer对象在字符串缓冲区被多个线程使用时，StringBuffer中很多方法可以带有synchronized关键字，所以可以保证线程是安全的，但StringBuilder的方法则没有该关键字，所以不能保证线程安全，有可能会出现一些错误的操作。所以如果要进行的操作是多线程的，那么就要使用StringBuffer，但是在单线程的情况下，还是建议使用速度比较快的StringBuilder。 Thread类的sleep()方法和对象的wait()方法都可以让线程暂停执行，它们有什么区别?答： - [ ] sleep()方法（休眠）是线程类（Thread）的静态方法，调用此方法会让当前线程暂停执行指定的时间，将执行机会（CPU）让给其他线程，但是对象的锁依然保持，因此休眠时间结束后会自动恢复（线程回到就绪状态，请参考第66题中的线程状态转换图）。 - [ ] wait()是Object类的方法，调用对象的wait()方法导致当前线程放弃对象的锁（线程暂停执行），进入对象的等待池（wait pool），只有调用对象的notify()方法（或notifyAll()方法）时才能唤醒等待池中的线程进入等锁池（lock pool），如果线程重新获得对象的锁就可以进入就绪状态。 补充：可能不少人对什么是进程，什么是线程还比较模糊，对于为什么需要多线程编程也不是特别理解。简单的说：进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，是操作系统进行资源分配和调度的一个独立单位；线程是进程的一个实体，是CPU调度和分派的基本单位，是比进程更小的能独立运行的基本单位。线程的划分尺度小于进程，这使得多线程程序的并发性高；进程在执行时通常拥有独立的内存单元，而线程之间可以共享内存。使用多线程的编程通常能够带来更好的性能和用户体验，但是多线程的程序对于其他程序是不友好的，因为它可能占用了更多的CPU资源。当然，也不是线程越多，程序的性能就越好，因为线程之间的调度和切换也会浪费CPU时间。时下很时髦的Node.js就采用了单线程异步I/O的工作模式。 重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？答：方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。重载发生在一个类中，同名的方法如果有不同的参数列表（参数类型不同、参数个数不同或者二者都不同）则视为重载；重写发生在子类与父类之间，重写要求子类被重写方法与父类被重写方法有相同的返回类型，比父类被重写方法更好访问，不能比父类被重写方法声明更多的异常（里氏代换原则）。重载对返回类型没有特殊的要求。 因为调用时不能指定类型信息，编译器不知道你要调用哪个函数。因为jvm无法根据返回值类型来判断应该调用哪个方法。例如 12float max(int a, int b);int max(int a, int b); 当调用max(1, 2);时无法确定调用的是哪个，单从这一点上来说，仅返回值类型不同的重载是不应该允许的。函数的返回值只是作为函数运行之后的一个“状态”他是保持方法的调用者与被调用者进行通信的关键并不能作为某个方法的“标识” 阐述静态变量和实例变量的区别。答：静态变量是被static修饰符修饰的变量，也称为类变量，它属于类，不属于类的任何一个对象，一个类不管创建多少个对象，静态变量在内存中有且仅有一个拷贝；实例变量必须依存于某一实例，需要先创建对象然后通过对象才能访问到它。静态变量可以实现让多个对象共享内存。 静态变量：线程非安全静态变量即类变量，位于方法区，为所有对象共享，共享一份内存，一旦静态变量被修改，其他对象均对修改可见，故线程非安全。 实例变量：单例模式（只有一个对象实例存在）线程非安全，非单例线程安全。实例变量为对象实例私有，在虚拟机的堆中分配，若在系统中只存在一个此对象的实例，在多线程环境下，“犹如”静态变量那样，被某个线程修改后，其他线程对修改均可见，故线程非安全；如果每个线程执行都是在不同的对象中，那对象与对象之间的实例变量的修改将互不影响，故线程安全。局部变量：线程安全。每个线程执行时将会把局部变量放在各自栈帧的工作内存中，线程间不共享，故不存在线程安全问题 JVMJVM内存分哪几个区，每个区的作用是什么?java虚拟机主要分为以下几个区: 方法区有时候也成为永久代，在该区内很少发生垃圾回收，但是并不代表不发生GC，在这里进行的GC主要是对方法区里的常量池和对类型的卸载方法区主要用来存储已被虚拟机加载的类的信息、常量、静态变量和即时编译器编译后的代码等数据。该区域是被线程共享的。方法区里有一个运行时常量池，用于存放静态编译产生的字面量和符号引用。该常量池具有动态性，也就是说常量并不一定是编译时确定，运行时生成的常量也会存在这个常量池中。 虚拟机栈 （1）虚拟机栈也就是我们平常所称的栈内存,它为java方法服务，每个方法在执行的时候都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接和方法出口等信息。 （2）虚拟机栈是线程私有的，它的生命周期与线程相同。 （3）局部变量表里存储的是基本数据类型、returnAddress类型（指向一条字节码指令的地址）和对象引用，这个对象引用有可能是指向对象起始地址的一个指针，也有可能是代表对象的句柄或者与对象相关联的位置。局部变量所需的内存空间在编译期间确定 （4）操作数栈的作用主要用来存储运算结果以及运算的操作数，它不同于局部变量表通过索引来访问，而是压栈和出栈的方式 （5）每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接.动态链接就是将常量池中的符号引用在运行期转化为直接引用。 本地方法栈本地方法栈和虚拟机栈类似，只不过本地方法栈为Native方法服务。 堆java堆是所有线程所共享的一块内存，在虚拟机启动时创建，几乎所有的对象实例都在这里创建，因此该区域经常发生垃圾回收操作。 程序计数器内存空间小，字节码解释器工作时通过改变这个计数值可以选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理和线程恢复等功能都需要依赖这个计数器完成。该内存区域是唯一一个java虚拟机规范没有规定任何OOM情况的区域。 java类加载过程?java类加载需要经历一下7个过程： 123456789101112131415161718192021222324252627加载加载时类加载的第一个过程，在这个阶段，将完成一下三件事情：通过一个类的全限定名获取该类的二进制流。将该二进制流中的静态存储结构转化为方法去运行时数据结构。在内存中生成该类的Class对象，作为该类的数据访问入口。验证验证的目的是为了确保Class文件的字节流中的信息不回危害到虚拟机.在该阶段主要完成以下四钟验证:文件格式验证：验证字节流是否符合Class文件的规范，如主次版本号是否在当前虚拟机范围内，常量池中的常量是否有不被支持的类型.元数据验证:对字节码描述的信息进行语义分析，如这个类是否有父类，是否集成了不被继承的类等。字节码验证：是整个验证过程中最复杂的一个阶段，通过验证数据流和控制流的分析，确定程序语义是否正确，主要针对方法体的验证。如：方法中的类型转换是否正确，跳转指令是否正确等。符号引用验证：这个动作在后面的解析过程中发生，主要是为了确保解析动作能正确执行。准备准备阶段是为类的静态变量分配内存并将其初始化为默认值，这些内存都将在方法区中进行分配。准备阶段不分配类中的实例变量的内存，实例变量将会在对象实例化时随着对象一起分配在Java堆中。 public static int value=123;//在准备阶段value初始值为0 。在初始化阶段才会变为123 。解析该阶段主要完成符号引用到直接引用的转换动作。解析动作并不一定在初始化动作完成之前，也有可能在初始化之后。初始化初始化时类加载的最后一步，前面的类加载过程，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码。 解释内存中的栈(stack)、堆(heap)和方法区(method area)的用法答： 通常我们定义一个基本数据类型的变量，一个对象的引用，还有就是函数调用的现场保存都使用JVM中的栈空间； 而通过new关键字和构造器创建的对象则放在堆空间，堆是垃圾收集器管理的主要区域，由于现在的垃圾收集器都采用分代收集算法，所以堆空间还可以细分为新生代和老生代，再具体一点可以分为Eden、Survivor（又可分为From Survivor和To Survivor）、Tenured； 方法区和堆都是各个线程共享的内存区域，用于存储已经被JVM加载的类信息、常量、静态变量、JIT编译器编译后的代码等数据；程序中的字面量（literal）如直接书写的100、”hello”和常量都是放在常量池中，常量池是方法区的一部分。 栈空间操作起来最快但是栈很小，通常大量的对象都是放在堆空间，栈和堆的大小都可以通过JVM的启动参数来进行调整，栈空间用光了会引发StackOverflowError，而堆和常量池空间不足则会引发OutOfMemoryError。 String str = new String(“hello”); 描述一下JVM加载class文件的原理机制？答：JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。最后JVM对类进行初始化，包括：1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；2)如果类中存在初始化语句，就依次执行这些初始化语句。类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制（PDM）。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。JVM不会向Java程序提供对Bootstrap的引用。下面是关于几个类加载器的说明： Bootstrap：一般用本地代码实现，负责加载JVM基础核心类库（rt.jar）； Extension：从java.ext.dirs系统属性所指定的目录中加载类库，它的父加载器是Bootstrap； System：又叫应用类加载器，其父类是Extension。它是应用最广泛的类加载器。它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类，是用户自定义加载器的默认父加载器。 Spring Bootioc与aop?IOC所谓IoC，对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系。IoC的一个重点是在系统运行中，动态的向某个对象提供它所需要的其他对象。这一点是通过DI（Dependency Injection，依赖注入）来实现的。 IoC是一个很大的概念，可以用不同的方式来实现。主要的实现形式有两种: 依赖查找：容器提供回调接口和上下文环境给组件。EJB和Apache Avalon都是使用这种方式。 依赖注入：组件不做定位查询，只是提供普通的Java方法让容器去决定依赖关系。容器全权负责组件的装配，它会把符合依赖关系的对象通过JavaBean属性或者构造子传递给需要的对象。通过JavaBean属性注射依赖关系的做法称为设值方法注入（SetterInjection）；将依赖关系作为构造子参数传入的做法称为构造子注入（ConstructorInjection）。 AOPAOP用途十分广泛，其中Spring内部的声明式事务和拦截器都是利用了AOP的强大威力，才得以优雅的实现。AOP是什么呢，简单来说，它可以让编程人员在不修改对象代码的情况下，为这个对象添加额外的功能或者限制。 Spring原理初探—-IOC、AOP 线程池几种实现？使用场景？Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程 newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 @Schedule执行原理？ 默认同步执行，一个任务执行超时会阻塞其他任务；可配置线程池方式异步执行； 统一异常处理？ @ControllerAdvice 类注解： 整个spring工程全局统一异常处理@ExceptionHandler 方法注解： 作用于 Controller 级别 MYSQLACID? 什么情况下会触发rollback?原子性、一致性、隔离型、持久性 索引是什么？有什么作用以及优缺点？索引是对数据库表中一或多个列的值进行排序的结构，是帮助MySQL高效获取数据的数据结构 你也可以这样理解：索引就是加快检索表中数据的方法。数据库的索引类似于书籍的索引。在书籍中，索引允许用户不必翻阅完整个书就能迅速地找到所需要的信息。在数据库中，索引也允许数据库程序迅速地找到表中的数据，而不必扫描整个数据库。 MySQL数据库几个基本的索引类型：普通索引、唯一索引、主键索引、全文索引 索引加快数据库的检索速度 索引降低了插入、删除、修改等维护任务的速度 唯一索引可以确保每一行数据的唯一性 通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能 索引需要占物理和数据空间如果你对索引还不太熟悉，建议阅读：漫谈数据库索引 什么是事务？事务（Transaction）是并发控制的基本单位。所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。事务是数据库维护数据一致性的单位，在每个事务结束时，都能保持数据一致性。如果你对索引还不太熟悉，建议阅读：数据库事务 - Mr. David 专栏 Having与Where的区别 where 子句的作用是在对查询结果进行分组前，将不符合where条件的行去掉，即在分组之前过滤数据，where条件中不能包含聚组函数，使用where条件过滤出特定的行。 having 子句的作用是筛选满足条件的组，即在分组之后过滤数据，条件中经常包含聚组函数，使用having 条件过滤出特定的组，也可以使用多个分组标准进行分组 聚集索引和非聚集索引 1)、聚集索引聚集索引决定数据在物理磁盘上的物理排序，一个表只能有一个聚集索引，如果定义了主键，那么InnoDB会通过主键来聚集数据，如果没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有唯一的非空索引，InnoDB会隐式定义一个主键来作为聚集索引。聚集索引可以很大程度的提高访问速度，因为聚集索引将索引和行数据保存在了同一个B-Tree中，所以找到了索引也就相应的找到了对应的行数据，但在使用聚集索引的时候需注意避免随机的聚集索引(一般指主键值不连续，且分布范围不均匀)，如使用UUID来作为聚集索引性能会很差，因为UUID值的不连续会导致增加很多的索引碎片和随机I/O，最终导致查询的性能急剧下降。 2)、非聚集索引与聚集索引不同的是非聚集索引并不决定数据在磁盘上的物理排序，且在B-Tree中包含索引但不包含行数据，行数据只是通过保存在B-Tree中的索引对应的指针来指向行数据，如：上面在(user_name，city, age)上建立的索引就是非聚集索引。 MYSQL索引？如何创建？怎么命中索引？ 最左前缀where查询条件中不包含索引列中的最左索引列，则无法使用到索引查询； 顺序无关行在命中索引的情况下，字段顺序不需要严格按照索引建立的顺序 前缀索引选择字符列的前n个字符作为索引，这样可以大大节约索引空间，从而提高索引效率，MySql无法使用前缀索引做ORDER BY 和 GROUP BY以及使用前缀索引做覆盖扫描 索引列不能是表达式的一部分，也不能作为函数的参数，否则无法使用索引查询 避免重复插入？ INSERT IGNORE INTO table_name (email) VALUES (‘test9@163.com’);REPLACE INTO table_name(col_name, …) VALUES (…);INSERT … ON DUPLICATE KEY UPDATE MySql避免重复插入记录方法 REPLACE的运行与INSERT很相像,但是如果旧记录与新记录有相同的值，则在新记录被插入之前，旧记录被删除 Linux统计apache的access.log中访问量最多的5个IPcat access.log | awk &#39;{print $1}&#39; | sort | uniq -c | sort -n -r | head -5 杀死全部python进程ps -ef | grep mysqld | awk &#39;{ print $2 }&#39; | xargs kill -9 符号链接与硬链接的区别可以把符号链接，也就是软连接 当做是 windows系统里的 快捷方式。硬链接 就好像是 又复制了一份.ln 3.txt 4.txt 这是硬链接，相当于复制，不可以跨分区，但修改3,4会跟着变，若删除3,4不受任何影响。 ln -s 3.txt 4.txt 这是软连接，相当于快捷方式。修改4,3也会跟着变，若删除3,4就坏掉了。不可以用了 网络请简述TCP\UDP的区别TCP和UDP是OSI模型中的运输层中的协议。TCP提供可靠的通信传输，而UDP则常被用于让广播和细节控制交给应用的通信传输。两者的区别大致如下： TCP面向连接，UDP面向非连接即发送数据前不需要建立链接 TCP提供可靠的服务（数据传输），UDP无法保证 TCP面向字节流，UDP面向报文 TCP数据传输慢，UDP数据传输快如果还不是太了解这两者的区别，点击阅读：TCP与UDP的区别 - yipiankongbai的专栏 get请求和post请求区别？从原理性看： 根据HTTP规范，GET用于信息获取，而且应该是安全和幂等的 根据HTTP规范，POST请求表示可能修改服务器上资源的请求 从表面上看： GET请求的数据会附在URL后面，POST的数据放在HTTP包体 POST安全性比GET安全性高 四层协议？TCP三次握手？在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的是同步连接双方的序列号和确认号并交换 TCP窗口大小信息。下面详细说一下三次握手（来自简析TCP的三次握手与四次分手） 更加深入的了解TCP的三次握手与四次分手：简析TCP的三次握手与四次分手 在浏览器中输入网址之后执行会发生什么？ 查找域名对应的IP地址。这一步会依次查找浏览器缓存，系统缓存，路由器缓存，ISPNDS缓存，根域名服务器 浏览器向IP对应的web服务器发送一个HTTP请求 服务器响应请求，发回网页内容 浏览器解析网页内容更加详细的一种说法（以百度为例）（来自计算机网络之面试常考 - 牛客网） 如果你想要更加深入的了解这个过程，点击阅读：从输入网址到显示网页的全过程分析 HTTP 协议包括哪些请求？ GET：对服务器资源的简单请求 POST：用于发送包含用户提交数据的请求 ————以及———— HEAD：类似于GET请求，不过返回的响应中没有具体内容，用于获取报头 PUT：传说中请求文档的一个版本 DELETE：发出一个删除指定文档的请求 TRACE：发送一个请求副本，以跟踪其处理进程 OPTIONS：返回所有可用的方法，检查服务器支持哪些方法 CONNECT：用于ssl隧道的基于代理的请求怎么解决跨域问题？ 说一说OSI七层模型？了解OSI七层模型，请点击阅读：OSI七层模型详解 （下面的图片来自啊该网址） 说一说TCP/IP四层模型如果你不了解，请直接点击阅读：TCP/IP四层模型 ISO制定的OSI参考模型的过于庞大、复杂招致了许多批评。与此对照，由技术人员自己开发的TCP/IP协议栈获得了更为广泛的应用。如图所示，是TCP/IP参考模型和OSI参考模型的对比示意图。 下面，分别介绍各层的主要功能。 1、主机到网络层 实际上TCP/IP参考模型没有真正描述这一层的实现，只是要求能够提供给其上层-网络互连层一个访问接口，以便在其上传递IP分组。由于这一层次未被定义，所以其具体的实现方法将随着网络类型的不同而不同。 2、网络互连层 网络互连层是整个TCP/IP协议栈的核心。它的功能是把分组发往目标网络或主机。同时，为了尽快地发送分组，可能需要沿不同的路径同时进行分组传递。因此，分组到达的顺序和发送的顺序可能不同，这就需要上层必须对分组进行排序。 网络互连层定义了分组格式和协议，即IP协议（Internet Protocol）。 网络互连层除了需要完成路由的功能外，也可以完成将不同类型的网络（异构网）互连的任务。除此之外，网络互连层还需要完成拥塞控制的功能。 3、传输层 在TCP/IP模型中，传输层的功能是使源端主机和目标端主机上的对等实体可以进行会话。在传输层定义了两种服务质量不同的协议。即：传输控制协议TCP（transmission control protocol）和用户数据报协议UDP（user datagram protocol）。 TCP协议是一个面向连接的、可靠的协议。它将一台主机发出的字节流无差错地发往互联网上的其他主机。在发送端，它负责把上层传送下来的字节流分成报文段并传递给下层。在接收端，它负责把收到的报文进行重组后递交给上层。TCP协议还要处理端到端的流量控制，以避免缓慢接收的接收方没有足够的缓冲区接收发送方发送的大量数据。 UDP协议是一个不可靠的、无连接协议，主要适用于不需要对报文进行排序和流量控制的场合。 4、应用层 TCP/IP模型将OSI参考模型中的会话层和表示层的功能合并到应用层实现。 应用层面向不同的网络应用引入了不同的应用层协议。其中，有基于TCP协议的，如文件传输协议（File Transfer Protocol，FTP）、虚拟终端协议（TELNET）、超文本链接协议（Hyper Text Transfer Protocol，HTTP），也有基于UDP协议的。 算法链接判断有无环？怎么找到入口点？ 判断单链表是否存在环及求环入口点 LRU缓存实现？ HashMap + 双向链表 连续子数组的最大和? 输入一个整型数组，数组有正数也有负数。数组中一个或连续的多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为O(n) http://wiki.jikexueyuan.com/project/for-offer/question-thirty-one.html 1234567891011121314151617181920212223242526public static int findGreatestSumOfSubArray(int[] arr) &#123; // 参数校验 if (arr == null || arr.length &lt; 1) &#123; throw new IllegalArgumentException(&quot;Array must contain an element&quot;); &#125; // 记录最大的子数组和，开始时是最小的整数 int max = Integer.MIN_VALUE; // 当前的和 int curMax = 0; // 数组遍历 for (int i : arr) &#123; // 如果当前和小于等于0，就重新设置当前和 if (curMax &lt;= 0) &#123; curMax = i; &#125; // 如果当前和大于0，累加当前和 else &#123; curMax += i; &#125; // 更新记录到的最在的子数组和 if (max &lt; curMax) &#123; max = curMax; &#125; &#125; return max; &#125; 0-1背包问题？ 题目描述：有编号分别为a,b,c,d,e的五件物品，它们的重量分别是2,2,6,5,4，它们的价值分别是6,3,5,4,6，现在给你个承重为10的背包，如何让背包里装入的物品具有最大的价值总和？ http://blog.csdn.net/buptgshengod/article/details/43529417 经验如何定位问题？]]></content>
      <categories>
        <category>Java</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
</search>
